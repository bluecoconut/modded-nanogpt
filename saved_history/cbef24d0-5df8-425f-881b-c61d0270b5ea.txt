====================================================================================================
import os
import sys
with open(sys.argv[0]) as f:
    code = f.read() # read the code of this file ASAP, for logging
import uuid
import glob
import time
from dataclasses import dataclass

import numpy as np
import torch
from torch import nn
import torch.nn.functional as F
import torch.distributed as dist
import torch._inductor.config as config
from torch.nn.parallel import DistributedDataParallel as DDP

# -----------------------------------------------------------------------------
# Muon optimizer

def zeropower_via_svd(G, steps=None):
    U, S, V = G.svd()
    return U @ V.T

@torch.compile
def zeropower_via_newtonschulz5(G, steps=10, eps=1e-7):
    r"""
    Newton-Schulz iteration to compute the zeroth power / orthogonalization of G. We opt to use a
    quintic iteration whose coefficients are selected to maximize the slope at zero. For the purpose
    of minimizing steps, it turns out to be empirically effective to keep increasing the slope at
    zero even beyond the point where the iteration no longer converges all the way to one everywhere
    on the interval. This iteration therefore does not produce UV^T but rather something like US'V^T
    where S' is diagonal with S_{ii}' \sim Uniform(0.5, 1.5), which turns out not to hurt model
    performance at all relative to UV^T, where USV^T = G is the SVD.
    """
    assert len(G.shape) == 2
    a, b, c = (3.4445, -4.7750,  2.0315)
    X = G.bfloat16()
    X /= (X.norm() + eps) # ensure top singular value <= 1
    if G.size(0) > G.size(1):
        X = X.T
    for _ in range(steps):
        A = X @ X.T
        B = b * A + c * A @ A # adapted from suggestion by @jxbz, @leloykun, and @YouJiacheng
        X = a * X + B @ X
    if G.size(0) > G.size(1):
        X = X.T
    return X

zeropower_backends = dict(svd=zeropower_via_svd, newtonschulz5=zeropower_via_newtonschulz5)

class Muon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Some warnings:
    - This optimizer assumes that all parameters passed in are 2D.
    - It should not be used for the embedding layer, the final fully connected layer, or any {0,1}-D
    parameters; those should all be optimized by a standard method (e.g., AdamW).
    - To use it with 4D convolutional filters, it works well to just flatten their last 3 dimensions.
    - We believe it is unlikely to work well for training with small batch size.
    - We believe it may not work well for finetuning pretrained models, but we haven't tested this.
    - We have not yet tried this optimizer for training scenarios larger than NanoGPT (124M).

    Arguments:
        lr: The learning rate used by the internal SGD.
        momentum: The momentum used by the internal SGD.
        nesterov: Whether to use Nesterov-style momentum in the internal SGD. (recommended)
        backend: The chosen backend for the orthogonalization step. (recommended: 'newtonschulz5')
        backend_steps: The number of iteration steps to use in the backend, if it is iterative.
    """
    def __init__(self, params, lr=0.02, momentum=0.95, nesterov=True,
                 backend='newtonschulz5', backend_steps=5):
        defaults = dict(lr=lr, momentum=momentum, nesterov=nesterov, backend=backend, backend_steps=backend_steps)
        super().__init__(params, defaults)

    def step(self):

        for group in self.param_groups:

            lr = group['lr']
            momentum = group['momentum']
            zeropower_backend = zeropower_backends[group['backend']]

            # generate weight updates in distributed fashion
            total_params = sum(p.numel() for p in group['params'])
            updates_flat = torch.zeros(total_params, device='cuda', dtype=torch.bfloat16)
            curr_idx = 0
            for i, p in enumerate(group['params']):
                # luckily this will perfectly distribute a transformer with multiple of 4 layers to 8 GPUs
                if i % int(os.environ['WORLD_SIZE']) == int(os.environ['RANK']):
                    g = p.grad
                    assert g is not None
                    state = self.state[p]
                    if 'momentum_buffer' not in state:
                        state['momentum_buffer'] = torch.zeros_like(g)
                    buf = state['momentum_buffer']
                    buf.mul_(momentum).add_(g)
                    if group['nesterov']:
                        g = g.add(buf, alpha=momentum)
                    g = zeropower_backend(g, steps=group['backend_steps'])
                    g *= max(1, g.size(0)/g.size(1))**0.5
                    updates_flat[curr_idx:curr_idx+p.numel()] = g.flatten()
                curr_idx += p.numel()

            # sync updates across devices. we are not memory-constrained so can do this simple deserialization
            dist.all_reduce(updates_flat, op=dist.ReduceOp.SUM)

            # deserialize and apply updates
            curr_idx = 0
            for p in group['params']:
                g = updates_flat[curr_idx:curr_idx+p.numel()].view_as(p.data).type_as(p.data)
                p.data.add_(g, alpha=-lr)
                curr_idx += p.numel()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the GPT-2 model

class Rotary(torch.nn.Module):

    def __init__(self, dim, base=10000):
        super().__init__()
        self.dim = dim
        self.base = base
        self.inv_freq = None
        self.seq_len_cached = None
        self.cos_cached = None
        self.sin_cached = None

    def forward(self, x):
        seq_len = x.shape[1]
        if seq_len != self.seq_len_cached:
            self.inv_freq = 1.0 / (self.base ** (torch.arange(0, self.dim, 2, device=x.device).float() / self.dim))
            self.seq_len_cached = seq_len
            t = torch.arange(seq_len, device=x.device).type_as(self.inv_freq)
            freqs = torch.outer(t, self.inv_freq)
            self.cos_cached = freqs.cos().bfloat16()
            self.sin_cached = freqs.sin().bfloat16()
        return self.cos_cached[None, :, None, :], self.sin_cached[None, :, None, :]

def apply_rotary_emb(x, cos, sin):
    assert x.ndim == 4 # multihead attention
    d = x.shape[3]//2
    x1 = x[..., :d]
    x2 = x[..., d:]
    y1 = x1 * cos + x2 * sin
    y2 = x1 * (-sin) + x2 * cos
    return torch.cat([y1, y2], 3).type_as(x)

class CastedLinear(nn.Linear):
    def forward(self, x):
        return F.linear(x, self.weight.to(x.dtype))

class CausalSelfAttention(nn.Module):

    def __init__(self, config):
        super().__init__()
        self.n_head = config.n_head
        self.n_embd = config.n_embd
        self.head_dim = self.n_embd // self.n_head
        assert self.n_embd % self.n_head == 0
        self.c_q = CastedLinear(self.n_embd, self.n_embd, bias=False)
        self.c_k = CastedLinear(self.n_embd, self.n_embd, bias=False)
        self.c_v = CastedLinear(self.n_embd, self.n_embd, bias=False)
        # output projection
        self.c_proj = CastedLinear(self.n_embd, self.n_embd, bias=False)
        self.c_proj.weight.data.zero_() # zero init suggested by @Grad62304977
        self.rotary = Rotary(self.head_dim)
        self.lamb = nn.Parameter(torch.tensor(0.5)) # @Grad62304977

    def forward(self, x, v1=None):
        B, T, C = x.size() # batch size, sequence length, embedding dimensionality (n_embd)
        q = self.c_q(x).view(B, T, self.n_head, self.head_dim)
        k = self.c_k(x).view(B, T, self.n_head, self.head_dim)
        v = self.c_v(x).view(B, T, self.n_head, self.head_dim)
        if v1 is None:
            v1 = v # This happens if we are in the first block. v needs to be accessed by subsequent blocks
        v = (1 - self.lamb) * v + self.lamb * v1.view_as(v) # @Grad62304977
        cos, sin = self.rotary(q)
        q, k = F.rms_norm(q, (q.size(-1),)), F.rms_norm(k, (k.size(-1),)) # QK norm suggested by @Grad62304977
        q, k = apply_rotary_emb(q, cos, sin), apply_rotary_emb(k, cos, sin)
        y = F.scaled_dot_product_attention(q.transpose(1, 2), k.transpose(1, 2), v.transpose(1, 2), is_causal=True)
        y = y.transpose(1, 2).contiguous().view_as(x) # re-assemble all head outputs side by side
        y = self.c_proj(y)
        return y, v1

class MLP(nn.Module):
    def __init__(self, config):
        super().__init__()
        self.c_fc    = CastedLinear(config.n_embd, 4 * config.n_embd, bias=False)
        self.c_proj  = CastedLinear(4 * config.n_embd, config.n_embd, bias=False)
        self.c_proj.weight.data.zero_() # zero init suggested by @Grad62304977

    def forward(self, x):
        x = self.c_fc(x)
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = self.c_proj(x)
        return x

class Block(nn.Module):

    def __init__(self, config):
        super().__init__()
        self.attn = CausalSelfAttention(config)
        self.mlp = MLP(config)
        self.lambdas = nn.Parameter(torch.tensor([1., 0.]))

    def forward(self, x, v1, x0):
        x = self.lambdas[0] * x + self.lambdas[1] * x0
        x1, v1 = self.attn(F.rms_norm(x, (x.size(-1),)), v1)
        x = x + x1
        x = x + self.mlp(F.rms_norm(x, (x.size(-1),)))
        return x, v1

# -----------------------------------------------------------------------------
# The main GPT-2 model

@dataclass
class GPTConfig:
    vocab_size : int = 50304
    n_layer : int = 12
    n_head : int = 8 # head dim 128 suggested by @Grad62304977
    n_embd : int = 1024
    vocab_embed: int = 512

class GPT(nn.Module):

    def __init__(self, config):
        super().__init__()
        self.config = config

        self.transformer = nn.ModuleDict(dict(
            wte = nn.Embedding(config.vocab_size, config.vocab_embed),
            h = nn.ModuleList([Block(config) for _ in range(config.n_layer)]),
        ))
        self.proj = nn.ModuleDict(dict(
            proj_in = CastedLinear(config.vocab_embed, config.n_embd, bias=False),
            proj_out = CastedLinear(config.n_embd, config.vocab_embed, bias=False)
        ))
        self.lm_head = CastedLinear(config.vocab_embed, config.vocab_size, bias=False)
        self.lm_head.weight.data.zero_() # @Grad62304977
        self.skip_weights = nn.Parameter(torch.ones(config.n_layer // 2))

    def forward(self, idx, target):
        # forward the GPT model itself
        x = self.transformer.wte(idx) # token embeddings of shape (b, t, vocab_embed)
        x = F.rms_norm(x, (x.size(-1),)) # @Grad62304977
        x = F.relu(self.proj.proj_in(x))
        x0 = x
        v1 = None
        skip_connections = []

        for i, block in enumerate(self.transformer.h):
            if i < self.config.n_layer//2:
                x, v1 = block(x, v1, x0)
                skip_connections.append(x)
            else:
                weighted_skip = skip_connections.pop() * self.skip_weights[i - self.config.n_layer//2]
                x, v1 = block(x + weighted_skip, v1, x0)

        x = F.relu(self.proj.proj_out(x))
        x = F.rms_norm(x, (x.size(-1),))
        logits = self.lm_head(x)
        logits = 30 * torch.tanh(logits / 30) # @Grad62304977
        logits = logits.float()
        loss = F.cross_entropy(logits.view(-1, logits.size(-1)), target.view(-1))
        return loss.float()

# -----------------------------------------------------------------------------
# Our own simple Distributed Data Loader

def _peek_data_shard(filename):
    # only reads the header, returns header data
    with open(filename, "rb") as f:
        # first read the header, which is 256 int32 integers (4 bytes each)
        header = np.frombuffer(f.read(256*4), dtype=np.int32)
    if header[0] != 20240520:
        print("ERROR: magic number mismatch in the data .bin file!")
        print("---> HINT: Are you passing in a correct file with --input_bin?")
        print("---> HINT: Dataset encoding changed recently, re-run data prepro or refer again to README")
        print("---> HINT: For example re-run: `python dev/data/tinyshakespeare.py`, then re-try")
        exit(1)
    assert header[1] == 1, "unsupported version"
    ntok = header[2] # number of tokens (claimed)
    return ntok # for now just return the number of tokens

def _load_data_shard(filename):
    with open(filename, "rb") as f:
        # first read the header, which is 256 int32 integers (4 bytes each)
        header = np.frombuffer(f.read(256*4), dtype=np.int32)
        assert header[0] == 20240520, "magic number mismatch in the data .bin file"
        assert header[1] == 1, "unsupported version"
        ntok = header[2] # number of tokens (claimed)
        # the rest of it are tokens, stored as uint16
        tokens = np.frombuffer(f.read(), dtype=np.uint16)
    assert len(tokens) == ntok, "number of tokens read does not match header?"
    return tokens

class DistributedDataLoader:
    def __init__(self, filename_pattern, B, T, process_rank, num_processes):
        self.process_rank = process_rank
        self.num_processes = num_processes
        self.B = B
        self.T = T

        # glob files that match the pattern
        self.files = sorted(glob.glob(filename_pattern))
        assert len(self.files) > 0, f"did not find any files that match the pattern {filename_pattern}"

        # load and validate all data shards, count number of tokens in total
        ntok_total = 0
        for fname in self.files:
            shard_ntok = _peek_data_shard(fname)
            assert shard_ntok >= num_processes * B * T + 1
            ntok_total += int(shard_ntok)
        self.ntok_total = ntok_total

        # kick things off
        self.reset()

    def reset(self):
        self.current_shard = 0
        self.current_position = self.process_rank * self.B * self.T
        self.tokens = _load_data_shard(self.files[self.current_shard])

    def advance(self): # advance to next data shard
        self.current_shard = (self.current_shard + 1) % len(self.files)
        self.current_position = self.process_rank * self.B * self.T
        self.tokens = _load_data_shard(self.files[self.current_shard])

    def next_batch(self):
        B = self.B
        T = self.T
        buf = self.tokens[self.current_position : self.current_position+B*T+1]
        buf = torch.tensor(buf.astype(np.int32), dtype=torch.long)
        x = (buf[:-1]).view(B, T) # inputs
        y = (buf[1:]).view(B, T) # targets
        # advance current position and load next shard if necessary
        self.current_position += B * T * self.num_processes
        if self.current_position + (B * T * self.num_processes + 1) > len(self.tokens):
            self.advance()
        return x.cuda(), y.cuda()

# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data hyperparams
    input_bin : str = 'data/fineweb10B/fineweb_train_*.bin' # input .bin to train on
    input_val_bin : str = 'data/fineweb10B/fineweb_val_*.bin' # input .bin to eval validation loss on
    # optimization hyperparams
    batch_size : int = 8*64 # batch size, in sequences, across all devices
    device_batch_size : int = 64 # batch size, in sequences, per device
    sequence_length : int = 1024 # sequence length, in tokens
    num_iterations : int = 3242 # number of iterations to run
    warmup_iters : int = 0
    warmdown_iters : int = 926 # number of iterations of linear warmup/warmdown for triangular or trapezoidal schedule
    weight_decay : float = 0
    # evaluation and logging hyperparams
    val_loss_every : int = 125 # every how many steps to evaluate val loss? 0 for only at the end
    val_tokens : int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    save_every : int = 0 # every how many steps to save the checkpoint? 0 for only at the end
args = Hyperparameters()

# set up DDP (distributed data parallel). torchrun sets this env variable
assert torch.cuda.is_available()
dist.init_process_group(backend='nccl')
ddp_rank = int(os.environ['RANK'])
ddp_local_rank = int(os.environ['LOCAL_RANK'])
ddp_world_size = int(os.environ['WORLD_SIZE'])
device = f'cuda:{ddp_local_rank}'
torch.cuda.set_device(device)
print(f"using device: {device}")
master_process = (ddp_rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = str(uuid.uuid4())
    logdir = 'logs/%s/' % run_id
    os.makedirs(logdir, exist_ok=True)
    logfile = 'logs/%s.txt' % run_id
    # create the log file
    with open(logfile, "w") as f:
        # begin the log by printing this file (the Python code)
        f.write('='*100 + '\n')
        f.write(code)
        f.write('='*100 + '\n')
def print0(s, logonly=False):
    if master_process:
        with open(logfile, "a") as f:
            if not logonly:
                print(s)
            f.write(s+'\n')
# log information about the hardware/software environment this is running on
# and print the full `nvidia-smi` to file
print0(f"Running pytorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}\nnvidia-smi:")
import subprocess
result = subprocess.run(['nvidia-smi'], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)
print0(f'{result.stdout}', logonly=True)
print0('='*100, logonly=True)

# convenience variables
B, T = args.device_batch_size, args.sequence_length
# calculate the number of steps to take in the val loop.
assert args.val_tokens % (B * T * ddp_world_size) == 0
val_steps = args.val_tokens // (B * T * ddp_world_size)
# calculate the steps of gradient accumulation required to attain the desired global batch size.
assert args.batch_size % (B * ddp_world_size) == 0
train_accumulation_steps = args.batch_size // (B * ddp_world_size)

# load tokens
train_loader = DistributedDataLoader(args.input_bin, B, T, ddp_rank, ddp_world_size)
val_loader = DistributedDataLoader(args.input_val_bin, B, T, ddp_rank, ddp_world_size)
print0(f"Training DataLoader: total number of tokens: {train_loader.ntok_total} across {len(train_loader.files)} files")
print0(f"Validation DataLoader: total number of tokens: {val_loader.ntok_total} across {len(val_loader.files)} files")
print0('='*100, logonly=True)
x, y = train_loader.next_batch()

# there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency. suggested to me by @Grad62304977.
# this originates from Karpathy's experiments.
num_vocab = 50304
model = GPT(GPTConfig(vocab_size=num_vocab, n_layer=10, n_head=6, n_embd=768, vocab_embed=512))
model = model.cuda().bfloat16()
trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
print0(f"number of parameters: {trainable_params}")

for m in model.modules():
    if isinstance(m, CastedLinear):
        m.float()
if hasattr(config, "coordinate_descent_tuning"):
    config.coordinate_descent_tuning = True # suggested by @Chillee
model = torch.compile(model)
# here we wrap model into DDP container
model = DDP(model, device_ids=[ddp_local_rank])
raw_model = model.module # always contains the "raw" unwrapped model

# CUDNN attention is ~4ms faster than Flash, but doesn't get selected by default in PyTorch 2.5.1
from torch.backends.cuda import enable_cudnn_sdp, enable_flash_sdp, enable_math_sdp, enable_mem_efficient_sdp
enable_cudnn_sdp(True)
enable_flash_sdp(False)
enable_mem_efficient_sdp(False)
enable_math_sdp(False)

# init the optimizer(s)
optimizer1 = torch.optim.Adam([raw_model.transformer.wte.weight, *list(raw_model.proj.proj_in.parameters())], lr=0.2,   betas=(0.9, 0.95), fused=True)
optimizer2 = torch.optim.Adam([raw_model.lm_head.weight, *list(raw_model.proj.proj_out.parameters())],         lr=0.005, betas=(0.9, 0.95), fused=True)
params = list(raw_model.transformer.h.parameters())
matrix_params = [p for p in params if p.ndim == 2]
scalar_params = [p for p in params if p.ndim < 2] + [raw_model.skip_weights]
optimizer3 = Muon(matrix_params, lr=0.08, momentum=0.95)
optimizer4 = torch.optim.Adam(scalar_params, lr=0.08, betas=(0.9, 0.95), fused=True) # note that this learning rate is neither sensitive nor tuned
optimizers = [optimizer1, optimizer2, optimizer3, optimizer4]

optimized_params = set()
for opt in optimizers:
    for group in opt.param_groups:
        for p in group['params']:
            optimized_params.add(p)

for name, param in model.named_parameters():
    if param not in optimized_params:
        print0(f"WARNING: Parameter `{name}` is not included in any optimizer.")

# learning rate decay scheduler (linear warmup and warmdown)
def get_lr(it):
    assert it <= args.num_iterations
    # 1) linear warmup for warmup_iters steps
    if it < args.warmup_iters:
        return (it+1) / args.warmup_iters
    # 2) constant lr for a while
    elif it < args.num_iterations - args.warmdown_iters:
        return 1.0
    # 3) linear warmdown
    else:
        decay_ratio = (args.num_iterations - it) / args.warmdown_iters
        return decay_ratio
schedulers = [torch.optim.lr_scheduler.LambdaLR(opt, get_lr) for opt in optimizers]

# Start training loop
training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.time()
# begin training
train_loader.reset()
for step in range(args.num_iterations + 1):
    last_step = (step == args.num_iterations)
    # This effectively ignores timing first 10 steps, which are slower for weird reasons.
    # Alternately, and slightly more correctly in terms of benchmarking, we could do 10
    # steps with dummy data first, and then re-initialize the model and reset the loader.
    if step == 10:
        training_time_ms = 0
        t0 = time.time()
    timed_steps = float('nan') if step <= 11 else (step - 10) + 1 # <= 11 to avoid bug in val

    # once in a while evaluate the validation dataset
    if (last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0)):
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.time() - t0)
        # run validation batches
        model.eval()
        val_loader.reset()
        val_loss = 0.0
        for _ in range(val_steps):
            with torch.no_grad():
                x_val, y_val = val_loader.next_batch()
                val_loss += model(x_val, y_val)
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        val_loss /= val_steps
        # log val loss to console and to logfile
        print0(f'step:{step}/{args.num_iterations} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/(timed_steps-1):.2f}ms')
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.time()

    if master_process and (last_step or (args.save_every > 0 and step % args.save_every == 0)):
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.time() - t0)
        # save the state of the training process
        log = dict(step=step, code=code, model=raw_model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
        torch.save(log, 'logs/%s/state_step%06d.pt' % (run_id, step))
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.time()

    # bit confusing: we want to make sure to eval on 0th iteration
    # but also after the very last iteration. so we loop for step <= num_iterations
    # instead of just < num_iterations (one extra due to <=), only to do
    # the validation/sampling one last time, and then we break right here as we're done.
    if last_step:
        break

    # --------------- TRAINING SECTION BEGIN -----------------
    model.train()
    for i in range(1, train_accumulation_steps+1):
        # forward pass
        loss = model(x, y)
        train_loss = loss.detach()
        # advance the dataset for the next batch
        x, y = train_loader.next_batch()
        # backward pass
        if i < train_accumulation_steps:
            with model.no_sync(): # there's no need to sync gradients every accumulation step
                loss.backward()
        else:
            loss.backward() # just sync on the last step
    for name, p in model.named_parameters():
        if p.grad is not None:
            p.grad /= train_accumulation_steps
        else:
            raise ValueError(f"Parameter `{name}` has no gradient and was skipped during normalization.")
    # momentum warmup for Muon
    frac = min(step/500, 1)
    optimizer3.param_groups[0]['momentum'] = (1 - frac) * 0.85 + frac * 0.95
    # step the optimizers and schedulers
    for opt, sched in zip(optimizers, schedulers):
        opt.step()
        sched.step()
    # null the gradients
    model.zero_grad(set_to_none=True)
    # --------------- TRAINING SECTION END -------------------
    # everything that follows now is just diagnostics, prints, logging, etc.

    #dist.all_reduce(train_loss, op=dist.ReduceOp.AVG) # all-reducing the training loss would be more correct in terms of logging, but slower
    approx_time = training_time_ms + 1000 * (time.time() - t0)
    print0(f"step:{step+1}/{args.num_iterations} train_loss:{train_loss.item():.4f} train_time:{approx_time:.0f}ms step_avg:{approx_time/timed_steps:.2f}ms")

if master_process:
    print(f"peak memory consumption: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB")

# -------------------------------------------------------------------------
# clean up nice
dist.destroy_process_group()
====================================================================================================
Running pytorch 2.5.1+cu124 compiled for CUDA 12.4
nvidia-smi:
Tue Nov 12 22:51:30 2024       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 550.90.12              Driver Version: 550.90.12      CUDA Version: 12.6     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA H100 80GB HBM3          On  |   00000000:61:00.0 Off |                    0 |
| N/A   43C    P0             83W /  700W |       4MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  |   00000000:62:00.0 Off |                    0 |
| N/A   40C    P0             91W /  700W |       4MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  |   00000000:63:00.0 Off |                    0 |
| N/A   41C    P0             78W /  700W |       4MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  |   00000000:64:00.0 Off |                    0 |
| N/A   37C    P0             88W /  700W |       4MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  |   00000000:6A:00.0 Off |                    0 |
| N/A   36C    P0             76W /  700W |       4MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  |   00000000:6B:00.0 Off |                    0 |
| N/A   41C    P0            123W /  700W |      23MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  |   00000000:6C:00.0 Off |                    0 |
| N/A   39C    P0             72W /  700W |       4MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  |   00000000:6D:00.0 Off |                    0 |
| N/A   34C    P0             76W /  700W |       4MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
+-----------------------------------------------------------------------------------------+

====================================================================================================
Training DataLoader: total number of tokens: 2000000000 across 20 files
Validation DataLoader: total number of tokens: 100000000 across 1 files
====================================================================================================
number of parameters: 123076643
step:0/3242 val_loss:10.8258 train_time:224ms step_avg:nanms
step:1/3242 train_loss:10.8258 train_time:83841ms step_avg:nanms
step:2/3242 train_loss:9.1708 train_time:83919ms step_avg:nanms
step:3/3242 train_loss:8.7273 train_time:84033ms step_avg:nanms
step:4/3242 train_loss:8.3409 train_time:84150ms step_avg:nanms
step:5/3242 train_loss:8.0715 train_time:84268ms step_avg:nanms
step:6/3242 train_loss:8.1322 train_time:84388ms step_avg:nanms
step:7/3242 train_loss:7.7872 train_time:84507ms step_avg:nanms
step:8/3242 train_loss:8.1218 train_time:84626ms step_avg:nanms
step:9/3242 train_loss:7.7378 train_time:84750ms step_avg:nanms
step:10/3242 train_loss:7.4795 train_time:84879ms step_avg:nanms
step:11/3242 train_loss:7.4290 train_time:74ms step_avg:nanms
step:12/3242 train_loss:7.3295 train_time:193ms step_avg:nanms
step:13/3242 train_loss:7.1325 train_time:312ms step_avg:104.07ms
step:14/3242 train_loss:7.1381 train_time:430ms step_avg:107.41ms
step:15/3242 train_loss:7.0805 train_time:549ms step_avg:109.71ms
step:16/3242 train_loss:6.9711 train_time:668ms step_avg:111.30ms
step:17/3242 train_loss:6.9375 train_time:788ms step_avg:112.63ms
step:18/3242 train_loss:6.9501 train_time:912ms step_avg:113.99ms
step:19/3242 train_loss:6.7894 train_time:1033ms step_avg:114.78ms
step:20/3242 train_loss:6.8036 train_time:1153ms step_avg:115.25ms
step:21/3242 train_loss:6.4787 train_time:1272ms step_avg:115.64ms
step:22/3242 train_loss:6.8223 train_time:1391ms step_avg:115.93ms
step:23/3242 train_loss:7.0721 train_time:1510ms step_avg:116.17ms
step:24/3242 train_loss:6.7128 train_time:1629ms step_avg:116.33ms
step:25/3242 train_loss:6.8181 train_time:1748ms step_avg:116.54ms
step:26/3242 train_loss:6.5449 train_time:1869ms step_avg:116.84ms
step:27/3242 train_loss:6.4671 train_time:1992ms step_avg:117.20ms
step:28/3242 train_loss:6.6281 train_time:2112ms step_avg:117.35ms
step:29/3242 train_loss:6.3023 train_time:2232ms step_avg:117.48ms
step:30/3242 train_loss:6.5671 train_time:2353ms step_avg:117.63ms
step:31/3242 train_loss:6.4310 train_time:2471ms step_avg:117.68ms
step:32/3242 train_loss:6.3791 train_time:2589ms step_avg:117.70ms
step:33/3242 train_loss:6.2102 train_time:2708ms step_avg:117.73ms
step:34/3242 train_loss:6.6236 train_time:2827ms step_avg:117.81ms
step:35/3242 train_loss:6.4583 train_time:2950ms step_avg:118.01ms
step:36/3242 train_loss:6.6375 train_time:3071ms step_avg:118.12ms
step:37/3242 train_loss:6.5461 train_time:3191ms step_avg:118.19ms
step:38/3242 train_loss:6.4168 train_time:3311ms step_avg:118.23ms
step:39/3242 train_loss:6.3157 train_time:3429ms step_avg:118.23ms
step:40/3242 train_loss:6.3852 train_time:3548ms step_avg:118.27ms
step:41/3242 train_loss:6.2788 train_time:3668ms step_avg:118.33ms
step:42/3242 train_loss:6.3207 train_time:3788ms step_avg:118.37ms
step:43/3242 train_loss:6.1914 train_time:3909ms step_avg:118.45ms
step:44/3242 train_loss:6.2645 train_time:4030ms step_avg:118.52ms
step:45/3242 train_loss:6.2647 train_time:4151ms step_avg:118.59ms
step:46/3242 train_loss:6.4452 train_time:4270ms step_avg:118.62ms
step:47/3242 train_loss:6.2575 train_time:4389ms step_avg:118.61ms
step:48/3242 train_loss:6.1136 train_time:4509ms step_avg:118.66ms
step:49/3242 train_loss:6.3191 train_time:4628ms step_avg:118.67ms
step:50/3242 train_loss:6.2098 train_time:4748ms step_avg:118.70ms
step:51/3242 train_loss:6.3665 train_time:4868ms step_avg:118.74ms
step:52/3242 train_loss:6.2085 train_time:4989ms step_avg:118.80ms
step:53/3242 train_loss:6.0586 train_time:5110ms step_avg:118.85ms
step:54/3242 train_loss:6.1842 train_time:5231ms step_avg:118.88ms
step:55/3242 train_loss:6.1039 train_time:5349ms step_avg:118.88ms
step:56/3242 train_loss:6.4082 train_time:5470ms step_avg:118.92ms
step:57/3242 train_loss:6.0845 train_time:5588ms step_avg:118.90ms
step:58/3242 train_loss:5.9686 train_time:5708ms step_avg:118.92ms
step:59/3242 train_loss:6.1415 train_time:5828ms step_avg:118.95ms
step:60/3242 train_loss:6.0668 train_time:5949ms step_avg:118.99ms
step:61/3242 train_loss:6.1919 train_time:6069ms step_avg:119.01ms
step:62/3242 train_loss:5.9504 train_time:6189ms step_avg:119.01ms
step:63/3242 train_loss:6.0359 train_time:6309ms step_avg:119.04ms
step:64/3242 train_loss:5.9940 train_time:6429ms step_avg:119.05ms
step:65/3242 train_loss:6.1552 train_time:6549ms step_avg:119.08ms
step:66/3242 train_loss:5.8344 train_time:6669ms step_avg:119.09ms
step:67/3242 train_loss:5.9953 train_time:6789ms step_avg:119.10ms
step:68/3242 train_loss:5.8538 train_time:6909ms step_avg:119.12ms
step:69/3242 train_loss:6.1554 train_time:7030ms step_avg:119.15ms
step:70/3242 train_loss:5.7737 train_time:7150ms step_avg:119.16ms
step:71/3242 train_loss:5.8731 train_time:7269ms step_avg:119.16ms
step:72/3242 train_loss:6.0419 train_time:7390ms step_avg:119.19ms
step:73/3242 train_loss:5.9115 train_time:7510ms step_avg:119.20ms
step:74/3242 train_loss:5.8222 train_time:7628ms step_avg:119.19ms
step:75/3242 train_loss:5.9240 train_time:7749ms step_avg:119.22ms
step:76/3242 train_loss:5.9284 train_time:7869ms step_avg:119.23ms
step:77/3242 train_loss:5.8770 train_time:7989ms step_avg:119.24ms
step:78/3242 train_loss:5.9430 train_time:8110ms step_avg:119.26ms
step:79/3242 train_loss:6.1046 train_time:8229ms step_avg:119.25ms
step:80/3242 train_loss:5.8477 train_time:8348ms step_avg:119.26ms
step:81/3242 train_loss:5.9250 train_time:8469ms step_avg:119.28ms
step:82/3242 train_loss:5.6687 train_time:8588ms step_avg:119.28ms
step:83/3242 train_loss:5.8538 train_time:8708ms step_avg:119.29ms
step:84/3242 train_loss:5.8279 train_time:8829ms step_avg:119.31ms
step:85/3242 train_loss:5.7540 train_time:8950ms step_avg:119.33ms
step:86/3242 train_loss:5.6297 train_time:9070ms step_avg:119.34ms
step:87/3242 train_loss:5.8235 train_time:9188ms step_avg:119.33ms
step:88/3242 train_loss:5.7605 train_time:9309ms step_avg:119.35ms
step:89/3242 train_loss:5.8559 train_time:9429ms step_avg:119.35ms
step:90/3242 train_loss:5.8216 train_time:9549ms step_avg:119.36ms
step:91/3242 train_loss:5.7001 train_time:9669ms step_avg:119.37ms
step:92/3242 train_loss:5.6907 train_time:9789ms step_avg:119.38ms
step:93/3242 train_loss:5.7931 train_time:9910ms step_avg:119.40ms
step:94/3242 train_loss:5.6651 train_time:10030ms step_avg:119.41ms
step:95/3242 train_loss:5.6263 train_time:10150ms step_avg:119.41ms
step:96/3242 train_loss:5.6577 train_time:10269ms step_avg:119.41ms
step:97/3242 train_loss:5.5596 train_time:10388ms step_avg:119.41ms
step:98/3242 train_loss:5.6392 train_time:10510ms step_avg:119.43ms
step:99/3242 train_loss:5.5576 train_time:10628ms step_avg:119.42ms
step:100/3242 train_loss:5.7144 train_time:10749ms step_avg:119.44ms
step:101/3242 train_loss:5.6492 train_time:10870ms step_avg:119.45ms
step:102/3242 train_loss:5.5553 train_time:10990ms step_avg:119.45ms
step:103/3242 train_loss:5.6570 train_time:11109ms step_avg:119.45ms
step:104/3242 train_loss:5.6479 train_time:11229ms step_avg:119.46ms
step:105/3242 train_loss:5.4624 train_time:11348ms step_avg:119.46ms
step:106/3242 train_loss:5.5658 train_time:11468ms step_avg:119.46ms
step:107/3242 train_loss:5.7723 train_time:11588ms step_avg:119.47ms
step:108/3242 train_loss:5.5571 train_time:11709ms step_avg:119.48ms
step:109/3242 train_loss:5.2933 train_time:11829ms step_avg:119.49ms
step:110/3242 train_loss:5.5085 train_time:11950ms step_avg:119.50ms
step:111/3242 train_loss:5.4994 train_time:12070ms step_avg:119.51ms
step:112/3242 train_loss:5.4628 train_time:12189ms step_avg:119.50ms
step:113/3242 train_loss:5.5626 train_time:12308ms step_avg:119.50ms
step:114/3242 train_loss:5.4810 train_time:12429ms step_avg:119.51ms
step:115/3242 train_loss:5.3140 train_time:12549ms step_avg:119.51ms
step:116/3242 train_loss:5.5195 train_time:12670ms step_avg:119.53ms
step:117/3242 train_loss:5.3533 train_time:12790ms step_avg:119.53ms
step:118/3242 train_loss:5.3503 train_time:12909ms step_avg:119.53ms
step:119/3242 train_loss:5.4917 train_time:13029ms step_avg:119.53ms
step:120/3242 train_loss:5.5049 train_time:13149ms step_avg:119.53ms
step:121/3242 train_loss:5.4030 train_time:13269ms step_avg:119.54ms
step:122/3242 train_loss:5.2848 train_time:13389ms step_avg:119.54ms
step:123/3242 train_loss:5.3731 train_time:13509ms step_avg:119.55ms
step:124/3242 train_loss:5.2510 train_time:13629ms step_avg:119.55ms
step:125/3242 train_loss:5.5466 train_time:13749ms step_avg:119.56ms
step:125/3242 val_loss:5.3790 train_time:13797ms step_avg:119.97ms
step:126/3242 train_loss:5.3912 train_time:13869ms step_avg:119.56ms
step:127/3242 train_loss:5.3556 train_time:14005ms step_avg:119.70ms
step:128/3242 train_loss:5.4406 train_time:14129ms step_avg:119.73ms
step:129/3242 train_loss:5.2925 train_time:14248ms step_avg:119.73ms
step:130/3242 train_loss:5.5875 train_time:14367ms step_avg:119.72ms
step:131/3242 train_loss:5.3890 train_time:14485ms step_avg:119.71ms
step:132/3242 train_loss:5.3737 train_time:14604ms step_avg:119.71ms
step:133/3242 train_loss:5.3119 train_time:14724ms step_avg:119.70ms
step:134/3242 train_loss:5.3363 train_time:14843ms step_avg:119.70ms
step:135/3242 train_loss:5.3002 train_time:14969ms step_avg:119.75ms
step:136/3242 train_loss:5.3355 train_time:15094ms step_avg:119.79ms
step:137/3242 train_loss:5.1395 train_time:15215ms step_avg:119.81ms
step:138/3242 train_loss:5.2955 train_time:15336ms step_avg:119.82ms
step:139/3242 train_loss:5.2750 train_time:15458ms step_avg:119.83ms
step:140/3242 train_loss:5.2807 train_time:15576ms step_avg:119.82ms
step:141/3242 train_loss:5.3228 train_time:15695ms step_avg:119.81ms
step:142/3242 train_loss:5.2257 train_time:15816ms step_avg:119.82ms
step:143/3242 train_loss:5.2954 train_time:15942ms step_avg:119.86ms
step:144/3242 train_loss:5.1127 train_time:16065ms step_avg:119.89ms
step:145/3242 train_loss:5.2550 train_time:16186ms step_avg:119.90ms
step:146/3242 train_loss:5.1982 train_time:16306ms step_avg:119.90ms
step:147/3242 train_loss:5.1182 train_time:16427ms step_avg:119.91ms
step:148/3242 train_loss:5.2277 train_time:16547ms step_avg:119.91ms
step:149/3242 train_loss:5.2188 train_time:16667ms step_avg:119.90ms
step:150/3242 train_loss:5.2880 train_time:16788ms step_avg:119.91ms
step:151/3242 train_loss:5.3166 train_time:16909ms step_avg:119.93ms
step:152/3242 train_loss:5.1886 train_time:17030ms step_avg:119.93ms
step:153/3242 train_loss:5.1710 train_time:17150ms step_avg:119.93ms
step:154/3242 train_loss:5.2290 train_time:17272ms step_avg:119.95ms
step:155/3242 train_loss:5.1959 train_time:17394ms step_avg:119.96ms
step:156/3242 train_loss:5.1672 train_time:17513ms step_avg:119.95ms
step:157/3242 train_loss:5.1816 train_time:17634ms step_avg:119.96ms
step:158/3242 train_loss:5.3095 train_time:17754ms step_avg:119.96ms
step:159/3242 train_loss:5.0897 train_time:17874ms step_avg:119.96ms
step:160/3242 train_loss:5.1572 train_time:17995ms step_avg:119.96ms
step:161/3242 train_loss:4.9977 train_time:18117ms step_avg:119.98ms
step:162/3242 train_loss:5.1561 train_time:18241ms step_avg:120.01ms
step:163/3242 train_loss:5.1890 train_time:18364ms step_avg:120.02ms
step:164/3242 train_loss:5.1935 train_time:18485ms step_avg:120.03ms
step:165/3242 train_loss:4.9887 train_time:18605ms step_avg:120.03ms
step:166/3242 train_loss:5.1166 train_time:18727ms step_avg:120.04ms
step:167/3242 train_loss:5.2847 train_time:18849ms step_avg:120.05ms
step:168/3242 train_loss:5.0554 train_time:18970ms step_avg:120.06ms
step:169/3242 train_loss:5.0992 train_time:19090ms step_avg:120.07ms
step:170/3242 train_loss:5.0086 train_time:19212ms step_avg:120.07ms
step:171/3242 train_loss:4.9530 train_time:19334ms step_avg:120.09ms
step:172/3242 train_loss:5.0480 train_time:19456ms step_avg:120.10ms
step:173/3242 train_loss:5.0137 train_time:19577ms step_avg:120.11ms
step:174/3242 train_loss:5.0846 train_time:19700ms step_avg:120.12ms
step:175/3242 train_loss:5.2375 train_time:19824ms step_avg:120.14ms
step:176/3242 train_loss:5.1016 train_time:19945ms step_avg:120.15ms
step:177/3242 train_loss:4.9362 train_time:20066ms step_avg:120.16ms
step:178/3242 train_loss:4.9066 train_time:20189ms step_avg:120.17ms
step:179/3242 train_loss:4.9523 train_time:20310ms step_avg:120.18ms
step:180/3242 train_loss:4.9931 train_time:20432ms step_avg:120.19ms
step:181/3242 train_loss:4.9828 train_time:20554ms step_avg:120.20ms
step:182/3242 train_loss:5.0958 train_time:20676ms step_avg:120.21ms
step:183/3242 train_loss:4.9595 train_time:20799ms step_avg:120.23ms
step:184/3242 train_loss:4.9109 train_time:20925ms step_avg:120.26ms
step:185/3242 train_loss:4.9335 train_time:21046ms step_avg:120.26ms
step:186/3242 train_loss:5.0469 train_time:21168ms step_avg:120.27ms
step:187/3242 train_loss:4.9401 train_time:21290ms step_avg:120.28ms
step:188/3242 train_loss:5.1674 train_time:21410ms step_avg:120.28ms
step:189/3242 train_loss:4.9726 train_time:21708ms step_avg:121.27ms
step:190/3242 train_loss:4.8860 train_time:22035ms step_avg:122.42ms
step:191/3242 train_loss:5.0465 train_time:22152ms step_avg:122.39ms
step:192/3242 train_loss:4.8695 train_time:22272ms step_avg:122.37ms
step:193/3242 train_loss:4.8087 train_time:22392ms step_avg:122.36ms
step:194/3242 train_loss:5.0119 train_time:22511ms step_avg:122.34ms
step:195/3242 train_loss:4.9313 train_time:22632ms step_avg:122.33ms
step:196/3242 train_loss:5.1474 train_time:22750ms step_avg:122.31ms
step:197/3242 train_loss:4.9868 train_time:22871ms step_avg:122.30ms
step:198/3242 train_loss:4.8613 train_time:23003ms step_avg:122.35ms
step:199/3242 train_loss:4.8919 train_time:23126ms step_avg:122.36ms
step:200/3242 train_loss:4.7765 train_time:23248ms step_avg:122.36ms
step:201/3242 train_loss:4.8838 train_time:23369ms step_avg:122.35ms
step:202/3242 train_loss:4.7816 train_time:23490ms step_avg:122.34ms
step:203/3242 train_loss:5.0021 train_time:23610ms step_avg:122.33ms
step:204/3242 train_loss:4.8503 train_time:23730ms step_avg:122.32ms
step:205/3242 train_loss:4.9002 train_time:23851ms step_avg:122.32ms
step:206/3242 train_loss:5.0014 train_time:23977ms step_avg:122.33ms
step:207/3242 train_loss:4.6906 train_time:24103ms step_avg:122.35ms
step:208/3242 train_loss:4.8266 train_time:24226ms step_avg:122.35ms
step:209/3242 train_loss:4.8126 train_time:24347ms step_avg:122.35ms
step:210/3242 train_loss:4.9725 train_time:24470ms step_avg:122.35ms
step:211/3242 train_loss:4.8952 train_time:24592ms step_avg:122.35ms
step:212/3242 train_loss:4.7881 train_time:24712ms step_avg:122.33ms
step:213/3242 train_loss:4.8721 train_time:24833ms step_avg:122.33ms
step:214/3242 train_loss:4.7403 train_time:24957ms step_avg:122.34ms
step:215/3242 train_loss:4.8407 train_time:25084ms step_avg:122.36ms
step:216/3242 train_loss:4.6464 train_time:25206ms step_avg:122.36ms
step:217/3242 train_loss:4.7599 train_time:25328ms step_avg:122.36ms
step:218/3242 train_loss:4.7621 train_time:25450ms step_avg:122.36ms
step:219/3242 train_loss:4.7593 train_time:25572ms step_avg:122.35ms
step:220/3242 train_loss:4.7819 train_time:25692ms step_avg:122.34ms
step:221/3242 train_loss:4.7787 train_time:25813ms step_avg:122.34ms
step:222/3242 train_loss:4.8142 train_time:25937ms step_avg:122.34ms
step:223/3242 train_loss:4.7519 train_time:26061ms step_avg:122.35ms
step:224/3242 train_loss:4.7052 train_time:26186ms step_avg:122.36ms
step:225/3242 train_loss:4.9355 train_time:26309ms step_avg:122.37ms
step:226/3242 train_loss:4.5564 train_time:26431ms step_avg:122.36ms
step:227/3242 train_loss:4.6605 train_time:26553ms step_avg:122.36ms
step:228/3242 train_loss:4.6596 train_time:26675ms step_avg:122.36ms
step:229/3242 train_loss:4.8136 train_time:26796ms step_avg:122.36ms
step:230/3242 train_loss:4.6201 train_time:26919ms step_avg:122.36ms
step:231/3242 train_loss:4.7517 train_time:27043ms step_avg:122.37ms
step:232/3242 train_loss:4.6109 train_time:27168ms step_avg:122.38ms
step:233/3242 train_loss:4.6147 train_time:27291ms step_avg:122.38ms
step:234/3242 train_loss:4.7847 train_time:27413ms step_avg:122.38ms
step:235/3242 train_loss:4.6715 train_time:27535ms step_avg:122.38ms
step:236/3242 train_loss:4.5469 train_time:27657ms step_avg:122.38ms
step:237/3242 train_loss:4.7852 train_time:27780ms step_avg:122.38ms
step:238/3242 train_loss:4.7281 train_time:27904ms step_avg:122.38ms
step:239/3242 train_loss:4.6002 train_time:28027ms step_avg:122.39ms
step:240/3242 train_loss:4.7519 train_time:28150ms step_avg:122.39ms
step:241/3242 train_loss:4.7809 train_time:28273ms step_avg:122.39ms
step:242/3242 train_loss:4.6171 train_time:28397ms step_avg:122.40ms
step:243/3242 train_loss:4.8140 train_time:28520ms step_avg:122.40ms
step:244/3242 train_loss:4.6682 train_time:28643ms step_avg:122.41ms
step:245/3242 train_loss:4.6936 train_time:28765ms step_avg:122.41ms
step:246/3242 train_loss:4.7660 train_time:28888ms step_avg:122.41ms
step:247/3242 train_loss:4.7027 train_time:29010ms step_avg:122.41ms
step:248/3242 train_loss:4.6460 train_time:29133ms step_avg:122.41ms
step:249/3242 train_loss:4.7848 train_time:29256ms step_avg:122.41ms
step:250/3242 train_loss:4.5592 train_time:29382ms step_avg:122.43ms
step:250/3242 val_loss:4.6387 train_time:29429ms step_avg:122.62ms
step:251/3242 train_loss:4.5826 train_time:29507ms step_avg:122.43ms
step:252/3242 train_loss:4.7111 train_time:29643ms step_avg:122.49ms
step:253/3242 train_loss:4.7400 train_time:29767ms step_avg:122.50ms
step:254/3242 train_loss:4.5827 train_time:29889ms step_avg:122.50ms
step:255/3242 train_loss:4.5478 train_time:30010ms step_avg:122.49ms
step:256/3242 train_loss:4.6901 train_time:30131ms step_avg:122.48ms
step:257/3242 train_loss:4.6423 train_time:30252ms step_avg:122.48ms
step:258/3242 train_loss:4.6403 train_time:30376ms step_avg:122.48ms
step:259/3242 train_loss:4.5659 train_time:30502ms step_avg:122.50ms
step:260/3242 train_loss:4.6014 train_time:30630ms step_avg:122.52ms
step:261/3242 train_loss:4.6514 train_time:30757ms step_avg:122.54ms
step:262/3242 train_loss:4.6396 train_time:30880ms step_avg:122.54ms
step:263/3242 train_loss:4.5954 train_time:31003ms step_avg:122.54ms
step:264/3242 train_loss:4.5263 train_time:31126ms step_avg:122.54ms
step:265/3242 train_loss:4.5829 train_time:31246ms step_avg:122.53ms
step:266/3242 train_loss:4.4338 train_time:31369ms step_avg:122.53ms
step:267/3242 train_loss:4.5118 train_time:31492ms step_avg:122.54ms
step:268/3242 train_loss:4.5411 train_time:31619ms step_avg:122.55ms
step:269/3242 train_loss:4.5167 train_time:31743ms step_avg:122.56ms
step:270/3242 train_loss:4.4774 train_time:31865ms step_avg:122.56ms
step:271/3242 train_loss:4.6813 train_time:31988ms step_avg:122.56ms
step:272/3242 train_loss:4.5947 train_time:32113ms step_avg:122.57ms
step:273/3242 train_loss:4.4707 train_time:32236ms step_avg:122.57ms
step:274/3242 train_loss:4.5241 train_time:32359ms step_avg:122.57ms
step:275/3242 train_loss:4.6332 train_time:32481ms step_avg:122.57ms
step:276/3242 train_loss:4.6520 train_time:32607ms step_avg:122.58ms
step:277/3242 train_loss:4.7938 train_time:32730ms step_avg:122.59ms
step:278/3242 train_loss:4.6034 train_time:32856ms step_avg:122.60ms
step:279/3242 train_loss:4.6850 train_time:32980ms step_avg:122.60ms
step:280/3242 train_loss:4.5796 train_time:33101ms step_avg:122.60ms
step:281/3242 train_loss:4.6341 train_time:33224ms step_avg:122.60ms
step:282/3242 train_loss:4.5225 train_time:33347ms step_avg:122.60ms
step:283/3242 train_loss:4.5679 train_time:33472ms step_avg:122.61ms
step:284/3242 train_loss:4.4819 train_time:33598ms step_avg:122.62ms
step:285/3242 train_loss:4.6314 train_time:33721ms step_avg:122.62ms
step:286/3242 train_loss:4.6256 train_time:33844ms step_avg:122.62ms
step:287/3242 train_loss:4.6667 train_time:33969ms step_avg:122.63ms
step:288/3242 train_loss:4.4854 train_time:34092ms step_avg:122.63ms
step:289/3242 train_loss:4.5889 train_time:34217ms step_avg:122.64ms
step:290/3242 train_loss:4.4153 train_time:34340ms step_avg:122.64ms
step:291/3242 train_loss:4.4337 train_time:34463ms step_avg:122.64ms
step:292/3242 train_loss:4.5236 train_time:34587ms step_avg:122.65ms
step:293/3242 train_loss:4.4457 train_time:34710ms step_avg:122.65ms
step:294/3242 train_loss:4.4738 train_time:34834ms step_avg:122.65ms
step:295/3242 train_loss:4.5226 train_time:34959ms step_avg:122.66ms
step:296/3242 train_loss:4.3955 train_time:35082ms step_avg:122.67ms
step:297/3242 train_loss:4.3897 train_time:35206ms step_avg:122.67ms
step:298/3242 train_loss:4.4033 train_time:35327ms step_avg:122.66ms
step:299/3242 train_loss:4.5108 train_time:35450ms step_avg:122.66ms
step:300/3242 train_loss:4.4013 train_time:35576ms step_avg:122.67ms
step:301/3242 train_loss:4.5533 train_time:35700ms step_avg:122.68ms
step:302/3242 train_loss:4.5514 train_time:35822ms step_avg:122.68ms
step:303/3242 train_loss:4.4734 train_time:35947ms step_avg:122.68ms
step:304/3242 train_loss:4.5225 train_time:36070ms step_avg:122.69ms
step:305/3242 train_loss:4.5253 train_time:36194ms step_avg:122.69ms
step:306/3242 train_loss:4.9673 train_time:36319ms step_avg:122.70ms
step:307/3242 train_loss:4.4722 train_time:36441ms step_avg:122.70ms
step:308/3242 train_loss:4.3818 train_time:36564ms step_avg:122.70ms
step:309/3242 train_loss:4.5534 train_time:36687ms step_avg:122.70ms
step:310/3242 train_loss:4.3773 train_time:36811ms step_avg:122.70ms
step:311/3242 train_loss:4.6173 train_time:36936ms step_avg:122.71ms
step:312/3242 train_loss:4.5033 train_time:37061ms step_avg:122.72ms
step:313/3242 train_loss:4.4113 train_time:37183ms step_avg:122.72ms
step:314/3242 train_loss:4.5421 train_time:37307ms step_avg:122.72ms
step:315/3242 train_loss:4.6530 train_time:37428ms step_avg:122.72ms
step:316/3242 train_loss:4.4944 train_time:37553ms step_avg:122.72ms
step:317/3242 train_loss:4.3415 train_time:37678ms step_avg:122.73ms
step:318/3242 train_loss:4.4211 train_time:37802ms step_avg:122.73ms
step:319/3242 train_loss:4.4253 train_time:37924ms step_avg:122.73ms
step:320/3242 train_loss:4.4068 train_time:38048ms step_avg:122.74ms
step:321/3242 train_loss:4.5102 train_time:38171ms step_avg:122.74ms
step:322/3242 train_loss:4.4838 train_time:38299ms step_avg:122.75ms
step:323/3242 train_loss:4.4378 train_time:38421ms step_avg:122.75ms
step:324/3242 train_loss:4.5326 train_time:38544ms step_avg:122.75ms
step:325/3242 train_loss:4.5035 train_time:38669ms step_avg:122.76ms
step:326/3242 train_loss:4.5864 train_time:38793ms step_avg:122.76ms
step:327/3242 train_loss:4.4230 train_time:38920ms step_avg:122.78ms
step:328/3242 train_loss:4.9246 train_time:39043ms step_avg:122.78ms
step:329/3242 train_loss:4.6051 train_time:39166ms step_avg:122.78ms
step:330/3242 train_loss:4.3515 train_time:39290ms step_avg:122.78ms
step:331/3242 train_loss:4.3135 train_time:39414ms step_avg:122.78ms
step:332/3242 train_loss:4.4907 train_time:39539ms step_avg:122.79ms
step:333/3242 train_loss:4.4101 train_time:39662ms step_avg:122.79ms
step:334/3242 train_loss:4.3960 train_time:39785ms step_avg:122.79ms
step:335/3242 train_loss:4.3483 train_time:39909ms step_avg:122.80ms
step:336/3242 train_loss:4.5421 train_time:40034ms step_avg:122.80ms
step:337/3242 train_loss:4.4662 train_time:40160ms step_avg:122.81ms
step:338/3242 train_loss:4.9359 train_time:40282ms step_avg:122.81ms
step:339/3242 train_loss:4.4587 train_time:40407ms step_avg:122.82ms
step:340/3242 train_loss:4.4199 train_time:40529ms step_avg:122.82ms
step:341/3242 train_loss:4.4310 train_time:40654ms step_avg:122.82ms
step:342/3242 train_loss:4.3612 train_time:40779ms step_avg:122.83ms
step:343/3242 train_loss:4.3336 train_time:40901ms step_avg:122.83ms
step:344/3242 train_loss:4.3856 train_time:41025ms step_avg:122.83ms
step:345/3242 train_loss:4.5131 train_time:41150ms step_avg:122.83ms
step:346/3242 train_loss:4.3841 train_time:41273ms step_avg:122.84ms
step:347/3242 train_loss:4.3068 train_time:41399ms step_avg:122.85ms
step:348/3242 train_loss:4.3343 train_time:41522ms step_avg:122.85ms
step:349/3242 train_loss:4.3769 train_time:41645ms step_avg:122.85ms
step:350/3242 train_loss:4.3245 train_time:41770ms step_avg:122.85ms
step:351/3242 train_loss:4.0170 train_time:41894ms step_avg:122.86ms
step:352/3242 train_loss:4.3077 train_time:42019ms step_avg:122.86ms
step:353/3242 train_loss:4.6881 train_time:42142ms step_avg:122.86ms
step:354/3242 train_loss:4.1865 train_time:42267ms step_avg:122.87ms
step:355/3242 train_loss:4.4335 train_time:42390ms step_avg:122.87ms
step:356/3242 train_loss:4.3018 train_time:42517ms step_avg:122.88ms
step:357/3242 train_loss:4.4167 train_time:42640ms step_avg:122.88ms
step:358/3242 train_loss:4.3800 train_time:42764ms step_avg:122.89ms
step:359/3242 train_loss:4.3439 train_time:42886ms step_avg:122.88ms
step:360/3242 train_loss:4.3759 train_time:43010ms step_avg:122.89ms
step:361/3242 train_loss:3.9701 train_time:43135ms step_avg:122.89ms
step:362/3242 train_loss:4.5344 train_time:43260ms step_avg:122.90ms
step:363/3242 train_loss:4.4375 train_time:43382ms step_avg:122.90ms
step:364/3242 train_loss:4.3356 train_time:43506ms step_avg:122.90ms
step:365/3242 train_loss:4.2601 train_time:43630ms step_avg:122.90ms
step:366/3242 train_loss:4.4102 train_time:43754ms step_avg:122.91ms
step:367/3242 train_loss:4.3638 train_time:43878ms step_avg:122.91ms
step:368/3242 train_loss:4.3604 train_time:44001ms step_avg:122.91ms
step:369/3242 train_loss:4.3434 train_time:44125ms step_avg:122.91ms
step:370/3242 train_loss:4.2309 train_time:44250ms step_avg:122.92ms
step:371/3242 train_loss:4.3991 train_time:44375ms step_avg:122.92ms
step:372/3242 train_loss:4.2985 train_time:44500ms step_avg:122.93ms
step:373/3242 train_loss:4.1823 train_time:44623ms step_avg:122.93ms
step:374/3242 train_loss:4.4009 train_time:44746ms step_avg:122.93ms
step:375/3242 train_loss:4.3350 train_time:44870ms step_avg:122.93ms
step:375/3242 val_loss:4.3336 train_time:44916ms step_avg:123.06ms
step:376/3242 train_loss:4.3086 train_time:44995ms step_avg:122.94ms
step:377/3242 train_loss:4.3790 train_time:45133ms step_avg:122.98ms
step:378/3242 train_loss:4.2667 train_time:45406ms step_avg:123.38ms
step:379/3242 train_loss:4.3366 train_time:45535ms step_avg:123.40ms
step:380/3242 train_loss:4.3902 train_time:45839ms step_avg:123.89ms
step:381/3242 train_loss:4.4488 train_time:45960ms step_avg:123.88ms
step:382/3242 train_loss:4.3612 train_time:46083ms step_avg:123.88ms
step:383/3242 train_loss:4.3353 train_time:46206ms step_avg:123.88ms
step:384/3242 train_loss:4.2662 train_time:46327ms step_avg:123.87ms
step:385/3242 train_loss:4.3644 train_time:46449ms step_avg:123.86ms
step:386/3242 train_loss:4.2769 train_time:46572ms step_avg:123.86ms
step:387/3242 train_loss:4.3927 train_time:46696ms step_avg:123.86ms
step:388/3242 train_loss:4.5878 train_time:46829ms step_avg:123.89ms
step:389/3242 train_loss:4.2984 train_time:46952ms step_avg:123.88ms
step:390/3242 train_loss:4.2674 train_time:47076ms step_avg:123.88ms
step:391/3242 train_loss:4.3797 train_time:47198ms step_avg:123.88ms
step:392/3242 train_loss:4.2971 train_time:47322ms step_avg:123.88ms
step:393/3242 train_loss:4.4016 train_time:47445ms step_avg:123.88ms
step:394/3242 train_loss:4.2333 train_time:47569ms step_avg:123.88ms
step:395/3242 train_loss:4.3753 train_time:47694ms step_avg:123.88ms
step:396/3242 train_loss:4.1144 train_time:47820ms step_avg:123.88ms
step:397/3242 train_loss:4.3173 train_time:47946ms step_avg:123.89ms
step:398/3242 train_loss:4.3846 train_time:48068ms step_avg:123.89ms
step:399/3242 train_loss:4.3558 train_time:48191ms step_avg:123.88ms
step:400/3242 train_loss:4.2843 train_time:48314ms step_avg:123.88ms
step:401/3242 train_loss:4.3307 train_time:48438ms step_avg:123.88ms
step:402/3242 train_loss:4.3924 train_time:48561ms step_avg:123.88ms
step:403/3242 train_loss:4.3359 train_time:48687ms step_avg:123.89ms
step:404/3242 train_loss:4.4462 train_time:48811ms step_avg:123.88ms
step:405/3242 train_loss:4.1951 train_time:48936ms step_avg:123.89ms
step:406/3242 train_loss:4.2646 train_time:49060ms step_avg:123.89ms
step:407/3242 train_loss:4.5464 train_time:49186ms step_avg:123.89ms
step:408/3242 train_loss:4.2871 train_time:49309ms step_avg:123.89ms
step:409/3242 train_loss:4.3118 train_time:49432ms step_avg:123.89ms
step:410/3242 train_loss:4.3477 train_time:49556ms step_avg:123.89ms
step:411/3242 train_loss:4.2297 train_time:49682ms step_avg:123.89ms
step:412/3242 train_loss:4.2504 train_time:49808ms step_avg:123.90ms
step:413/3242 train_loss:4.6619 train_time:49931ms step_avg:123.90ms
step:414/3242 train_loss:4.1137 train_time:50055ms step_avg:123.90ms
step:415/3242 train_loss:4.4947 train_time:50179ms step_avg:123.90ms
step:416/3242 train_loss:4.2352 train_time:50306ms step_avg:123.91ms
step:417/3242 train_loss:4.2534 train_time:50430ms step_avg:123.91ms
step:418/3242 train_loss:4.4215 train_time:50553ms step_avg:123.90ms
step:419/3242 train_loss:4.1695 train_time:50676ms step_avg:123.90ms
step:420/3242 train_loss:4.2789 train_time:50800ms step_avg:123.90ms
step:421/3242 train_loss:4.2238 train_time:50927ms step_avg:123.91ms
step:422/3242 train_loss:4.1188 train_time:51050ms step_avg:123.91ms
step:423/3242 train_loss:4.2416 train_time:51174ms step_avg:123.91ms
step:424/3242 train_loss:4.3411 train_time:51296ms step_avg:123.90ms
step:425/3242 train_loss:4.1041 train_time:51422ms step_avg:123.91ms
step:426/3242 train_loss:4.2997 train_time:51547ms step_avg:123.91ms
step:427/3242 train_loss:4.1810 train_time:51669ms step_avg:123.91ms
step:428/3242 train_loss:4.3713 train_time:51793ms step_avg:123.91ms
step:429/3242 train_loss:4.3102 train_time:51917ms step_avg:123.91ms
step:430/3242 train_loss:4.2250 train_time:52042ms step_avg:123.91ms
step:431/3242 train_loss:4.2046 train_time:52167ms step_avg:123.91ms
step:432/3242 train_loss:4.1191 train_time:52290ms step_avg:123.91ms
step:433/3242 train_loss:4.2402 train_time:52413ms step_avg:123.91ms
step:434/3242 train_loss:4.3172 train_time:52538ms step_avg:123.91ms
step:435/3242 train_loss:4.2371 train_time:52663ms step_avg:123.91ms
step:436/3242 train_loss:4.2872 train_time:52789ms step_avg:123.92ms
step:437/3242 train_loss:4.3038 train_time:52911ms step_avg:123.91ms
step:438/3242 train_loss:4.1661 train_time:53035ms step_avg:123.91ms
step:439/3242 train_loss:4.1929 train_time:53158ms step_avg:123.91ms
step:440/3242 train_loss:4.1627 train_time:53284ms step_avg:123.92ms
step:441/3242 train_loss:4.3496 train_time:53408ms step_avg:123.92ms
step:442/3242 train_loss:4.2419 train_time:53532ms step_avg:123.92ms
step:443/3242 train_loss:4.2239 train_time:53655ms step_avg:123.92ms
step:444/3242 train_loss:4.1117 train_time:53779ms step_avg:123.92ms
step:445/3242 train_loss:4.3767 train_time:53906ms step_avg:123.92ms
step:446/3242 train_loss:4.3019 train_time:54030ms step_avg:123.92ms
step:447/3242 train_loss:4.2973 train_time:54152ms step_avg:123.92ms
step:448/3242 train_loss:4.2143 train_time:54276ms step_avg:123.92ms
step:449/3242 train_loss:4.3110 train_time:54401ms step_avg:123.92ms
step:450/3242 train_loss:4.1430 train_time:54526ms step_avg:123.92ms
step:451/3242 train_loss:4.1707 train_time:54649ms step_avg:123.92ms
step:452/3242 train_loss:4.0486 train_time:54773ms step_avg:123.92ms
step:453/3242 train_loss:4.1721 train_time:54897ms step_avg:123.92ms
step:454/3242 train_loss:4.1368 train_time:55022ms step_avg:123.92ms
step:455/3242 train_loss:4.1148 train_time:55147ms step_avg:123.93ms
step:456/3242 train_loss:4.3184 train_time:55270ms step_avg:123.92ms
step:457/3242 train_loss:4.1868 train_time:55394ms step_avg:123.92ms
step:458/3242 train_loss:4.2634 train_time:55519ms step_avg:123.93ms
step:459/3242 train_loss:4.2958 train_time:55644ms step_avg:123.93ms
step:460/3242 train_loss:4.0878 train_time:55769ms step_avg:123.93ms
step:461/3242 train_loss:4.2684 train_time:55892ms step_avg:123.93ms
step:462/3242 train_loss:4.1788 train_time:56016ms step_avg:123.93ms
step:463/3242 train_loss:4.1675 train_time:56140ms step_avg:123.93ms
step:464/3242 train_loss:4.2511 train_time:56267ms step_avg:123.94ms
step:465/3242 train_loss:4.1881 train_time:56390ms step_avg:123.93ms
step:466/3242 train_loss:4.1769 train_time:56514ms step_avg:123.93ms
step:467/3242 train_loss:4.2913 train_time:56638ms step_avg:123.93ms
step:468/3242 train_loss:4.3021 train_time:56765ms step_avg:123.94ms
step:469/3242 train_loss:4.2710 train_time:56889ms step_avg:123.94ms
step:470/3242 train_loss:4.1647 train_time:57012ms step_avg:123.94ms
step:471/3242 train_loss:4.2566 train_time:57137ms step_avg:123.94ms
step:472/3242 train_loss:4.2905 train_time:57262ms step_avg:123.94ms
step:473/3242 train_loss:4.2332 train_time:57388ms step_avg:123.95ms
step:474/3242 train_loss:4.1874 train_time:57512ms step_avg:123.95ms
step:475/3242 train_loss:4.0503 train_time:57635ms step_avg:123.95ms
step:476/3242 train_loss:4.4778 train_time:57759ms step_avg:123.95ms
step:477/3242 train_loss:4.2468 train_time:57885ms step_avg:123.95ms
step:478/3242 train_loss:4.0516 train_time:58009ms step_avg:123.95ms
step:479/3242 train_loss:4.2624 train_time:58132ms step_avg:123.95ms
step:480/3242 train_loss:4.2272 train_time:58256ms step_avg:123.95ms
step:481/3242 train_loss:4.3721 train_time:58381ms step_avg:123.95ms
step:482/3242 train_loss:4.1838 train_time:58508ms step_avg:123.96ms
step:483/3242 train_loss:3.9943 train_time:58632ms step_avg:123.96ms
step:484/3242 train_loss:4.2676 train_time:58755ms step_avg:123.96ms
step:485/3242 train_loss:4.1322 train_time:58879ms step_avg:123.96ms
step:486/3242 train_loss:4.1480 train_time:59005ms step_avg:123.96ms
step:487/3242 train_loss:4.0628 train_time:59128ms step_avg:123.96ms
step:488/3242 train_loss:4.1158 train_time:59252ms step_avg:123.96ms
step:489/3242 train_loss:4.3194 train_time:59376ms step_avg:123.96ms
step:490/3242 train_loss:4.1818 train_time:59499ms step_avg:123.96ms
step:491/3242 train_loss:4.0774 train_time:59627ms step_avg:123.96ms
step:492/3242 train_loss:4.0772 train_time:59750ms step_avg:123.96ms
step:493/3242 train_loss:4.1964 train_time:59874ms step_avg:123.96ms
step:494/3242 train_loss:4.0360 train_time:59998ms step_avg:123.96ms
step:495/3242 train_loss:4.1789 train_time:60125ms step_avg:123.97ms
step:496/3242 train_loss:4.1026 train_time:60249ms step_avg:123.97ms
step:497/3242 train_loss:4.0197 train_time:60371ms step_avg:123.97ms
step:498/3242 train_loss:4.1946 train_time:60494ms step_avg:123.96ms
step:499/3242 train_loss:4.2675 train_time:60618ms step_avg:123.96ms
step:500/3242 train_loss:4.3153 train_time:60746ms step_avg:123.97ms
step:500/3242 val_loss:4.1707 train_time:60791ms step_avg:124.06ms
step:501/3242 train_loss:4.2142 train_time:60870ms step_avg:123.97ms
step:502/3242 train_loss:4.2448 train_time:61004ms step_avg:123.99ms
step:503/3242 train_loss:4.1957 train_time:61128ms step_avg:123.99ms
step:504/3242 train_loss:4.2338 train_time:61251ms step_avg:123.99ms
step:505/3242 train_loss:4.2009 train_time:61373ms step_avg:123.99ms
step:506/3242 train_loss:4.2863 train_time:61498ms step_avg:123.99ms
step:507/3242 train_loss:4.0834 train_time:61619ms step_avg:123.98ms
step:508/3242 train_loss:4.2171 train_time:61741ms step_avg:123.98ms
step:509/3242 train_loss:4.2977 train_time:61866ms step_avg:123.98ms
step:510/3242 train_loss:4.2437 train_time:61994ms step_avg:123.99ms
step:511/3242 train_loss:4.0456 train_time:62121ms step_avg:123.99ms
step:512/3242 train_loss:4.2414 train_time:62245ms step_avg:123.99ms
step:513/3242 train_loss:4.1737 train_time:62370ms step_avg:124.00ms
step:514/3242 train_loss:4.1474 train_time:62492ms step_avg:123.99ms
step:515/3242 train_loss:4.1872 train_time:62615ms step_avg:123.99ms
step:516/3242 train_loss:4.1963 train_time:62741ms step_avg:123.99ms
step:517/3242 train_loss:4.5358 train_time:62864ms step_avg:123.99ms
step:518/3242 train_loss:4.1274 train_time:62988ms step_avg:123.99ms
step:519/3242 train_loss:4.2546 train_time:63114ms step_avg:124.00ms
step:520/3242 train_loss:4.1605 train_time:63242ms step_avg:124.00ms
step:521/3242 train_loss:4.1438 train_time:63364ms step_avg:124.00ms
step:522/3242 train_loss:4.0891 train_time:63489ms step_avg:124.00ms
step:523/3242 train_loss:4.1071 train_time:63612ms step_avg:124.00ms
step:524/3242 train_loss:4.7398 train_time:63738ms step_avg:124.00ms
step:525/3242 train_loss:4.2084 train_time:63862ms step_avg:124.00ms
step:526/3242 train_loss:4.1458 train_time:63986ms step_avg:124.00ms
step:527/3242 train_loss:4.1404 train_time:64111ms step_avg:124.01ms
step:528/3242 train_loss:4.1052 train_time:64238ms step_avg:124.01ms
step:529/3242 train_loss:4.0806 train_time:64362ms step_avg:124.01ms
step:530/3242 train_loss:4.2813 train_time:64485ms step_avg:124.01ms
step:531/3242 train_loss:4.0969 train_time:64609ms step_avg:124.01ms
step:532/3242 train_loss:4.3728 train_time:64733ms step_avg:124.01ms
step:533/3242 train_loss:4.1805 train_time:64859ms step_avg:124.01ms
step:534/3242 train_loss:4.1157 train_time:64983ms step_avg:124.01ms
step:535/3242 train_loss:4.1280 train_time:65107ms step_avg:124.01ms
step:536/3242 train_loss:4.0639 train_time:65231ms step_avg:124.01ms
step:537/3242 train_loss:4.1830 train_time:65358ms step_avg:124.02ms
step:538/3242 train_loss:4.1813 train_time:65482ms step_avg:124.02ms
step:539/3242 train_loss:4.0947 train_time:65606ms step_avg:124.02ms
step:540/3242 train_loss:4.5751 train_time:65730ms step_avg:124.02ms
step:541/3242 train_loss:4.1199 train_time:65854ms step_avg:124.02ms
step:542/3242 train_loss:4.2319 train_time:65980ms step_avg:124.02ms
step:543/3242 train_loss:4.0647 train_time:66104ms step_avg:124.02ms
step:544/3242 train_loss:4.0530 train_time:66226ms step_avg:124.02ms
step:545/3242 train_loss:4.1341 train_time:66350ms step_avg:124.02ms
step:546/3242 train_loss:4.0633 train_time:66475ms step_avg:124.02ms
step:547/3242 train_loss:4.1009 train_time:66600ms step_avg:124.02ms
step:548/3242 train_loss:4.1066 train_time:66723ms step_avg:124.02ms
step:549/3242 train_loss:4.0932 train_time:66848ms step_avg:124.02ms
step:550/3242 train_loss:4.1708 train_time:66972ms step_avg:124.02ms
step:551/3242 train_loss:4.0422 train_time:67098ms step_avg:124.03ms
step:552/3242 train_loss:4.0787 train_time:67222ms step_avg:124.03ms
step:553/3242 train_loss:4.4046 train_time:67345ms step_avg:124.02ms
step:554/3242 train_loss:4.1985 train_time:67468ms step_avg:124.02ms
step:555/3242 train_loss:4.1717 train_time:67593ms step_avg:124.02ms
step:556/3242 train_loss:4.1400 train_time:67718ms step_avg:124.03ms
step:557/3242 train_loss:4.1434 train_time:67841ms step_avg:124.02ms
step:558/3242 train_loss:3.8043 train_time:67965ms step_avg:124.02ms
step:559/3242 train_loss:4.0546 train_time:68089ms step_avg:124.02ms
step:560/3242 train_loss:4.1075 train_time:68213ms step_avg:124.02ms
step:561/3242 train_loss:4.1555 train_time:68341ms step_avg:124.03ms
step:562/3242 train_loss:4.0587 train_time:68464ms step_avg:124.03ms
step:563/3242 train_loss:3.9988 train_time:68588ms step_avg:124.03ms
step:564/3242 train_loss:4.2014 train_time:68712ms step_avg:124.03ms
step:565/3242 train_loss:4.0325 train_time:68838ms step_avg:124.03ms
step:566/3242 train_loss:4.1335 train_time:68961ms step_avg:124.03ms
step:567/3242 train_loss:4.0925 train_time:69249ms step_avg:124.33ms
step:568/3242 train_loss:4.0325 train_time:69379ms step_avg:124.34ms
step:569/3242 train_loss:4.1285 train_time:69501ms step_avg:124.33ms
step:570/3242 train_loss:4.1061 train_time:69801ms step_avg:124.64ms
step:571/3242 train_loss:4.1228 train_time:69923ms step_avg:124.64ms
step:572/3242 train_loss:4.2323 train_time:70046ms step_avg:124.64ms
step:573/3242 train_loss:4.1388 train_time:70167ms step_avg:124.63ms
step:574/3242 train_loss:4.1518 train_time:70289ms step_avg:124.63ms
step:575/3242 train_loss:4.2172 train_time:70410ms step_avg:124.62ms
step:576/3242 train_loss:4.1821 train_time:70531ms step_avg:124.61ms
step:577/3242 train_loss:4.1780 train_time:70659ms step_avg:124.62ms
step:578/3242 train_loss:4.1303 train_time:70788ms step_avg:124.63ms
step:579/3242 train_loss:4.0950 train_time:70913ms step_avg:124.63ms
step:580/3242 train_loss:4.0946 train_time:71040ms step_avg:124.63ms
step:581/3242 train_loss:4.0440 train_time:71163ms step_avg:124.63ms
step:582/3242 train_loss:4.0701 train_time:71285ms step_avg:124.62ms
step:583/3242 train_loss:4.3001 train_time:71407ms step_avg:124.62ms
step:584/3242 train_loss:4.0688 train_time:71529ms step_avg:124.62ms
step:585/3242 train_loss:4.0276 train_time:71655ms step_avg:124.62ms
step:586/3242 train_loss:4.2004 train_time:71782ms step_avg:124.62ms
step:587/3242 train_loss:3.9667 train_time:71905ms step_avg:124.62ms
step:588/3242 train_loss:4.0938 train_time:72031ms step_avg:124.62ms
step:589/3242 train_loss:4.0991 train_time:72156ms step_avg:124.62ms
step:590/3242 train_loss:4.4496 train_time:72280ms step_avg:124.62ms
step:591/3242 train_loss:4.2188 train_time:72402ms step_avg:124.62ms
step:592/3242 train_loss:3.9501 train_time:72525ms step_avg:124.61ms
step:593/3242 train_loss:3.9690 train_time:72650ms step_avg:124.61ms
step:594/3242 train_loss:3.9669 train_time:72773ms step_avg:124.61ms
step:595/3242 train_loss:4.0194 train_time:72901ms step_avg:124.62ms
step:596/3242 train_loss:4.3541 train_time:73024ms step_avg:124.61ms
step:597/3242 train_loss:4.0807 train_time:73149ms step_avg:124.62ms
step:598/3242 train_loss:4.0271 train_time:73273ms step_avg:124.61ms
step:599/3242 train_loss:4.0985 train_time:73398ms step_avg:124.61ms
step:600/3242 train_loss:3.9055 train_time:73522ms step_avg:124.61ms
step:601/3242 train_loss:4.0324 train_time:73644ms step_avg:124.61ms
step:602/3242 train_loss:4.0579 train_time:73769ms step_avg:124.61ms
step:603/3242 train_loss:4.0836 train_time:73893ms step_avg:124.61ms
step:604/3242 train_loss:4.2094 train_time:74019ms step_avg:124.61ms
step:605/3242 train_loss:4.0784 train_time:74142ms step_avg:124.61ms
step:606/3242 train_loss:4.0455 train_time:74266ms step_avg:124.61ms
step:607/3242 train_loss:3.9814 train_time:74390ms step_avg:124.61ms
step:608/3242 train_loss:4.2241 train_time:74514ms step_avg:124.61ms
step:609/3242 train_loss:4.0734 train_time:74640ms step_avg:124.61ms
step:610/3242 train_loss:4.0388 train_time:74763ms step_avg:124.61ms
step:611/3242 train_loss:4.1556 train_time:74888ms step_avg:124.61ms
step:612/3242 train_loss:4.0592 train_time:75011ms step_avg:124.60ms
step:613/3242 train_loss:4.0210 train_time:75136ms step_avg:124.60ms
step:614/3242 train_loss:4.1996 train_time:75262ms step_avg:124.61ms
step:615/3242 train_loss:4.1707 train_time:75385ms step_avg:124.60ms
step:616/3242 train_loss:4.1335 train_time:75510ms step_avg:124.60ms
step:617/3242 train_loss:4.0392 train_time:75633ms step_avg:124.60ms
step:618/3242 train_loss:3.9896 train_time:75760ms step_avg:124.60ms
step:619/3242 train_loss:4.0976 train_time:75882ms step_avg:124.60ms
step:620/3242 train_loss:4.0154 train_time:76006ms step_avg:124.60ms
step:621/3242 train_loss:4.0228 train_time:76131ms step_avg:124.60ms
step:622/3242 train_loss:4.3112 train_time:76257ms step_avg:124.60ms
step:623/3242 train_loss:4.0229 train_time:76382ms step_avg:124.60ms
step:624/3242 train_loss:4.0683 train_time:76505ms step_avg:124.60ms
step:625/3242 train_loss:4.1207 train_time:76629ms step_avg:124.60ms
step:625/3242 val_loss:4.0562 train_time:76675ms step_avg:124.67ms
step:626/3242 train_loss:4.1708 train_time:76753ms step_avg:124.60ms
step:627/3242 train_loss:4.1824 train_time:76888ms step_avg:124.62ms
step:628/3242 train_loss:4.1489 train_time:77010ms step_avg:124.61ms
step:629/3242 train_loss:4.2083 train_time:77134ms step_avg:124.61ms
step:630/3242 train_loss:4.0192 train_time:77255ms step_avg:124.61ms
step:631/3242 train_loss:4.1407 train_time:77377ms step_avg:124.60ms
step:632/3242 train_loss:4.1884 train_time:77500ms step_avg:124.60ms
step:633/3242 train_loss:4.0870 train_time:77625ms step_avg:124.60ms
step:634/3242 train_loss:3.9907 train_time:77752ms step_avg:124.60ms
step:635/3242 train_loss:4.1027 train_time:77882ms step_avg:124.61ms
step:636/3242 train_loss:4.3635 train_time:78006ms step_avg:124.61ms
step:637/3242 train_loss:3.9475 train_time:78129ms step_avg:124.61ms
step:638/3242 train_loss:3.7769 train_time:78251ms step_avg:124.60ms
step:639/3242 train_loss:4.0059 train_time:78375ms step_avg:124.60ms
step:640/3242 train_loss:4.0209 train_time:78498ms step_avg:124.60ms
step:641/3242 train_loss:4.0024 train_time:78622ms step_avg:124.60ms
step:642/3242 train_loss:4.0026 train_time:78748ms step_avg:124.60ms
step:643/3242 train_loss:4.0481 train_time:78873ms step_avg:124.60ms
step:644/3242 train_loss:4.0731 train_time:78998ms step_avg:124.60ms
step:645/3242 train_loss:3.9847 train_time:79126ms step_avg:124.61ms
step:646/3242 train_loss:4.2038 train_time:79248ms step_avg:124.60ms
step:647/3242 train_loss:4.1082 train_time:79372ms step_avg:124.60ms
step:648/3242 train_loss:4.1006 train_time:79495ms step_avg:124.60ms
step:649/3242 train_loss:4.1012 train_time:79617ms step_avg:124.60ms
step:650/3242 train_loss:4.1832 train_time:79741ms step_avg:124.60ms
step:651/3242 train_loss:4.0424 train_time:79867ms step_avg:124.60ms
step:652/3242 train_loss:4.1734 train_time:79990ms step_avg:124.59ms
step:653/3242 train_loss:4.0136 train_time:80114ms step_avg:124.59ms
step:654/3242 train_loss:4.0827 train_time:80237ms step_avg:124.59ms
step:655/3242 train_loss:3.8544 train_time:80364ms step_avg:124.60ms
step:656/3242 train_loss:3.9906 train_time:80487ms step_avg:124.59ms
step:657/3242 train_loss:4.0019 train_time:80610ms step_avg:124.59ms
step:658/3242 train_loss:3.9431 train_time:80735ms step_avg:124.59ms
step:659/3242 train_loss:4.1215 train_time:80858ms step_avg:124.59ms
step:660/3242 train_loss:4.0130 train_time:80985ms step_avg:124.59ms
step:661/3242 train_loss:4.0943 train_time:81109ms step_avg:124.59ms
step:662/3242 train_loss:4.1676 train_time:81232ms step_avg:124.59ms
step:663/3242 train_loss:4.0874 train_time:81355ms step_avg:124.59ms
step:664/3242 train_loss:3.9630 train_time:81479ms step_avg:124.58ms
step:665/3242 train_loss:4.0552 train_time:81604ms step_avg:124.59ms
step:666/3242 train_loss:3.9091 train_time:81728ms step_avg:124.58ms
step:667/3242 train_loss:4.2079 train_time:81852ms step_avg:124.58ms
step:668/3242 train_loss:4.0534 train_time:81976ms step_avg:124.58ms
step:669/3242 train_loss:4.0369 train_time:82099ms step_avg:124.58ms
step:670/3242 train_loss:3.8998 train_time:82225ms step_avg:124.58ms
step:671/3242 train_loss:4.0078 train_time:82348ms step_avg:124.58ms
step:672/3242 train_loss:3.9652 train_time:82472ms step_avg:124.58ms
step:673/3242 train_loss:3.9968 train_time:82595ms step_avg:124.58ms
step:674/3242 train_loss:4.2595 train_time:82718ms step_avg:124.58ms
step:675/3242 train_loss:4.0606 train_time:82845ms step_avg:124.58ms
step:676/3242 train_loss:4.1222 train_time:82968ms step_avg:124.58ms
step:677/3242 train_loss:3.9014 train_time:83092ms step_avg:124.58ms
step:678/3242 train_loss:4.0080 train_time:83215ms step_avg:124.57ms
step:679/3242 train_loss:3.9583 train_time:83340ms step_avg:124.57ms
step:680/3242 train_loss:4.0936 train_time:83465ms step_avg:124.57ms
step:681/3242 train_loss:4.0043 train_time:83587ms step_avg:124.57ms
step:682/3242 train_loss:4.0279 train_time:83711ms step_avg:124.57ms
step:683/3242 train_loss:4.0956 train_time:83835ms step_avg:124.57ms
step:684/3242 train_loss:4.1485 train_time:83961ms step_avg:124.57ms
step:685/3242 train_loss:4.0455 train_time:84085ms step_avg:124.57ms
step:686/3242 train_loss:4.1303 train_time:84209ms step_avg:124.57ms
step:687/3242 train_loss:4.0533 train_time:84333ms step_avg:124.57ms
step:688/3242 train_loss:4.0996 train_time:84456ms step_avg:124.57ms
step:689/3242 train_loss:3.7208 train_time:84581ms step_avg:124.57ms
step:690/3242 train_loss:3.8323 train_time:84705ms step_avg:124.57ms
step:691/3242 train_loss:3.9754 train_time:84828ms step_avg:124.56ms
step:692/3242 train_loss:3.8716 train_time:84952ms step_avg:124.56ms
step:693/3242 train_loss:4.0709 train_time:85074ms step_avg:124.56ms
step:694/3242 train_loss:4.0886 train_time:85198ms step_avg:124.56ms
step:695/3242 train_loss:3.9694 train_time:85324ms step_avg:124.56ms
step:696/3242 train_loss:3.9557 train_time:85446ms step_avg:124.56ms
step:697/3242 train_loss:4.2604 train_time:85570ms step_avg:124.56ms
step:698/3242 train_loss:4.0298 train_time:85693ms step_avg:124.55ms
step:699/3242 train_loss:4.0461 train_time:85816ms step_avg:124.55ms
step:700/3242 train_loss:4.2317 train_time:85942ms step_avg:124.55ms
step:701/3242 train_loss:3.9941 train_time:86066ms step_avg:124.55ms
step:702/3242 train_loss:3.9418 train_time:86189ms step_avg:124.55ms
step:703/3242 train_loss:3.9392 train_time:86312ms step_avg:124.55ms
step:704/3242 train_loss:3.8828 train_time:86435ms step_avg:124.55ms
step:705/3242 train_loss:3.9745 train_time:86559ms step_avg:124.54ms
step:706/3242 train_loss:3.9688 train_time:86685ms step_avg:124.55ms
step:707/3242 train_loss:3.9939 train_time:86808ms step_avg:124.54ms
step:708/3242 train_loss:4.0633 train_time:86932ms step_avg:124.54ms
step:709/3242 train_loss:4.0015 train_time:87055ms step_avg:124.54ms
step:710/3242 train_loss:3.9750 train_time:87180ms step_avg:124.54ms
step:711/3242 train_loss:3.9672 train_time:87305ms step_avg:124.54ms
step:712/3242 train_loss:4.0096 train_time:87427ms step_avg:124.54ms
step:713/3242 train_loss:4.0620 train_time:87550ms step_avg:124.54ms
step:714/3242 train_loss:4.0827 train_time:87674ms step_avg:124.54ms
step:715/3242 train_loss:3.9833 train_time:87798ms step_avg:124.54ms
step:716/3242 train_loss:3.9927 train_time:87924ms step_avg:124.54ms
step:717/3242 train_loss:4.0083 train_time:88047ms step_avg:124.54ms
step:718/3242 train_loss:4.1498 train_time:88171ms step_avg:124.53ms
step:719/3242 train_loss:4.0169 train_time:88295ms step_avg:124.53ms
step:720/3242 train_loss:4.0776 train_time:88419ms step_avg:124.53ms
step:721/3242 train_loss:4.2201 train_time:88544ms step_avg:124.53ms
step:722/3242 train_loss:3.8624 train_time:88667ms step_avg:124.53ms
step:723/3242 train_loss:4.1355 train_time:88790ms step_avg:124.53ms
step:724/3242 train_loss:4.1888 train_time:88915ms step_avg:124.53ms
step:725/3242 train_loss:3.9678 train_time:89039ms step_avg:124.53ms
step:726/3242 train_loss:4.0574 train_time:89164ms step_avg:124.53ms
step:727/3242 train_loss:3.9694 train_time:89287ms step_avg:124.53ms
step:728/3242 train_loss:3.9503 train_time:89411ms step_avg:124.53ms
step:729/3242 train_loss:4.1437 train_time:89535ms step_avg:124.53ms
step:730/3242 train_loss:4.0960 train_time:89658ms step_avg:124.52ms
step:731/3242 train_loss:4.1152 train_time:89783ms step_avg:124.53ms
step:732/3242 train_loss:3.9669 train_time:89906ms step_avg:124.52ms
step:733/3242 train_loss:4.0010 train_time:90030ms step_avg:124.52ms
step:734/3242 train_loss:4.2382 train_time:90152ms step_avg:124.52ms
step:735/3242 train_loss:3.9598 train_time:90275ms step_avg:124.52ms
step:736/3242 train_loss:4.0325 train_time:90398ms step_avg:124.51ms
step:737/3242 train_loss:4.1568 train_time:90524ms step_avg:124.52ms
step:738/3242 train_loss:4.0590 train_time:90647ms step_avg:124.51ms
step:739/3242 train_loss:4.0112 train_time:90770ms step_avg:124.51ms
step:740/3242 train_loss:3.9078 train_time:90893ms step_avg:124.51ms
step:741/3242 train_loss:4.5686 train_time:91016ms step_avg:124.51ms
step:742/3242 train_loss:3.9199 train_time:91142ms step_avg:124.51ms
step:743/3242 train_loss:4.0013 train_time:91265ms step_avg:124.51ms
step:744/3242 train_loss:3.9819 train_time:91388ms step_avg:124.51ms
step:745/3242 train_loss:4.0490 train_time:91512ms step_avg:124.51ms
step:746/3242 train_loss:4.0366 train_time:91635ms step_avg:124.50ms
step:747/3242 train_loss:4.0051 train_time:91758ms step_avg:124.50ms
step:748/3242 train_loss:4.0408 train_time:91884ms step_avg:124.50ms
step:749/3242 train_loss:3.9699 train_time:92006ms step_avg:124.50ms
step:750/3242 train_loss:3.9748 train_time:92130ms step_avg:124.50ms
step:750/3242 val_loss:3.9816 train_time:92176ms step_avg:124.56ms
step:751/3242 train_loss:4.0174 train_time:92256ms step_avg:124.50ms
step:752/3242 train_loss:3.9677 train_time:92390ms step_avg:124.51ms
step:753/3242 train_loss:3.9995 train_time:92515ms step_avg:124.52ms
step:754/3242 train_loss:4.0241 train_time:92638ms step_avg:124.51ms
step:755/3242 train_loss:3.9878 train_time:92760ms step_avg:124.51ms
step:756/3242 train_loss:4.0748 train_time:93047ms step_avg:124.73ms
step:757/3242 train_loss:3.9010 train_time:93177ms step_avg:124.73ms
step:758/3242 train_loss:4.1258 train_time:93297ms step_avg:124.73ms
step:759/3242 train_loss:4.0530 train_time:93419ms step_avg:124.72ms
step:760/3242 train_loss:3.9719 train_time:93730ms step_avg:124.97ms
step:761/3242 train_loss:4.0780 train_time:93850ms step_avg:124.97ms
step:762/3242 train_loss:3.8016 train_time:93973ms step_avg:124.96ms
step:763/3242 train_loss:3.9688 train_time:94094ms step_avg:124.96ms
step:764/3242 train_loss:4.0794 train_time:94216ms step_avg:124.95ms
step:765/3242 train_loss:3.7338 train_time:94336ms step_avg:124.95ms
step:766/3242 train_loss:4.1506 train_time:94458ms step_avg:124.94ms
step:767/3242 train_loss:4.0153 train_time:94581ms step_avg:124.94ms
step:768/3242 train_loss:3.9523 train_time:94712ms step_avg:124.95ms
step:769/3242 train_loss:3.9870 train_time:94836ms step_avg:124.95ms
step:770/3242 train_loss:4.0064 train_time:94959ms step_avg:124.95ms
step:771/3242 train_loss:4.0675 train_time:95082ms step_avg:124.94ms
step:772/3242 train_loss:4.2874 train_time:95207ms step_avg:124.94ms
step:773/3242 train_loss:3.8644 train_time:95329ms step_avg:124.94ms
step:774/3242 train_loss:4.0747 train_time:95449ms step_avg:124.93ms
step:775/3242 train_loss:4.0382 train_time:95575ms step_avg:124.93ms
step:776/3242 train_loss:4.0053 train_time:95701ms step_avg:124.94ms
step:777/3242 train_loss:3.8347 train_time:95828ms step_avg:124.94ms
step:778/3242 train_loss:3.8065 train_time:95949ms step_avg:124.93ms
step:779/3242 train_loss:3.8789 train_time:96072ms step_avg:124.93ms
step:780/3242 train_loss:3.9597 train_time:96195ms step_avg:124.93ms
step:781/3242 train_loss:4.0052 train_time:96318ms step_avg:124.93ms
step:782/3242 train_loss:4.0638 train_time:96442ms step_avg:124.93ms
step:783/3242 train_loss:3.9532 train_time:96569ms step_avg:124.93ms
step:784/3242 train_loss:3.9844 train_time:96692ms step_avg:124.92ms
step:785/3242 train_loss:3.9619 train_time:96816ms step_avg:124.92ms
step:786/3242 train_loss:3.9589 train_time:96940ms step_avg:124.92ms
step:787/3242 train_loss:3.8673 train_time:97066ms step_avg:124.92ms
step:788/3242 train_loss:4.1053 train_time:97187ms step_avg:124.92ms
step:789/3242 train_loss:3.9036 train_time:97310ms step_avg:124.92ms
step:790/3242 train_loss:3.9725 train_time:97434ms step_avg:124.92ms
step:791/3242 train_loss:4.0310 train_time:97558ms step_avg:124.91ms
step:792/3242 train_loss:4.1633 train_time:97683ms step_avg:124.91ms
step:793/3242 train_loss:4.1771 train_time:97808ms step_avg:124.91ms
step:794/3242 train_loss:3.9147 train_time:97931ms step_avg:124.91ms
step:795/3242 train_loss:3.9965 train_time:98055ms step_avg:124.91ms
step:796/3242 train_loss:4.0448 train_time:98177ms step_avg:124.91ms
step:797/3242 train_loss:4.1961 train_time:98301ms step_avg:124.91ms
step:798/3242 train_loss:3.9181 train_time:98427ms step_avg:124.91ms
step:799/3242 train_loss:4.0569 train_time:98549ms step_avg:124.90ms
step:800/3242 train_loss:3.9661 train_time:98674ms step_avg:124.90ms
step:801/3242 train_loss:3.9457 train_time:98795ms step_avg:124.90ms
step:802/3242 train_loss:4.0570 train_time:98919ms step_avg:124.90ms
step:803/3242 train_loss:3.8937 train_time:99044ms step_avg:124.90ms
step:804/3242 train_loss:3.9460 train_time:99168ms step_avg:124.90ms
step:805/3242 train_loss:4.0573 train_time:99289ms step_avg:124.89ms
step:806/3242 train_loss:3.9398 train_time:99414ms step_avg:124.89ms
step:807/3242 train_loss:3.9414 train_time:99535ms step_avg:124.89ms
step:808/3242 train_loss:4.0452 train_time:99661ms step_avg:124.89ms
step:809/3242 train_loss:3.9604 train_time:99785ms step_avg:124.89ms
step:810/3242 train_loss:3.8879 train_time:99909ms step_avg:124.89ms
step:811/3242 train_loss:3.9688 train_time:100031ms step_avg:124.88ms
step:812/3242 train_loss:4.0095 train_time:100154ms step_avg:124.88ms
step:813/3242 train_loss:3.9822 train_time:100278ms step_avg:124.88ms
step:814/3242 train_loss:4.0393 train_time:100403ms step_avg:124.88ms
step:815/3242 train_loss:3.9760 train_time:100526ms step_avg:124.88ms
step:816/3242 train_loss:3.9649 train_time:100649ms step_avg:124.88ms
step:817/3242 train_loss:4.0450 train_time:100772ms step_avg:124.87ms
step:818/3242 train_loss:4.1499 train_time:100895ms step_avg:124.87ms
step:819/3242 train_loss:3.9387 train_time:101019ms step_avg:124.87ms
step:820/3242 train_loss:4.1276 train_time:101145ms step_avg:124.87ms
step:821/3242 train_loss:3.9096 train_time:101268ms step_avg:124.87ms
step:822/3242 train_loss:3.9445 train_time:101390ms step_avg:124.87ms
step:823/3242 train_loss:4.0609 train_time:101514ms step_avg:124.86ms
step:824/3242 train_loss:3.9853 train_time:101636ms step_avg:124.86ms
step:825/3242 train_loss:3.9105 train_time:101760ms step_avg:124.86ms
step:826/3242 train_loss:3.9992 train_time:101883ms step_avg:124.86ms
step:827/3242 train_loss:3.9084 train_time:102008ms step_avg:124.86ms
step:828/3242 train_loss:4.1280 train_time:102131ms step_avg:124.85ms
step:829/3242 train_loss:4.0182 train_time:102252ms step_avg:124.85ms
step:830/3242 train_loss:4.0859 train_time:102377ms step_avg:124.85ms
step:831/3242 train_loss:3.9234 train_time:102500ms step_avg:124.85ms
step:832/3242 train_loss:3.9848 train_time:102625ms step_avg:124.85ms
step:833/3242 train_loss:3.9217 train_time:102748ms step_avg:124.85ms
step:834/3242 train_loss:4.0397 train_time:102871ms step_avg:124.84ms
step:835/3242 train_loss:3.8972 train_time:102994ms step_avg:124.84ms
step:836/3242 train_loss:3.8634 train_time:103118ms step_avg:124.84ms
step:837/3242 train_loss:4.1389 train_time:103242ms step_avg:124.84ms
step:838/3242 train_loss:3.8299 train_time:103368ms step_avg:124.84ms
step:839/3242 train_loss:3.9964 train_time:103489ms step_avg:124.84ms
step:840/3242 train_loss:3.8300 train_time:103614ms step_avg:124.84ms
step:841/3242 train_loss:3.8712 train_time:103735ms step_avg:124.83ms
step:842/3242 train_loss:3.9553 train_time:103859ms step_avg:124.83ms
step:843/3242 train_loss:3.9679 train_time:103983ms step_avg:124.83ms
step:844/3242 train_loss:3.9701 train_time:104108ms step_avg:124.83ms
step:845/3242 train_loss:3.8169 train_time:104230ms step_avg:124.83ms
step:846/3242 train_loss:4.0657 train_time:104353ms step_avg:124.82ms
step:847/3242 train_loss:3.9203 train_time:104476ms step_avg:124.82ms
step:848/3242 train_loss:3.8936 train_time:104600ms step_avg:124.82ms
step:849/3242 train_loss:4.0214 train_time:104725ms step_avg:124.82ms
step:850/3242 train_loss:3.8864 train_time:104847ms step_avg:124.82ms
step:851/3242 train_loss:3.8463 train_time:104971ms step_avg:124.82ms
step:852/3242 train_loss:4.1575 train_time:105093ms step_avg:124.81ms
step:853/3242 train_loss:3.8504 train_time:105216ms step_avg:124.81ms
step:854/3242 train_loss:3.9570 train_time:105340ms step_avg:124.81ms
step:855/3242 train_loss:4.0455 train_time:105466ms step_avg:124.81ms
step:856/3242 train_loss:3.9464 train_time:105588ms step_avg:124.81ms
step:857/3242 train_loss:3.9490 train_time:105712ms step_avg:124.81ms
step:858/3242 train_loss:3.9839 train_time:105834ms step_avg:124.80ms
step:859/3242 train_loss:3.9008 train_time:105956ms step_avg:124.80ms
step:860/3242 train_loss:3.9810 train_time:106081ms step_avg:124.80ms
step:861/3242 train_loss:4.0055 train_time:106206ms step_avg:124.80ms
step:862/3242 train_loss:4.0415 train_time:106328ms step_avg:124.80ms
step:863/3242 train_loss:3.9706 train_time:106451ms step_avg:124.80ms
step:864/3242 train_loss:3.9627 train_time:106573ms step_avg:124.79ms
step:865/3242 train_loss:3.8057 train_time:106697ms step_avg:124.79ms
step:866/3242 train_loss:3.9874 train_time:106821ms step_avg:124.79ms
step:867/3242 train_loss:4.2546 train_time:106945ms step_avg:124.79ms
step:868/3242 train_loss:3.8308 train_time:107068ms step_avg:124.79ms
step:869/3242 train_loss:4.0206 train_time:107190ms step_avg:124.78ms
step:870/3242 train_loss:4.0041 train_time:107313ms step_avg:124.78ms
step:871/3242 train_loss:3.8429 train_time:107435ms step_avg:124.78ms
step:872/3242 train_loss:3.8338 train_time:107558ms step_avg:124.78ms
step:873/3242 train_loss:4.0639 train_time:107683ms step_avg:124.78ms
step:874/3242 train_loss:3.8485 train_time:107808ms step_avg:124.78ms
step:875/3242 train_loss:3.5093 train_time:107930ms step_avg:124.77ms
step:875/3242 val_loss:3.9229 train_time:107976ms step_avg:124.83ms
step:876/3242 train_loss:4.0345 train_time:108059ms step_avg:124.78ms
step:877/3242 train_loss:3.8618 train_time:108196ms step_avg:124.79ms
step:878/3242 train_loss:4.0293 train_time:108319ms step_avg:124.79ms
step:879/3242 train_loss:3.8801 train_time:108440ms step_avg:124.79ms
step:880/3242 train_loss:4.0533 train_time:108562ms step_avg:124.78ms
step:881/3242 train_loss:3.7075 train_time:108682ms step_avg:124.78ms
step:882/3242 train_loss:3.8997 train_time:108804ms step_avg:124.77ms
step:883/3242 train_loss:4.0868 train_time:108925ms step_avg:124.77ms
step:884/3242 train_loss:4.2342 train_time:109048ms step_avg:124.77ms
step:885/3242 train_loss:3.9659 train_time:109177ms step_avg:124.77ms
step:886/3242 train_loss:3.8750 train_time:109302ms step_avg:124.77ms
step:887/3242 train_loss:3.9933 train_time:109425ms step_avg:124.77ms
step:888/3242 train_loss:4.4884 train_time:109548ms step_avg:124.77ms
step:889/3242 train_loss:4.2050 train_time:109672ms step_avg:124.77ms
step:890/3242 train_loss:3.9191 train_time:109795ms step_avg:124.77ms
step:891/3242 train_loss:3.9297 train_time:109918ms step_avg:124.76ms
step:892/3242 train_loss:3.7564 train_time:110041ms step_avg:124.76ms
step:893/3242 train_loss:4.0896 train_time:110165ms step_avg:124.76ms
step:894/3242 train_loss:3.8248 train_time:110292ms step_avg:124.76ms
step:895/3242 train_loss:4.1161 train_time:110416ms step_avg:124.76ms
step:896/3242 train_loss:4.0950 train_time:110537ms step_avg:124.76ms
step:897/3242 train_loss:3.9028 train_time:110660ms step_avg:124.76ms
step:898/3242 train_loss:3.9225 train_time:110782ms step_avg:124.76ms
step:899/3242 train_loss:3.9801 train_time:110904ms step_avg:124.75ms
step:900/3242 train_loss:3.8774 train_time:111029ms step_avg:124.75ms
step:901/3242 train_loss:3.8080 train_time:111154ms step_avg:124.75ms
step:902/3242 train_loss:4.0274 train_time:111278ms step_avg:124.75ms
step:903/3242 train_loss:4.0306 train_time:111401ms step_avg:124.75ms
step:904/3242 train_loss:3.9260 train_time:111523ms step_avg:124.75ms
step:905/3242 train_loss:3.8940 train_time:111645ms step_avg:124.74ms
step:906/3242 train_loss:3.9044 train_time:111768ms step_avg:124.74ms
step:907/3242 train_loss:4.1166 train_time:111892ms step_avg:124.74ms
step:908/3242 train_loss:3.9005 train_time:112016ms step_avg:124.74ms
step:909/3242 train_loss:3.9424 train_time:112138ms step_avg:124.74ms
step:910/3242 train_loss:3.8475 train_time:112261ms step_avg:124.73ms
step:911/3242 train_loss:3.9418 train_time:112384ms step_avg:124.73ms
step:912/3242 train_loss:4.0069 train_time:112508ms step_avg:124.73ms
step:913/3242 train_loss:3.9653 train_time:112633ms step_avg:124.73ms
step:914/3242 train_loss:3.8763 train_time:112756ms step_avg:124.73ms
step:915/3242 train_loss:4.1305 train_time:112878ms step_avg:124.73ms
step:916/3242 train_loss:3.9318 train_time:113001ms step_avg:124.73ms
step:917/3242 train_loss:4.0221 train_time:113124ms step_avg:124.72ms
step:918/3242 train_loss:4.0035 train_time:113248ms step_avg:124.72ms
step:919/3242 train_loss:5.1646 train_time:113375ms step_avg:124.72ms
step:920/3242 train_loss:3.9246 train_time:113498ms step_avg:124.72ms
step:921/3242 train_loss:3.9554 train_time:113620ms step_avg:124.72ms
step:922/3242 train_loss:3.9085 train_time:113742ms step_avg:124.72ms
step:923/3242 train_loss:3.9863 train_time:113865ms step_avg:124.72ms
step:924/3242 train_loss:3.9841 train_time:113990ms step_avg:124.72ms
step:925/3242 train_loss:4.0765 train_time:114115ms step_avg:124.72ms
step:926/3242 train_loss:4.0502 train_time:114236ms step_avg:124.71ms
step:927/3242 train_loss:3.9377 train_time:114360ms step_avg:124.71ms
step:928/3242 train_loss:3.9334 train_time:114482ms step_avg:124.71ms
step:929/3242 train_loss:4.1402 train_time:114605ms step_avg:124.71ms
step:930/3242 train_loss:3.9941 train_time:114727ms step_avg:124.70ms
step:931/3242 train_loss:3.7806 train_time:114851ms step_avg:124.70ms
step:932/3242 train_loss:3.8808 train_time:114975ms step_avg:124.70ms
step:933/3242 train_loss:4.0779 train_time:115098ms step_avg:124.70ms
step:934/3242 train_loss:3.8167 train_time:115220ms step_avg:124.70ms
step:935/3242 train_loss:3.9483 train_time:115342ms step_avg:124.69ms
step:936/3242 train_loss:3.8512 train_time:115466ms step_avg:124.69ms
step:937/3242 train_loss:3.8955 train_time:115591ms step_avg:124.69ms
step:938/3242 train_loss:3.9958 train_time:115715ms step_avg:124.69ms
step:939/3242 train_loss:3.9247 train_time:115836ms step_avg:124.69ms
step:940/3242 train_loss:4.0864 train_time:115960ms step_avg:124.69ms
step:941/3242 train_loss:3.8795 train_time:116081ms step_avg:124.68ms
step:942/3242 train_loss:3.9411 train_time:116203ms step_avg:124.68ms
step:943/3242 train_loss:3.7518 train_time:116327ms step_avg:124.68ms
step:944/3242 train_loss:4.0836 train_time:116450ms step_avg:124.68ms
step:945/3242 train_loss:3.7949 train_time:116739ms step_avg:124.85ms
step:946/3242 train_loss:3.8167 train_time:116868ms step_avg:124.86ms
step:947/3242 train_loss:5.4171 train_time:116991ms step_avg:124.86ms
step:948/3242 train_loss:3.9804 train_time:117111ms step_avg:124.85ms
step:949/3242 train_loss:3.8825 train_time:117234ms step_avg:124.85ms
step:950/3242 train_loss:3.7888 train_time:117541ms step_avg:125.04ms
step:951/3242 train_loss:3.8389 train_time:117661ms step_avg:125.04ms
step:952/3242 train_loss:3.7980 train_time:117781ms step_avg:125.03ms
step:953/3242 train_loss:3.8646 train_time:117903ms step_avg:125.03ms
step:954/3242 train_loss:3.9298 train_time:118024ms step_avg:125.03ms
step:955/3242 train_loss:3.8097 train_time:118145ms step_avg:125.02ms
step:956/3242 train_loss:3.8685 train_time:118266ms step_avg:125.02ms
step:957/3242 train_loss:3.8272 train_time:118392ms step_avg:125.02ms
step:958/3242 train_loss:3.8948 train_time:118521ms step_avg:125.02ms
step:959/3242 train_loss:3.8764 train_time:118645ms step_avg:125.02ms
step:960/3242 train_loss:3.8985 train_time:118768ms step_avg:125.02ms
step:961/3242 train_loss:3.7640 train_time:118891ms step_avg:125.02ms
step:962/3242 train_loss:4.0467 train_time:119015ms step_avg:125.02ms
step:963/3242 train_loss:3.9957 train_time:119136ms step_avg:125.01ms
step:964/3242 train_loss:3.8292 train_time:119257ms step_avg:125.01ms
step:965/3242 train_loss:3.8282 train_time:119382ms step_avg:125.01ms
step:966/3242 train_loss:3.8678 train_time:119507ms step_avg:125.01ms
step:967/3242 train_loss:4.0872 train_time:119634ms step_avg:125.01ms
step:968/3242 train_loss:3.9180 train_time:119757ms step_avg:125.01ms
step:969/3242 train_loss:3.9180 train_time:119878ms step_avg:125.00ms
step:970/3242 train_loss:3.9499 train_time:120002ms step_avg:125.00ms
step:971/3242 train_loss:3.7689 train_time:120123ms step_avg:125.00ms
step:972/3242 train_loss:3.9389 train_time:120247ms step_avg:125.00ms
step:973/3242 train_loss:3.8947 train_time:120371ms step_avg:125.00ms
step:974/3242 train_loss:3.9377 train_time:120497ms step_avg:125.00ms
step:975/3242 train_loss:4.0076 train_time:120620ms step_avg:124.99ms
step:976/3242 train_loss:3.8763 train_time:120745ms step_avg:124.99ms
step:977/3242 train_loss:4.0667 train_time:120867ms step_avg:124.99ms
step:978/3242 train_loss:3.9541 train_time:120993ms step_avg:124.99ms
step:979/3242 train_loss:3.7895 train_time:121117ms step_avg:124.99ms
step:980/3242 train_loss:4.0763 train_time:121238ms step_avg:124.99ms
step:981/3242 train_loss:3.8044 train_time:121362ms step_avg:124.99ms
step:982/3242 train_loss:3.9728 train_time:121484ms step_avg:124.98ms
step:983/3242 train_loss:3.9626 train_time:121611ms step_avg:124.99ms
step:984/3242 train_loss:3.9769 train_time:121735ms step_avg:124.98ms
step:985/3242 train_loss:3.8954 train_time:121857ms step_avg:124.98ms
step:986/3242 train_loss:3.9850 train_time:121980ms step_avg:124.98ms
step:987/3242 train_loss:3.8053 train_time:122103ms step_avg:124.98ms
step:988/3242 train_loss:3.8781 train_time:122225ms step_avg:124.97ms
step:989/3242 train_loss:3.8664 train_time:122349ms step_avg:124.97ms
step:990/3242 train_loss:3.8202 train_time:122473ms step_avg:124.97ms
step:991/3242 train_loss:4.0152 train_time:122596ms step_avg:124.97ms
step:992/3242 train_loss:3.8680 train_time:122718ms step_avg:124.97ms
step:993/3242 train_loss:3.8151 train_time:122841ms step_avg:124.97ms
step:994/3242 train_loss:3.9121 train_time:122964ms step_avg:124.96ms
step:995/3242 train_loss:3.9846 train_time:123086ms step_avg:124.96ms
step:996/3242 train_loss:3.9342 train_time:123209ms step_avg:124.96ms
step:997/3242 train_loss:3.8361 train_time:123334ms step_avg:124.96ms
step:998/3242 train_loss:4.2073 train_time:123457ms step_avg:124.96ms
step:999/3242 train_loss:3.8470 train_time:123579ms step_avg:124.95ms
step:1000/3242 train_loss:3.9647 train_time:123703ms step_avg:124.95ms
step:1000/3242 val_loss:3.8724 train_time:123748ms step_avg:125.00ms
step:1001/3242 train_loss:3.8403 train_time:123827ms step_avg:124.95ms
step:1002/3242 train_loss:3.9095 train_time:123964ms step_avg:124.96ms
step:1003/3242 train_loss:3.7840 train_time:124088ms step_avg:124.96ms
step:1004/3242 train_loss:3.9626 train_time:124210ms step_avg:124.96ms
step:1005/3242 train_loss:4.0176 train_time:124330ms step_avg:124.96ms
step:1006/3242 train_loss:3.7844 train_time:124452ms step_avg:124.95ms
step:1007/3242 train_loss:3.8672 train_time:124572ms step_avg:124.95ms
step:1008/3242 train_loss:3.8362 train_time:124693ms step_avg:124.94ms
step:1009/3242 train_loss:3.9614 train_time:124818ms step_avg:124.94ms
step:1010/3242 train_loss:4.0512 train_time:124947ms step_avg:124.95ms
step:1011/3242 train_loss:3.9547 train_time:125072ms step_avg:124.95ms
step:1012/3242 train_loss:3.9031 train_time:125194ms step_avg:124.94ms
step:1013/3242 train_loss:3.7748 train_time:125316ms step_avg:124.94ms
step:1014/3242 train_loss:3.9261 train_time:125440ms step_avg:124.94ms
step:1015/3242 train_loss:4.0436 train_time:125562ms step_avg:124.94ms
step:1016/3242 train_loss:3.7424 train_time:125684ms step_avg:124.93ms
step:1017/3242 train_loss:3.8432 train_time:125807ms step_avg:124.93ms
step:1018/3242 train_loss:3.8547 train_time:125933ms step_avg:124.93ms
step:1019/3242 train_loss:3.7732 train_time:126057ms step_avg:124.93ms
step:1020/3242 train_loss:3.9262 train_time:126185ms step_avg:124.94ms
step:1021/3242 train_loss:3.8395 train_time:126306ms step_avg:124.93ms
step:1022/3242 train_loss:3.7679 train_time:126429ms step_avg:124.93ms
step:1023/3242 train_loss:3.8757 train_time:126551ms step_avg:124.93ms
step:1024/3242 train_loss:3.9006 train_time:126673ms step_avg:124.92ms
step:1025/3242 train_loss:3.8736 train_time:126797ms step_avg:124.92ms
step:1026/3242 train_loss:3.9044 train_time:126924ms step_avg:124.92ms
step:1027/3242 train_loss:4.0579 train_time:127048ms step_avg:124.92ms
step:1028/3242 train_loss:3.7246 train_time:127171ms step_avg:124.92ms
step:1029/3242 train_loss:3.7752 train_time:127293ms step_avg:124.92ms
step:1030/3242 train_loss:3.7585 train_time:127418ms step_avg:124.92ms
step:1031/3242 train_loss:3.8966 train_time:127542ms step_avg:124.92ms
step:1032/3242 train_loss:3.9045 train_time:127665ms step_avg:124.92ms
step:1033/3242 train_loss:4.0861 train_time:127788ms step_avg:124.92ms
step:1034/3242 train_loss:3.8971 train_time:127912ms step_avg:124.91ms
step:1035/3242 train_loss:3.8476 train_time:128035ms step_avg:124.91ms
step:1036/3242 train_loss:3.8226 train_time:128158ms step_avg:124.91ms
step:1037/3242 train_loss:3.9038 train_time:128284ms step_avg:124.91ms
step:1038/3242 train_loss:4.2125 train_time:128407ms step_avg:124.91ms
step:1039/3242 train_loss:4.0187 train_time:128529ms step_avg:124.91ms
step:1040/3242 train_loss:3.9116 train_time:128651ms step_avg:124.90ms
step:1041/3242 train_loss:3.8199 train_time:128775ms step_avg:124.90ms
step:1042/3242 train_loss:3.8750 train_time:128899ms step_avg:124.90ms
step:1043/3242 train_loss:3.9250 train_time:129024ms step_avg:124.90ms
step:1044/3242 train_loss:3.8386 train_time:129146ms step_avg:124.90ms
step:1045/3242 train_loss:3.8456 train_time:129269ms step_avg:124.90ms
step:1046/3242 train_loss:3.9424 train_time:129394ms step_avg:124.90ms
step:1047/3242 train_loss:3.8434 train_time:129515ms step_avg:124.89ms
step:1048/3242 train_loss:4.0675 train_time:129638ms step_avg:124.89ms
step:1049/3242 train_loss:3.9016 train_time:129763ms step_avg:124.89ms
step:1050/3242 train_loss:3.8378 train_time:129886ms step_avg:124.89ms
step:1051/3242 train_loss:3.7914 train_time:130010ms step_avg:124.89ms
step:1052/3242 train_loss:3.9258 train_time:130133ms step_avg:124.89ms
step:1053/3242 train_loss:3.7964 train_time:130257ms step_avg:124.89ms
step:1054/3242 train_loss:4.1091 train_time:130382ms step_avg:124.89ms
step:1055/3242 train_loss:3.9427 train_time:130506ms step_avg:124.89ms
step:1056/3242 train_loss:3.8000 train_time:130627ms step_avg:124.88ms
step:1057/3242 train_loss:3.9087 train_time:130751ms step_avg:124.88ms
step:1058/3242 train_loss:3.9792 train_time:130872ms step_avg:124.88ms
step:1059/3242 train_loss:3.7046 train_time:130996ms step_avg:124.88ms
step:1060/3242 train_loss:3.8411 train_time:131122ms step_avg:124.88ms
step:1061/3242 train_loss:3.8538 train_time:131244ms step_avg:124.88ms
step:1062/3242 train_loss:3.8221 train_time:131367ms step_avg:124.87ms
step:1063/3242 train_loss:3.7961 train_time:131491ms step_avg:124.87ms
step:1064/3242 train_loss:3.9049 train_time:131614ms step_avg:124.87ms
step:1065/3242 train_loss:3.8036 train_time:131737ms step_avg:124.87ms
step:1066/3242 train_loss:3.7713 train_time:131860ms step_avg:124.87ms
step:1067/3242 train_loss:3.7922 train_time:131985ms step_avg:124.87ms
step:1068/3242 train_loss:3.7166 train_time:132107ms step_avg:124.86ms
step:1069/3242 train_loss:3.8266 train_time:132230ms step_avg:124.86ms
step:1070/3242 train_loss:3.7221 train_time:132354ms step_avg:124.86ms
step:1071/3242 train_loss:3.9516 train_time:132478ms step_avg:124.86ms
step:1072/3242 train_loss:3.9108 train_time:132603ms step_avg:124.86ms
step:1073/3242 train_loss:3.8601 train_time:132724ms step_avg:124.86ms
step:1074/3242 train_loss:3.9291 train_time:132848ms step_avg:124.86ms
step:1075/3242 train_loss:3.8841 train_time:132971ms step_avg:124.86ms
step:1076/3242 train_loss:3.8020 train_time:133095ms step_avg:124.85ms
step:1077/3242 train_loss:4.1837 train_time:133219ms step_avg:124.85ms
step:1078/3242 train_loss:3.8896 train_time:133344ms step_avg:124.85ms
step:1079/3242 train_loss:3.5652 train_time:133465ms step_avg:124.85ms
step:1080/3242 train_loss:3.9288 train_time:133589ms step_avg:124.85ms
step:1081/3242 train_loss:3.8591 train_time:133710ms step_avg:124.85ms
step:1082/3242 train_loss:3.9289 train_time:133833ms step_avg:124.84ms
step:1083/3242 train_loss:4.0255 train_time:133956ms step_avg:124.84ms
step:1084/3242 train_loss:3.9179 train_time:134082ms step_avg:124.84ms
step:1085/3242 train_loss:3.9038 train_time:134204ms step_avg:124.84ms
step:1086/3242 train_loss:3.8471 train_time:134326ms step_avg:124.84ms
step:1087/3242 train_loss:4.0487 train_time:134450ms step_avg:124.84ms
step:1088/3242 train_loss:3.9576 train_time:134573ms step_avg:124.84ms
step:1089/3242 train_loss:3.7667 train_time:134695ms step_avg:124.83ms
step:1090/3242 train_loss:3.7947 train_time:134817ms step_avg:124.83ms
step:1091/3242 train_loss:3.9133 train_time:134941ms step_avg:124.83ms
step:1092/3242 train_loss:3.7133 train_time:135064ms step_avg:124.83ms
step:1093/3242 train_loss:3.9106 train_time:135188ms step_avg:124.83ms
step:1094/3242 train_loss:4.0408 train_time:135311ms step_avg:124.83ms
step:1095/3242 train_loss:3.8836 train_time:135433ms step_avg:124.82ms
step:1096/3242 train_loss:3.8348 train_time:135557ms step_avg:124.82ms
step:1097/3242 train_loss:3.8580 train_time:135683ms step_avg:124.82ms
step:1098/3242 train_loss:3.9047 train_time:135805ms step_avg:124.82ms
step:1099/3242 train_loss:3.9683 train_time:135928ms step_avg:124.82ms
step:1100/3242 train_loss:3.9204 train_time:136052ms step_avg:124.82ms
step:1101/3242 train_loss:3.8665 train_time:136173ms step_avg:124.81ms
step:1102/3242 train_loss:3.7174 train_time:136298ms step_avg:124.81ms
step:1103/3242 train_loss:3.7865 train_time:136423ms step_avg:124.82ms
step:1104/3242 train_loss:3.8595 train_time:136545ms step_avg:124.81ms
step:1105/3242 train_loss:3.7379 train_time:136669ms step_avg:124.81ms
step:1106/3242 train_loss:4.4747 train_time:136792ms step_avg:124.81ms
step:1107/3242 train_loss:3.6487 train_time:136914ms step_avg:124.81ms
step:1108/3242 train_loss:3.9803 train_time:137039ms step_avg:124.81ms
step:1109/3242 train_loss:3.7639 train_time:137164ms step_avg:124.81ms
step:1110/3242 train_loss:3.9073 train_time:137285ms step_avg:124.80ms
step:1111/3242 train_loss:3.8446 train_time:137409ms step_avg:124.80ms
step:1112/3242 train_loss:3.8934 train_time:137532ms step_avg:124.80ms
step:1113/3242 train_loss:3.9867 train_time:137655ms step_avg:124.80ms
step:1114/3242 train_loss:3.8488 train_time:137779ms step_avg:124.80ms
step:1115/3242 train_loss:3.7695 train_time:137903ms step_avg:124.80ms
step:1116/3242 train_loss:3.6903 train_time:138027ms step_avg:124.80ms
step:1117/3242 train_loss:3.8502 train_time:138151ms step_avg:124.80ms
step:1118/3242 train_loss:4.0054 train_time:138273ms step_avg:124.79ms
step:1119/3242 train_loss:4.0335 train_time:138398ms step_avg:124.80ms
step:1120/3242 train_loss:3.8752 train_time:138521ms step_avg:124.79ms
step:1121/3242 train_loss:3.9022 train_time:138645ms step_avg:124.79ms
step:1122/3242 train_loss:3.8092 train_time:138767ms step_avg:124.79ms
step:1123/3242 train_loss:3.8630 train_time:138892ms step_avg:124.79ms
step:1124/3242 train_loss:4.0151 train_time:139015ms step_avg:124.79ms
step:1125/3242 train_loss:3.7825 train_time:139139ms step_avg:124.79ms
step:1125/3242 val_loss:3.8410 train_time:139185ms step_avg:124.83ms
step:1126/3242 train_loss:3.6799 train_time:139264ms step_avg:124.79ms
step:1127/3242 train_loss:3.9079 train_time:139403ms step_avg:124.80ms
step:1128/3242 train_loss:4.1241 train_time:139528ms step_avg:124.80ms
step:1129/3242 train_loss:3.6484 train_time:139651ms step_avg:124.80ms
step:1130/3242 train_loss:3.9655 train_time:139772ms step_avg:124.80ms
step:1131/3242 train_loss:3.8147 train_time:139892ms step_avg:124.79ms
step:1132/3242 train_loss:3.8289 train_time:140013ms step_avg:124.79ms
step:1133/3242 train_loss:3.7941 train_time:140133ms step_avg:124.78ms
step:1134/3242 train_loss:3.9596 train_time:140417ms step_avg:124.93ms
step:1135/3242 train_loss:3.8885 train_time:140544ms step_avg:124.93ms
step:1136/3242 train_loss:3.9307 train_time:140665ms step_avg:124.92ms
step:1137/3242 train_loss:3.9514 train_time:140785ms step_avg:124.92ms
step:1138/3242 train_loss:3.8786 train_time:140907ms step_avg:124.92ms
step:1139/3242 train_loss:3.7896 train_time:141028ms step_avg:124.91ms
step:1140/3242 train_loss:4.0809 train_time:141344ms step_avg:125.08ms
step:1141/3242 train_loss:3.8884 train_time:141464ms step_avg:125.08ms
step:1142/3242 train_loss:3.9771 train_time:141585ms step_avg:125.07ms
step:1143/3242 train_loss:3.8846 train_time:141705ms step_avg:125.07ms
