====================================================================================================
import os
import sys
with open(sys.argv[0]) as f:
    code = f.read() # read the code of this file ASAP, for logging
import uuid
import glob
import time
from dataclasses import dataclass

import numpy as np
import torch
from torch import nn
import torch.nn.functional as F
import torch.distributed as dist
import torch._inductor.config as config
from torch.nn.parallel import DistributedDataParallel as DDP

# -----------------------------------------------------------------------------
# Muon optimizer

def zeropower_via_svd(G, steps=None):
    U, S, V = G.svd()
    return U @ V.T

@torch.compile
def zeropower_via_newtonschulz5(G, steps=10, eps=1e-7):
    r"""
    Newton-Schulz iteration to compute the zeroth power / orthogonalization of G. We opt to use a
    quintic iteration whose coefficients are selected to maximize the slope at zero. For the purpose
    of minimizing steps, it turns out to be empirically effective to keep increasing the slope at
    zero even beyond the point where the iteration no longer converges all the way to one everywhere
    on the interval. This iteration therefore does not produce UV^T but rather something like US'V^T
    where S' is diagonal with S_{ii}' \sim Uniform(0.5, 1.5), which turns out not to hurt model
    performance at all relative to UV^T, where USV^T = G is the SVD.
    """
    assert len(G.shape) == 2
    a, b, c = (3.4445, -4.7750,  2.0315)
    X = G.bfloat16()
    X /= (X.norm() + eps) # ensure top singular value <= 1
    if G.size(0) > G.size(1):
        X = X.T
    for _ in range(steps):
        A = X @ X.T
        B = b * A + c * A @ A # adapted from suggestion by @jxbz, @leloykun, and @YouJiacheng
        X = a * X + B @ X
    if G.size(0) > G.size(1):
        X = X.T
    return X

zeropower_backends = dict(svd=zeropower_via_svd, newtonschulz5=zeropower_via_newtonschulz5)

class Muon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Some warnings:
    - This optimizer assumes that all parameters passed in are 2D.
    - It should not be used for the embedding layer, the final fully connected layer, or any {0,1}-D
    parameters; those should all be optimized by a standard method (e.g., AdamW).
    - To use it with 4D convolutional filters, it works well to just flatten their last 3 dimensions.
    - We believe it is unlikely to work well for training with small batch size.
    - We believe it may not work well for finetuning pretrained models, but we haven't tested this.
    - We have not yet tried this optimizer for training scenarios larger than NanoGPT (124M).

    Arguments:
        lr: The learning rate used by the internal SGD.
        momentum: The momentum used by the internal SGD.
        nesterov: Whether to use Nesterov-style momentum in the internal SGD. (recommended)
        backend: The chosen backend for the orthogonalization step. (recommended: 'newtonschulz5')
        backend_steps: The number of iteration steps to use in the backend, if it is iterative.
    """
    def __init__(self, params, lr=0.02, momentum=0.95, nesterov=True,
                 backend='newtonschulz5', backend_steps=5):
        defaults = dict(lr=lr, momentum=momentum, nesterov=nesterov, backend=backend, backend_steps=backend_steps)
        super().__init__(params, defaults)

    def step(self):

        for group in self.param_groups:

            lr = group['lr']
            momentum = group['momentum']
            zeropower_backend = zeropower_backends[group['backend']]

            # generate weight updates in distributed fashion
            total_params = sum(p.numel() for p in group['params'])
            updates_flat = torch.zeros(total_params, device='cuda', dtype=torch.bfloat16)
            curr_idx = 0
            for i, p in enumerate(group['params']):
                # luckily this will perfectly distribute a transformer with multiple of 4 layers to 8 GPUs
                if i % int(os.environ['WORLD_SIZE']) == int(os.environ['RANK']):
                    g = p.grad
                    assert g is not None
                    state = self.state[p]
                    if 'momentum_buffer' not in state:
                        state['momentum_buffer'] = torch.zeros_like(g)
                    buf = state['momentum_buffer']
                    buf.mul_(momentum).add_(g)
                    if group['nesterov']:
                        g = g.add(buf, alpha=momentum)
                    g = zeropower_backend(g, steps=group['backend_steps'])
                    g *= max(1, g.size(0)/g.size(1))**0.5
                    updates_flat[curr_idx:curr_idx+p.numel()] = g.flatten()
                curr_idx += p.numel()

            # sync updates across devices. we are not memory-constrained so can do this simple deserialization
            dist.all_reduce(updates_flat, op=dist.ReduceOp.SUM)

            # deserialize and apply updates
            curr_idx = 0
            for p in group['params']:
                g = updates_flat[curr_idx:curr_idx+p.numel()].view_as(p.data).type_as(p.data)
                p.data.add_(g, alpha=-lr)
                curr_idx += p.numel()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the GPT-2 model

class Rotary(torch.nn.Module):

    def __init__(self, dim, base=10000):
        super().__init__()
        self.dim = dim
        self.base = base
        self.inv_freq = None
        self.seq_len_cached = None
        self.cos_cached = None
        self.sin_cached = None

    def forward(self, x):
        seq_len = x.shape[1]
        if seq_len != self.seq_len_cached:
            self.inv_freq = 1.0 / (self.base ** (torch.arange(0, self.dim, 2, device=x.device).float() / self.dim))
            self.seq_len_cached = seq_len
            t = torch.arange(seq_len, device=x.device).type_as(self.inv_freq)
            freqs = torch.outer(t, self.inv_freq)
            self.cos_cached = freqs.cos().bfloat16()
            self.sin_cached = freqs.sin().bfloat16()
        return self.cos_cached[None, :, None, :], self.sin_cached[None, :, None, :]

def apply_rotary_emb(x, cos, sin):
    assert x.ndim == 4 # multihead attention
    d = x.shape[3]//2
    x1 = x[..., :d]
    x2 = x[..., d:]
    y1 = x1 * cos + x2 * sin
    y2 = x1 * (-sin) + x2 * cos
    return torch.cat([y1, y2], 3).type_as(x)

class CastedLinear(nn.Linear):
    def forward(self, x):
        return F.linear(x, self.weight.to(x.dtype))

class CausalSelfAttention(nn.Module):

    def __init__(self, config):
        super().__init__()
        self.n_head = config.n_head
        self.n_embd = config.n_embd
        self.head_dim = self.n_embd // self.n_head
        assert self.n_embd % self.n_head == 0
        self.c_q = CastedLinear(self.n_embd, self.n_embd, bias=False)
        self.c_k = CastedLinear(self.n_embd, self.n_embd, bias=False)
        self.c_v = CastedLinear(self.n_embd, self.n_embd, bias=False)
        # output projection
        self.c_proj = CastedLinear(self.n_embd, self.n_embd, bias=False)
        self.c_proj.weight.data.zero_() # zero init suggested by @Grad62304977
        self.rotary = Rotary(self.head_dim)
        self.lamb = nn.Parameter(torch.tensor(0.5)) # @Grad62304977

    def forward(self, x, v1=None):
        B, T, C = x.size() # batch size, sequence length, embedding dimensionality (n_embd)
        q = self.c_q(x).view(B, T, self.n_head, self.head_dim)
        k = self.c_k(x).view(B, T, self.n_head, self.head_dim)
        v = self.c_v(x).view(B, T, self.n_head, self.head_dim)
        if v1 is None:
            v1 = v # This happens if we are in the first block. v needs to be accessed by subsequent blocks
        v = (1 - self.lamb) * v + self.lamb * v1.view_as(v) # @Grad62304977
        cos, sin = self.rotary(q)
        q, k = F.rms_norm(q, (q.size(-1),)), F.rms_norm(k, (k.size(-1),)) # QK norm suggested by @Grad62304977
        q, k = apply_rotary_emb(q, cos, sin), apply_rotary_emb(k, cos, sin)
        y = F.scaled_dot_product_attention(q.transpose(1, 2), k.transpose(1, 2), v.transpose(1, 2), is_causal=True)
        y = y.transpose(1, 2).contiguous().view_as(x) # re-assemble all head outputs side by side
        y = self.c_proj(y)
        return y, v1

class MLP(nn.Module):

    def __init__(self, config):
        super().__init__()
        self.c_fc    = CastedLinear(config.n_embd, 4 * config.n_embd, bias=False)
        self.c_proj  = CastedLinear(4 * config.n_embd, config.n_embd, bias=False)
        self.c_proj.weight.data.zero_() # zero init suggested by @Grad62304977

    def forward(self, x):
        x = self.c_fc(x)
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = self.c_proj(x)
        return x

class Block(nn.Module):

    def __init__(self, config):
        super().__init__()
        self.attn = CausalSelfAttention(config)
        self.mlp = MLP(config)
        self.lambdas = nn.Parameter(torch.tensor([1., 0.]))

    def forward(self, x, v1, x0):
        x = self.lambdas[0] * x + self.lambdas[1] * x0
        x1, v1 = self.attn(F.rms_norm(x, (x.size(-1),)), v1)
        x = x + x1
        x = x + self.mlp(F.rms_norm(x, (x.size(-1),)))
        return x, v1

# -----------------------------------------------------------------------------
# The main GPT-2 model

@dataclass
class GPTConfig:
    vocab_size : int = 50304
    n_layer : int = 12
    n_head : int = 6 # head dim 128 suggested by @Grad62304977
    n_embd : int = 768

class GPT(nn.Module):

    def __init__(self, config):
        super().__init__()
        self.config = config

        self.transformer = nn.ModuleDict(dict(
            wte = nn.Embedding(config.vocab_size, config.n_embd),
            h = nn.ModuleList([Block(config) for _ in range(config.n_layer)]),
        ))
        self.lm_head = CastedLinear(config.n_embd, config.vocab_size, bias=False)
        self.lm_head.weight.data.zero_() # @Grad62304977

    def forward(self, idx, target):

        # forward the GPT model itself
        x = self.transformer.wte(idx) # token embeddings of shape (b, t, n_embd)
        x = F.rms_norm(x, (x.size(-1),)) # @Grad62304977
        x0 = x
        v1 = None
        for block in self.transformer.h:
            x, v1 = block(x, v1, x0)
        x = F.rms_norm(x, (x.size(-1),))

        logits = self.lm_head(x)
        logits = 30 * torch.tanh(logits / 30) # @Grad62304977
        logits = logits.float()
        loss = F.cross_entropy(logits.view(-1, logits.size(-1)), target.view(-1))
        return loss.float()

# -----------------------------------------------------------------------------
# Our own simple Distributed Data Loader

def _peek_data_shard(filename):
    # only reads the header, returns header data
    with open(filename, "rb") as f:
        # first read the header, which is 256 int32 integers (4 bytes each)
        header = np.frombuffer(f.read(256*4), dtype=np.int32)
    if header[0] != 20240520:
        print("ERROR: magic number mismatch in the data .bin file!")
        print("---> HINT: Are you passing in a correct file with --input_bin?")
        print("---> HINT: Dataset encoding changed recently, re-run data prepro or refer again to README")
        print("---> HINT: For example re-run: `python dev/data/tinyshakespeare.py`, then re-try")
        exit(1)
    assert header[1] == 1, "unsupported version"
    ntok = header[2] # number of tokens (claimed)
    return ntok # for now just return the number of tokens

def _load_data_shard(filename):
    with open(filename, "rb") as f:
        # first read the header, which is 256 int32 integers (4 bytes each)
        header = np.frombuffer(f.read(256*4), dtype=np.int32)
        assert header[0] == 20240520, "magic number mismatch in the data .bin file"
        assert header[1] == 1, "unsupported version"
        ntok = header[2] # number of tokens (claimed)
        # the rest of it are tokens, stored as uint16
        tokens = np.frombuffer(f.read(), dtype=np.uint16)
    assert len(tokens) == ntok, "number of tokens read does not match header?"
    return tokens

class DistributedDataLoader:
    def __init__(self, filename_pattern, B, T, process_rank, num_processes):
        self.process_rank = process_rank
        self.num_processes = num_processes
        self.B = B
        self.T = T

        # glob files that match the pattern
        self.files = sorted(glob.glob(filename_pattern))
        assert len(self.files) > 0, f"did not find any files that match the pattern {filename_pattern}"

        # load and validate all data shards, count number of tokens in total
        ntok_total = 0
        for fname in self.files:
            shard_ntok = _peek_data_shard(fname)
            assert shard_ntok >= num_processes * B * T + 1
            ntok_total += int(shard_ntok)
        self.ntok_total = ntok_total

        # kick things off
        self.reset()

    def reset(self):
        self.current_shard = 0
        self.current_position = self.process_rank * self.B * self.T
        self.tokens = _load_data_shard(self.files[self.current_shard])

    def advance(self): # advance to next data shard
        self.current_shard = (self.current_shard + 1) % len(self.files)
        self.current_position = self.process_rank * self.B * self.T
        self.tokens = _load_data_shard(self.files[self.current_shard])

    def next_batch(self):
        B = self.B
        T = self.T
        buf = self.tokens[self.current_position : self.current_position+B*T+1]
        buf = torch.tensor(buf.astype(np.int32), dtype=torch.long)
        x = (buf[:-1]).view(B, T) # inputs
        y = (buf[1:]).view(B, T) # targets
        # advance current position and load next shard if necessary
        self.current_position += B * T * self.num_processes
        if self.current_position + (B * T * self.num_processes + 1) > len(self.tokens):
            self.advance()
        return x.cuda(), y.cuda()

# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data hyperparams
    input_bin : str = 'data/fineweb10B/fineweb_train_*.bin' # input .bin to train on
    input_val_bin : str = 'data/fineweb10B/fineweb_val_*.bin' # input .bin to eval validation loss on
    # optimization hyperparams
    batch_size : int = 8*64 # batch size, in sequences, across all devices
    device_batch_size : int = 64 # batch size, in sequences, per device
    sequence_length : int = 1024 # sequence length, in tokens
    num_iterations : int = 1000 # number of iterations to run
    warmup_iters : int = 0
    warmdown_iters : int = 500 # number of iterations of linear warmup/warmdown for triangular or trapezoidal schedule
    weight_decay : float = 0
    # evaluation and logging hyperparams
    val_loss_every : int = 125 # every how many steps to evaluate val loss? 0 for only at the end
    val_tokens : int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    save_every : int = 0 # every how many steps to save the checkpoint? 0 for only at the end
args = Hyperparameters()

# set up DDP (distributed data parallel). torchrun sets this env variable
assert torch.cuda.is_available()
dist.init_process_group(backend='nccl')
ddp_rank = int(os.environ['RANK'])
ddp_local_rank = int(os.environ['LOCAL_RANK'])
ddp_world_size = int(os.environ['WORLD_SIZE'])
device = f'cuda:{ddp_local_rank}'
torch.cuda.set_device(device)
print(f"using device: {device}")
master_process = (ddp_rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = str(uuid.uuid4())
    logdir = 'logs/%s/' % run_id
    os.makedirs(logdir, exist_ok=True)
    logfile = 'logs/%s.txt' % run_id
    # create the log file
    with open(logfile, "w") as f:
        # begin the log by printing this file (the Python code)
        f.write('='*100 + '\n')
        f.write(code)
        f.write('='*100 + '\n')
def print0(s, logonly=False):
    if master_process:
        with open(logfile, "a") as f:
            if not logonly:
                print(s)
            f.write(s+'\n')
# log information about the hardware/software environment this is running on
# and print the full `nvidia-smi` to file
print0(f"Running pytorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}\nnvidia-smi:")
import subprocess
result = subprocess.run(['nvidia-smi'], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)
print0(f'{result.stdout}', logonly=True)
print0('='*100, logonly=True)

# convenience variables
B, T = args.device_batch_size, args.sequence_length
# calculate the number of steps to take in the val loop.
assert args.val_tokens % (B * T * ddp_world_size) == 0
val_steps = args.val_tokens // (B * T * ddp_world_size)
# calculate the steps of gradient accumulation required to attain the desired global batch size.
assert args.batch_size % (B * ddp_world_size) == 0
train_accumulation_steps = args.batch_size // (B * ddp_world_size)

# load tokens
train_loader = DistributedDataLoader(args.input_bin, B, T, ddp_rank, ddp_world_size)
val_loader = DistributedDataLoader(args.input_val_bin, B, T, ddp_rank, ddp_world_size)
print0(f"Training DataLoader: total number of tokens: {train_loader.ntok_total} across {len(train_loader.files)} files")
print0(f"Validation DataLoader: total number of tokens: {val_loader.ntok_total} across {len(val_loader.files)} files")
print0('='*100, logonly=True)
x, y = train_loader.next_batch()

# there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency. suggested to me by @Grad62304977.
# this originates from Karpathy's experiments.
num_vocab = 50304
model = GPT(GPTConfig(vocab_size=num_vocab, n_layer=12, n_head=6, n_embd=768))
model = model.cuda().bfloat16()
for m in model.modules():
    if isinstance(m, CastedLinear):
        m.float()
if hasattr(config, "coordinate_descent_tuning"):
    config.coordinate_descent_tuning = True # suggested by @Chillee
model = torch.compile(model)
# here we wrap model into DDP container
model = DDP(model, device_ids=[ddp_local_rank])
raw_model = model.module # always contains the "raw" unwrapped model

# CUDNN attention is ~4ms faster than Flash, but doesn't get selected by default in PyTorch 2.5.1
from torch.backends.cuda import enable_cudnn_sdp, enable_flash_sdp, enable_math_sdp, enable_mem_efficient_sdp
enable_cudnn_sdp(True)
enable_flash_sdp(False)
enable_mem_efficient_sdp(False)
enable_math_sdp(False)

# init the optimizer(s)
optimizer1 = torch.optim.Adam([raw_model.transformer.wte.weight], lr=0.3,   betas=(0.9, 0.95), fused=True)
optimizer2 = torch.optim.Adam([raw_model.lm_head.weight],         lr=0.002, betas=(0.9, 0.95), fused=True)

# Collect per-layer parameters
layer_matrix_params = []
layer_scalar_params = []
for i, layer in enumerate(raw_model.transformer.h):
    matrix_params = [p for p in layer.parameters() if p.ndim == 2]
    scalar_params = [p for p in layer.parameters() if p.ndim < 2]
    layer_matrix_params.append((i, matrix_params))
    layer_scalar_params.append((i, scalar_params))

# Create per-layer parameter groups with initial learning rates
param_groups_muon = []
for i, matrix_params in layer_matrix_params:
    param_groups_muon.append({'params': matrix_params, 'layer': i, 'lr': 0.14})

param_groups_adam = []
for i, scalar_params in layer_scalar_params:
    param_groups_adam.append({'params': scalar_params, 'layer': i, 'lr': 0.04})

# Initialize the optimizers with the parameter groups
optimizer3 = Muon(param_groups_muon, momentum=0.97)
optimizer4 = torch.optim.Adam(param_groups_adam, betas=(0.9, 0.95), fused=True)
optimizers = [optimizer1, optimizer2, optimizer3, optimizer4]
# learning rate decay scheduler (linear warmup and warmdown)
def base_lr_multiplier(it):
    assert it <= args.num_iterations
    # 1) linear warmup for warmup_iters steps
    if it < args.warmup_iters:
        return (it+1) / args.warmup_iters
    # 2) constant lr for a while
    elif it < args.num_iterations - args.warmdown_iters:
        return 1.0
    # 3) linear warmdown
    else:
        decay_ratio = (args.num_iterations - it) / args.warmdown_iters
        return decay_ratio

def transformer_weight_lr(layer, step):
    return 0.55 + 0.45 * np.cos(np.pi * (layer / 12 ))

def make_lr_lambda(layer):
    return lambda step: base_lr_multiplier(step) * transformer_weight_lr(layer, step)

scheduler1 = torch.optim.lr_scheduler.LambdaLR(optimizer1, lr_lambda=base_lr_multiplier)
scheduler2 = torch.optim.lr_scheduler.LambdaLR(optimizer2, lr_lambda=base_lr_multiplier)
lambdas_muon = [make_lr_lambda(param_group['layer']) for param_group in optimizer3.param_groups]
scheduler3 = torch.optim.lr_scheduler.LambdaLR(optimizer3, lr_lambda=lambdas_muon)
lambdas_adam = [make_lr_lambda(param_group['layer']) for param_group in optimizer4.param_groups]
scheduler4 = torch.optim.lr_scheduler.LambdaLR(optimizer4, lr_lambda=lambdas_adam)
schedulers = [scheduler1, scheduler2, scheduler3, scheduler4]

# Start training loop
training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.time()
# begin training
train_loader.reset()
for step in range(args.num_iterations + 1):
    last_step = (step == args.num_iterations)
    # This effectively ignores timing first 10 steps, which are slower for weird reasons.
    # Alternately, and slightly more correctly in terms of benchmarking, we could do 10
    # steps with dummy data first, and then re-initialize the model and reset the loader.
    if step == 10:
        training_time_ms = 0
        t0 = time.time()
    timed_steps = float('nan') if step <= 11 else (step - 10) + 1 # <= 11 to avoid bug in val

    # once in a while evaluate the validation dataset
    if (last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0)):
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.time() - t0)
        # run validation batches
        model.eval()
        val_loader.reset()
        val_loss = 0.0
        for _ in range(val_steps):
            with torch.no_grad():
                x_val, y_val = val_loader.next_batch()
                val_loss += model(x_val, y_val)
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        val_loss /= val_steps
        # log val loss to console and to logfile
        print0(f'step:{step}/{args.num_iterations} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/(timed_steps-1):.2f}ms')
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.time()

    if master_process and (last_step or (args.save_every > 0 and step % args.save_every == 0)):
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.time() - t0)
        # save the state of the training process
        log = dict(step=step, code=code, model=raw_model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
        torch.save(log, 'logs/%s/state_step%06d.pt' % (run_id, step))
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.time()

    # bit confusing: we want to make sure to eval on 0th iteration
    # but also after the very last iteration. so we loop for step <= num_iterations
    # instead of just < num_iterations (one extra due to <=), only to do
    # the validation/sampling one last time, and then we break right here as we're done.
    if last_step:
        break

    # --------------- TRAINING SECTION BEGIN -----------------
    model.train()
    for i in range(1, train_accumulation_steps+1):
        # forward pass
        loss = model(x, y)
        train_loss = loss.detach()
        # advance the dataset for the next batch
        x, y = train_loader.next_batch()
        # backward pass
        if i < train_accumulation_steps:
            with model.no_sync(): # there's no need to sync gradients every accumulation step
                loss.backward()
        else:
            loss.backward() # just sync on the last step
    for p in model.parameters():
        p.grad /= train_accumulation_steps
    # momentum warmup for Muon
    frac = min(step/500, 1)
    optimizer3.param_groups[0]['momentum'] = (1 - frac) * 0.85 + frac * 0.95
    # step the optimizers and schedulers
    for opt, sched in zip(optimizers, schedulers):
        opt.step()
        sched.step()
    # null the gradients
    model.zero_grad(set_to_none=True)
    # --------------- TRAINING SECTION END -------------------
    # everything that follows now is just diagnostics, prints, logging, etc.

    #dist.all_reduce(train_loss, op=dist.ReduceOp.AVG) # all-reducing the training loss would be more correct in terms of logging, but slower
    approx_time = training_time_ms + 1000 * (time.time() - t0)
    print0(f"step:{step+1}/{args.num_iterations} train_loss:{train_loss.item():.4f} train_time:{approx_time:.0f}ms step_avg:{approx_time/timed_steps:.2f}ms")

if master_process:
    print(f"peak memory consumption: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB")

# -------------------------------------------------------------------------
# clean up nice
dist.destroy_process_group()
====================================================================================================
Running pytorch 2.5.1+cu124 compiled for CUDA 12.4
nvidia-smi:
Tue Nov 12 19:17:22 2024       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 550.90.12              Driver Version: 550.90.12      CUDA Version: 12.6     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA H100 80GB HBM3          On  |   00000000:61:00.0 Off |                    0 |
| N/A   41C    P0             82W /  700W |       4MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  |   00000000:62:00.0 Off |                    0 |
| N/A   35C    P0            121W /  700W |     530MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  |   00000000:63:00.0 Off |                    0 |
| N/A   35C    P0            113W /  700W |     530MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  |   00000000:64:00.0 Off |                    0 |
| N/A   33C    P0            117W /  700W |     530MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  |   00000000:6A:00.0 Off |                    0 |
| N/A   34C    P0            118W /  700W |     530MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  |   00000000:6B:00.0 Off |                    0 |
| N/A   35C    P0            119W /  700W |     530MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  |   00000000:6C:00.0 Off |                    0 |
| N/A   34C    P0            116W /  700W |     530MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  |   00000000:6D:00.0 Off |                    0 |
| N/A   32C    P0            120W /  700W |     530MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
+-----------------------------------------------------------------------------------------+

====================================================================================================
Training DataLoader: total number of tokens: 2000000000 across 20 files
Validation DataLoader: total number of tokens: 100000000 across 1 files
====================================================================================================
step:0/1000 val_loss:10.8258 train_time:230ms step_avg:nanms
step:1/1000 train_loss:10.8258 train_time:61669ms step_avg:nanms
step:2/1000 train_loss:10.4283 train_time:61791ms step_avg:nanms
step:3/1000 train_loss:9.7131 train_time:61932ms step_avg:nanms
step:4/1000 train_loss:8.6351 train_time:62076ms step_avg:nanms
step:5/1000 train_loss:8.1133 train_time:62221ms step_avg:nanms
step:6/1000 train_loss:7.6146 train_time:62365ms step_avg:nanms
step:7/1000 train_loss:7.3226 train_time:62509ms step_avg:nanms
step:8/1000 train_loss:7.3926 train_time:62662ms step_avg:nanms
step:9/1000 train_loss:7.1994 train_time:62812ms step_avg:nanms
step:10/1000 train_loss:6.8712 train_time:62957ms step_avg:nanms
step:11/1000 train_loss:6.8673 train_time:123ms step_avg:nanms
step:12/1000 train_loss:6.7576 train_time:267ms step_avg:nanms
step:13/1000 train_loss:6.6053 train_time:414ms step_avg:137.90ms
step:14/1000 train_loss:6.6380 train_time:560ms step_avg:140.06ms
step:15/1000 train_loss:6.5823 train_time:710ms step_avg:142.00ms
step:16/1000 train_loss:6.5102 train_time:858ms step_avg:143.05ms
step:17/1000 train_loss:6.5023 train_time:1005ms step_avg:143.56ms
step:18/1000 train_loss:6.5349 train_time:1151ms step_avg:143.86ms
step:19/1000 train_loss:6.3701 train_time:1298ms step_avg:144.20ms
step:20/1000 train_loss:6.3997 train_time:1443ms step_avg:144.27ms
step:21/1000 train_loss:6.0733 train_time:1590ms step_avg:144.58ms
step:22/1000 train_loss:6.4201 train_time:1739ms step_avg:144.95ms
step:23/1000 train_loss:6.6836 train_time:1886ms step_avg:145.11ms
step:24/1000 train_loss:6.3033 train_time:2036ms step_avg:145.43ms
step:25/1000 train_loss:6.4528 train_time:2182ms step_avg:145.48ms
step:26/1000 train_loss:6.1568 train_time:2330ms step_avg:145.60ms
step:27/1000 train_loss:6.0766 train_time:2478ms step_avg:145.79ms
step:28/1000 train_loss:6.2224 train_time:2626ms step_avg:145.89ms
step:29/1000 train_loss:5.8904 train_time:2773ms step_avg:145.96ms
step:30/1000 train_loss:6.1285 train_time:2921ms step_avg:146.05ms
step:31/1000 train_loss:5.9776 train_time:3070ms step_avg:146.18ms
step:32/1000 train_loss:5.9320 train_time:3218ms step_avg:146.27ms
step:33/1000 train_loss:5.7565 train_time:3364ms step_avg:146.26ms
step:34/1000 train_loss:6.0883 train_time:3513ms step_avg:146.38ms
step:35/1000 train_loss:5.9813 train_time:3662ms step_avg:146.48ms
step:36/1000 train_loss:6.1335 train_time:3809ms step_avg:146.50ms
step:37/1000 train_loss:6.0278 train_time:3956ms step_avg:146.52ms
step:38/1000 train_loss:5.9221 train_time:4104ms step_avg:146.57ms
step:39/1000 train_loss:5.8150 train_time:4249ms step_avg:146.52ms
step:40/1000 train_loss:5.8056 train_time:4399ms step_avg:146.63ms
step:41/1000 train_loss:5.7329 train_time:4544ms step_avg:146.59ms
step:42/1000 train_loss:5.7505 train_time:4691ms step_avg:146.60ms
step:43/1000 train_loss:5.6316 train_time:4839ms step_avg:146.65ms
step:44/1000 train_loss:5.7090 train_time:4986ms step_avg:146.65ms
step:45/1000 train_loss:5.7029 train_time:5135ms step_avg:146.72ms
step:46/1000 train_loss:5.8496 train_time:5281ms step_avg:146.71ms
step:47/1000 train_loss:5.6450 train_time:5430ms step_avg:146.77ms
step:48/1000 train_loss:5.5076 train_time:5578ms step_avg:146.80ms
step:49/1000 train_loss:5.7123 train_time:5725ms step_avg:146.80ms
step:50/1000 train_loss:5.5964 train_time:5873ms step_avg:146.83ms
step:51/1000 train_loss:5.7499 train_time:6020ms step_avg:146.84ms
step:52/1000 train_loss:5.6019 train_time:6167ms step_avg:146.84ms
step:53/1000 train_loss:5.4600 train_time:6316ms step_avg:146.89ms
step:54/1000 train_loss:5.6043 train_time:6463ms step_avg:146.89ms
step:55/1000 train_loss:5.4705 train_time:6611ms step_avg:146.90ms
step:56/1000 train_loss:5.8352 train_time:6759ms step_avg:146.94ms
step:57/1000 train_loss:5.4758 train_time:6906ms step_avg:146.93ms
step:58/1000 train_loss:5.3528 train_time:7053ms step_avg:146.94ms
step:59/1000 train_loss:5.5043 train_time:7201ms step_avg:146.95ms
step:60/1000 train_loss:5.4707 train_time:7348ms step_avg:146.95ms
step:61/1000 train_loss:5.5589 train_time:7496ms step_avg:146.98ms
step:62/1000 train_loss:5.3187 train_time:7644ms step_avg:147.00ms
step:63/1000 train_loss:5.4374 train_time:7791ms step_avg:147.00ms
step:64/1000 train_loss:5.4057 train_time:7939ms step_avg:147.01ms
step:65/1000 train_loss:5.2808 train_time:8084ms step_avg:146.99ms
step:66/1000 train_loss:5.2288 train_time:8233ms step_avg:147.02ms
step:67/1000 train_loss:5.3940 train_time:8380ms step_avg:147.01ms
step:68/1000 train_loss:5.2540 train_time:8529ms step_avg:147.05ms
step:69/1000 train_loss:5.5415 train_time:8677ms step_avg:147.07ms
step:70/1000 train_loss:5.1651 train_time:8824ms step_avg:147.07ms
step:71/1000 train_loss:5.2572 train_time:8971ms step_avg:147.07ms
step:72/1000 train_loss:5.4076 train_time:9119ms step_avg:147.07ms
step:73/1000 train_loss:5.3456 train_time:9266ms step_avg:147.08ms
step:74/1000 train_loss:5.2308 train_time:9414ms step_avg:147.10ms
step:75/1000 train_loss:5.3463 train_time:9562ms step_avg:147.11ms
step:76/1000 train_loss:5.3521 train_time:9711ms step_avg:147.13ms
step:77/1000 train_loss:5.2907 train_time:9858ms step_avg:147.13ms
step:78/1000 train_loss:5.3811 train_time:10005ms step_avg:147.13ms
step:79/1000 train_loss:5.5045 train_time:10151ms step_avg:147.12ms
step:80/1000 train_loss:5.2454 train_time:10299ms step_avg:147.13ms
step:81/1000 train_loss:5.3534 train_time:10446ms step_avg:147.13ms
step:82/1000 train_loss:5.0986 train_time:10595ms step_avg:147.16ms
step:83/1000 train_loss:5.3039 train_time:10742ms step_avg:147.16ms
step:84/1000 train_loss:5.2381 train_time:10891ms step_avg:147.18ms
step:85/1000 train_loss:5.2386 train_time:11040ms step_avg:147.20ms
step:86/1000 train_loss:5.1032 train_time:11188ms step_avg:147.21ms
step:87/1000 train_loss:5.3136 train_time:11336ms step_avg:147.22ms
step:88/1000 train_loss:5.2084 train_time:11482ms step_avg:147.21ms
step:89/1000 train_loss:5.2806 train_time:11631ms step_avg:147.22ms
step:90/1000 train_loss:5.2557 train_time:11778ms step_avg:147.23ms
step:91/1000 train_loss:5.1617 train_time:11925ms step_avg:147.23ms
step:92/1000 train_loss:5.1673 train_time:12074ms step_avg:147.25ms
step:93/1000 train_loss:5.2835 train_time:12222ms step_avg:147.25ms
step:94/1000 train_loss:5.1219 train_time:12368ms step_avg:147.24ms
step:95/1000 train_loss:5.1245 train_time:12518ms step_avg:147.27ms
step:96/1000 train_loss:5.1727 train_time:12666ms step_avg:147.28ms
step:97/1000 train_loss:5.0783 train_time:12813ms step_avg:147.28ms
step:98/1000 train_loss:5.1583 train_time:12962ms step_avg:147.29ms
step:99/1000 train_loss:5.0909 train_time:13110ms step_avg:147.30ms
step:100/1000 train_loss:5.2143 train_time:13257ms step_avg:147.31ms
step:101/1000 train_loss:5.1797 train_time:13404ms step_avg:147.29ms
step:102/1000 train_loss:5.1009 train_time:13551ms step_avg:147.29ms
step:103/1000 train_loss:5.1881 train_time:13698ms step_avg:147.29ms
step:104/1000 train_loss:5.1511 train_time:13846ms step_avg:147.29ms
step:105/1000 train_loss:4.9969 train_time:13994ms step_avg:147.31ms
step:106/1000 train_loss:5.0916 train_time:14142ms step_avg:147.31ms
step:107/1000 train_loss:5.3065 train_time:14289ms step_avg:147.31ms
step:108/1000 train_loss:5.0819 train_time:14437ms step_avg:147.31ms
step:109/1000 train_loss:4.8647 train_time:14584ms step_avg:147.32ms
step:110/1000 train_loss:5.0489 train_time:14734ms step_avg:147.34ms
step:111/1000 train_loss:5.0436 train_time:14882ms step_avg:147.35ms
step:112/1000 train_loss:5.0128 train_time:15027ms step_avg:147.32ms
step:113/1000 train_loss:5.1247 train_time:15176ms step_avg:147.34ms
step:114/1000 train_loss:5.0401 train_time:15323ms step_avg:147.34ms
step:115/1000 train_loss:4.9022 train_time:15471ms step_avg:147.34ms
step:116/1000 train_loss:5.0614 train_time:15620ms step_avg:147.35ms
step:117/1000 train_loss:4.9613 train_time:15767ms step_avg:147.35ms
step:118/1000 train_loss:4.9266 train_time:15917ms step_avg:147.38ms
step:119/1000 train_loss:5.0803 train_time:16064ms step_avg:147.37ms
step:120/1000 train_loss:5.0383 train_time:16212ms step_avg:147.38ms
step:121/1000 train_loss:4.9726 train_time:16361ms step_avg:147.39ms
step:122/1000 train_loss:4.8749 train_time:16508ms step_avg:147.40ms
step:123/1000 train_loss:4.9810 train_time:16657ms step_avg:147.41ms
step:124/1000 train_loss:4.8529 train_time:16804ms step_avg:147.41ms
step:125/1000 train_loss:5.1632 train_time:16952ms step_avg:147.41ms
step:125/1000 val_loss:4.9885 train_time:16976ms step_avg:147.62ms
step:126/1000 train_loss:5.0220 train_time:17111ms step_avg:147.51ms
step:127/1000 train_loss:4.9843 train_time:17260ms step_avg:147.52ms
step:128/1000 train_loss:5.0369 train_time:17408ms step_avg:147.52ms
step:129/1000 train_loss:4.9099 train_time:17554ms step_avg:147.51ms
step:130/1000 train_loss:5.2256 train_time:17703ms step_avg:147.53ms
step:131/1000 train_loss:4.9853 train_time:17848ms step_avg:147.50ms
step:132/1000 train_loss:4.9870 train_time:17999ms step_avg:147.53ms
step:133/1000 train_loss:4.9372 train_time:18149ms step_avg:147.55ms
step:134/1000 train_loss:4.9763 train_time:18298ms step_avg:147.57ms
step:135/1000 train_loss:4.8990 train_time:18444ms step_avg:147.55ms
step:136/1000 train_loss:4.9945 train_time:18591ms step_avg:147.54ms
step:137/1000 train_loss:4.7849 train_time:18737ms step_avg:147.54ms
step:138/1000 train_loss:4.9396 train_time:18884ms step_avg:147.53ms
step:139/1000 train_loss:4.9087 train_time:19032ms step_avg:147.54ms
step:140/1000 train_loss:4.9171 train_time:19183ms step_avg:147.56ms
step:141/1000 train_loss:4.9965 train_time:19331ms step_avg:147.57ms
step:142/1000 train_loss:4.8771 train_time:19480ms step_avg:147.57ms
step:143/1000 train_loss:4.9447 train_time:19626ms step_avg:147.56ms
step:144/1000 train_loss:4.7981 train_time:19773ms step_avg:147.56ms
step:145/1000 train_loss:4.9279 train_time:19920ms step_avg:147.56ms
step:146/1000 train_loss:4.8771 train_time:20069ms step_avg:147.56ms
step:147/1000 train_loss:4.7636 train_time:20216ms step_avg:147.57ms
step:148/1000 train_loss:4.9214 train_time:20367ms step_avg:147.58ms
step:149/1000 train_loss:4.9028 train_time:20513ms step_avg:147.58ms
step:150/1000 train_loss:4.9367 train_time:20662ms step_avg:147.59ms
step:151/1000 train_loss:4.9726 train_time:20809ms step_avg:147.58ms
step:152/1000 train_loss:4.8728 train_time:20955ms step_avg:147.57ms
step:153/1000 train_loss:4.8647 train_time:21105ms step_avg:147.59ms
step:154/1000 train_loss:4.9567 train_time:21252ms step_avg:147.58ms
step:155/1000 train_loss:4.9051 train_time:21400ms step_avg:147.58ms
step:156/1000 train_loss:4.8521 train_time:21547ms step_avg:147.58ms
step:157/1000 train_loss:4.8868 train_time:21694ms step_avg:147.58ms
step:158/1000 train_loss:5.0071 train_time:21842ms step_avg:147.58ms
step:159/1000 train_loss:4.7974 train_time:21990ms step_avg:147.58ms
step:160/1000 train_loss:4.8667 train_time:22137ms step_avg:147.58ms
step:161/1000 train_loss:4.7059 train_time:22286ms step_avg:147.59ms
step:162/1000 train_loss:4.8876 train_time:22434ms step_avg:147.59ms
step:163/1000 train_loss:4.9104 train_time:22583ms step_avg:147.60ms
step:164/1000 train_loss:4.9134 train_time:22730ms step_avg:147.60ms
step:165/1000 train_loss:4.7202 train_time:22878ms step_avg:147.60ms
step:166/1000 train_loss:4.8465 train_time:23026ms step_avg:147.60ms
step:167/1000 train_loss:4.9972 train_time:23174ms step_avg:147.60ms
step:168/1000 train_loss:4.7770 train_time:23322ms step_avg:147.61ms
step:169/1000 train_loss:4.8933 train_time:23470ms step_avg:147.61ms
step:170/1000 train_loss:4.7357 train_time:23617ms step_avg:147.61ms
step:171/1000 train_loss:4.6383 train_time:23766ms step_avg:147.62ms
step:172/1000 train_loss:4.8029 train_time:23913ms step_avg:147.61ms
step:173/1000 train_loss:4.7725 train_time:24062ms step_avg:147.62ms
step:174/1000 train_loss:4.8284 train_time:24209ms step_avg:147.62ms
step:175/1000 train_loss:4.9720 train_time:24357ms step_avg:147.62ms
step:176/1000 train_loss:4.8477 train_time:24505ms step_avg:147.62ms
step:177/1000 train_loss:4.6856 train_time:24652ms step_avg:147.62ms
step:178/1000 train_loss:4.6782 train_time:24803ms step_avg:147.64ms
step:179/1000 train_loss:4.7246 train_time:24950ms step_avg:147.63ms
step:180/1000 train_loss:4.7537 train_time:25098ms step_avg:147.64ms
step:181/1000 train_loss:4.7430 train_time:25246ms step_avg:147.64ms
step:182/1000 train_loss:4.8650 train_time:25394ms step_avg:147.64ms
step:183/1000 train_loss:4.7381 train_time:25542ms step_avg:147.64ms
step:184/1000 train_loss:4.6787 train_time:25691ms step_avg:147.65ms
step:185/1000 train_loss:4.6980 train_time:25839ms step_avg:147.65ms
step:186/1000 train_loss:4.8189 train_time:25986ms step_avg:147.65ms
step:187/1000 train_loss:4.7277 train_time:26133ms step_avg:147.65ms
step:188/1000 train_loss:4.9621 train_time:26281ms step_avg:147.65ms
step:189/1000 train_loss:4.7614 train_time:26591ms step_avg:148.55ms
step:190/1000 train_loss:4.6880 train_time:26935ms step_avg:149.64ms
step:191/1000 train_loss:4.8303 train_time:27082ms step_avg:149.63ms
step:192/1000 train_loss:4.6751 train_time:27228ms step_avg:149.60ms
step:193/1000 train_loss:4.6149 train_time:27376ms step_avg:149.59ms
step:194/1000 train_loss:4.8359 train_time:27522ms step_avg:149.57ms
step:195/1000 train_loss:4.7538 train_time:27667ms step_avg:149.55ms
step:196/1000 train_loss:4.9429 train_time:27819ms step_avg:149.57ms
step:197/1000 train_loss:4.8213 train_time:27969ms step_avg:149.57ms
step:198/1000 train_loss:4.6625 train_time:28117ms step_avg:149.56ms
step:199/1000 train_loss:4.7206 train_time:28264ms step_avg:149.55ms
step:200/1000 train_loss:4.6052 train_time:28411ms step_avg:149.53ms
step:201/1000 train_loss:4.6765 train_time:28559ms step_avg:149.52ms
step:202/1000 train_loss:4.5976 train_time:28707ms step_avg:149.52ms
step:203/1000 train_loss:4.8391 train_time:28857ms step_avg:149.52ms
step:204/1000 train_loss:4.7316 train_time:29007ms step_avg:149.52ms
step:205/1000 train_loss:4.7187 train_time:29153ms step_avg:149.50ms
step:206/1000 train_loss:4.8718 train_time:29303ms step_avg:149.51ms
step:207/1000 train_loss:4.5412 train_time:29450ms step_avg:149.49ms
step:208/1000 train_loss:4.6866 train_time:29599ms step_avg:149.49ms
step:209/1000 train_loss:4.6619 train_time:29747ms step_avg:149.48ms
step:210/1000 train_loss:4.8176 train_time:29896ms step_avg:149.48ms
step:211/1000 train_loss:4.7549 train_time:30044ms step_avg:149.47ms
step:212/1000 train_loss:4.6235 train_time:30192ms step_avg:149.47ms
step:213/1000 train_loss:4.7633 train_time:30340ms step_avg:149.46ms
step:214/1000 train_loss:4.6003 train_time:30486ms step_avg:149.44ms
step:215/1000 train_loss:4.6858 train_time:30634ms step_avg:149.44ms
step:216/1000 train_loss:4.5364 train_time:30783ms step_avg:149.43ms
step:217/1000 train_loss:4.6618 train_time:30931ms step_avg:149.42ms
step:218/1000 train_loss:4.6517 train_time:31080ms step_avg:149.42ms
step:219/1000 train_loss:4.6369 train_time:31227ms step_avg:149.41ms
step:220/1000 train_loss:4.6448 train_time:31376ms step_avg:149.41ms
step:221/1000 train_loss:4.6799 train_time:31523ms step_avg:149.40ms
step:222/1000 train_loss:4.7135 train_time:31671ms step_avg:149.39ms
step:223/1000 train_loss:4.6289 train_time:31818ms step_avg:149.38ms
step:224/1000 train_loss:4.6407 train_time:31967ms step_avg:149.38ms
step:225/1000 train_loss:4.7974 train_time:32114ms step_avg:149.37ms
step:226/1000 train_loss:4.5495 train_time:32264ms step_avg:149.37ms
step:227/1000 train_loss:4.5483 train_time:32411ms step_avg:149.36ms
step:228/1000 train_loss:4.5438 train_time:32561ms step_avg:149.36ms
step:229/1000 train_loss:4.7205 train_time:32708ms step_avg:149.35ms
step:230/1000 train_loss:4.5532 train_time:32857ms step_avg:149.35ms
step:231/1000 train_loss:4.6636 train_time:33006ms step_avg:149.35ms
step:232/1000 train_loss:4.5366 train_time:33154ms step_avg:149.34ms
step:233/1000 train_loss:4.5187 train_time:33303ms step_avg:149.34ms
step:234/1000 train_loss:4.7142 train_time:33451ms step_avg:149.33ms
step:235/1000 train_loss:4.5497 train_time:33599ms step_avg:149.33ms
step:236/1000 train_loss:4.5107 train_time:33746ms step_avg:149.32ms
step:237/1000 train_loss:4.7438 train_time:33895ms step_avg:149.32ms
step:238/1000 train_loss:4.6478 train_time:34043ms step_avg:149.31ms
step:239/1000 train_loss:4.5413 train_time:34191ms step_avg:149.31ms
step:240/1000 train_loss:4.6929 train_time:34341ms step_avg:149.31ms
step:241/1000 train_loss:4.6813 train_time:34488ms step_avg:149.30ms
step:242/1000 train_loss:4.5610 train_time:34636ms step_avg:149.29ms
step:243/1000 train_loss:4.7347 train_time:34784ms step_avg:149.29ms
step:244/1000 train_loss:4.5687 train_time:34933ms step_avg:149.29ms
step:245/1000 train_loss:4.6104 train_time:35082ms step_avg:149.29ms
step:246/1000 train_loss:4.6678 train_time:35230ms step_avg:149.28ms
step:247/1000 train_loss:4.6115 train_time:35378ms step_avg:149.28ms
step:248/1000 train_loss:4.5623 train_time:35526ms step_avg:149.27ms
step:249/1000 train_loss:4.7232 train_time:35676ms step_avg:149.27ms
step:250/1000 train_loss:4.4732 train_time:35823ms step_avg:149.26ms
step:250/1000 val_loss:4.5782 train_time:35847ms step_avg:149.36ms
step:251/1000 train_loss:4.5208 train_time:35981ms step_avg:149.30ms
step:252/1000 train_loss:4.6383 train_time:36131ms step_avg:149.30ms
step:253/1000 train_loss:4.6580 train_time:36277ms step_avg:149.29ms
step:254/1000 train_loss:4.5194 train_time:36425ms step_avg:149.28ms
step:255/1000 train_loss:4.5079 train_time:36571ms step_avg:149.27ms
step:256/1000 train_loss:4.6669 train_time:36717ms step_avg:149.26ms
step:257/1000 train_loss:4.6047 train_time:36869ms step_avg:149.27ms
step:258/1000 train_loss:4.5816 train_time:37018ms step_avg:149.27ms
step:259/1000 train_loss:4.5144 train_time:37170ms step_avg:149.28ms
step:260/1000 train_loss:4.5439 train_time:37317ms step_avg:149.27ms
step:261/1000 train_loss:4.5964 train_time:37464ms step_avg:149.26ms
step:262/1000 train_loss:4.6065 train_time:37611ms step_avg:149.25ms
step:263/1000 train_loss:4.5203 train_time:37758ms step_avg:149.24ms
step:264/1000 train_loss:4.4538 train_time:37908ms step_avg:149.24ms
step:265/1000 train_loss:4.5194 train_time:38057ms step_avg:149.24ms
step:266/1000 train_loss:4.3774 train_time:38208ms step_avg:149.25ms
step:267/1000 train_loss:4.4358 train_time:38356ms step_avg:149.24ms
step:268/1000 train_loss:4.4577 train_time:38504ms step_avg:149.24ms
step:269/1000 train_loss:4.4403 train_time:38652ms step_avg:149.23ms
step:270/1000 train_loss:4.3856 train_time:38798ms step_avg:149.22ms
step:271/1000 train_loss:4.6290 train_time:38948ms step_avg:149.23ms
step:272/1000 train_loss:4.5364 train_time:39096ms step_avg:149.22ms
step:273/1000 train_loss:4.4129 train_time:39246ms step_avg:149.22ms
step:274/1000 train_loss:4.4620 train_time:39394ms step_avg:149.22ms
step:275/1000 train_loss:4.5810 train_time:39542ms step_avg:149.21ms
step:276/1000 train_loss:4.5919 train_time:39691ms step_avg:149.21ms
step:277/1000 train_loss:4.7834 train_time:39837ms step_avg:149.20ms
step:278/1000 train_loss:4.5355 train_time:39984ms step_avg:149.19ms
step:279/1000 train_loss:4.6592 train_time:40134ms step_avg:149.20ms
step:280/1000 train_loss:4.5166 train_time:40281ms step_avg:149.19ms
step:281/1000 train_loss:4.5622 train_time:40430ms step_avg:149.19ms
step:282/1000 train_loss:4.4837 train_time:40577ms step_avg:149.18ms
step:283/1000 train_loss:4.5181 train_time:40725ms step_avg:149.18ms
step:284/1000 train_loss:4.4149 train_time:40873ms step_avg:149.17ms
step:285/1000 train_loss:4.5635 train_time:41021ms step_avg:149.17ms
step:286/1000 train_loss:4.5687 train_time:41170ms step_avg:149.17ms
step:287/1000 train_loss:4.5924 train_time:41317ms step_avg:149.16ms
step:288/1000 train_loss:4.4490 train_time:41469ms step_avg:149.17ms
step:289/1000 train_loss:4.5201 train_time:41616ms step_avg:149.16ms
step:290/1000 train_loss:4.3786 train_time:41766ms step_avg:149.16ms
step:291/1000 train_loss:4.3762 train_time:41913ms step_avg:149.16ms
step:292/1000 train_loss:4.4706 train_time:42063ms step_avg:149.16ms
step:293/1000 train_loss:4.3912 train_time:42211ms step_avg:149.16ms
step:294/1000 train_loss:4.4230 train_time:42359ms step_avg:149.15ms
step:295/1000 train_loss:4.4523 train_time:42509ms step_avg:149.15ms
step:296/1000 train_loss:4.3293 train_time:42656ms step_avg:149.15ms
step:297/1000 train_loss:4.3217 train_time:42805ms step_avg:149.15ms
step:298/1000 train_loss:4.3427 train_time:42953ms step_avg:149.14ms
step:299/1000 train_loss:4.4443 train_time:43101ms step_avg:149.14ms
step:300/1000 train_loss:4.3271 train_time:43250ms step_avg:149.14ms
step:301/1000 train_loss:4.4836 train_time:43397ms step_avg:149.13ms
step:302/1000 train_loss:4.4762 train_time:43547ms step_avg:149.13ms
step:303/1000 train_loss:4.4032 train_time:43694ms step_avg:149.13ms
step:304/1000 train_loss:4.4678 train_time:43840ms step_avg:149.12ms
step:305/1000 train_loss:4.4446 train_time:43989ms step_avg:149.12ms
step:306/1000 train_loss:4.9145 train_time:44137ms step_avg:149.11ms
step:307/1000 train_loss:4.4115 train_time:44284ms step_avg:149.11ms
step:308/1000 train_loss:4.3107 train_time:44433ms step_avg:149.11ms
step:309/1000 train_loss:4.4878 train_time:44581ms step_avg:149.10ms
step:310/1000 train_loss:4.3015 train_time:44730ms step_avg:149.10ms
step:311/1000 train_loss:4.5345 train_time:44877ms step_avg:149.09ms
step:312/1000 train_loss:4.4060 train_time:45025ms step_avg:149.09ms
step:313/1000 train_loss:4.3520 train_time:45174ms step_avg:149.09ms
step:314/1000 train_loss:4.4275 train_time:45321ms step_avg:149.08ms
step:315/1000 train_loss:4.5783 train_time:45471ms step_avg:149.09ms
step:316/1000 train_loss:4.4269 train_time:45618ms step_avg:149.08ms
step:317/1000 train_loss:4.2914 train_time:45767ms step_avg:149.08ms
step:318/1000 train_loss:4.3346 train_time:45914ms step_avg:149.07ms
step:319/1000 train_loss:4.3675 train_time:46062ms step_avg:149.07ms
step:320/1000 train_loss:4.3276 train_time:46211ms step_avg:149.07ms
step:321/1000 train_loss:4.4202 train_time:46358ms step_avg:149.06ms
step:322/1000 train_loss:4.4080 train_time:46507ms step_avg:149.06ms
step:323/1000 train_loss:4.3595 train_time:46655ms step_avg:149.06ms
step:324/1000 train_loss:4.4451 train_time:46804ms step_avg:149.06ms
step:325/1000 train_loss:4.4055 train_time:46952ms step_avg:149.05ms
step:326/1000 train_loss:4.5032 train_time:47100ms step_avg:149.05ms
step:327/1000 train_loss:4.3478 train_time:47249ms step_avg:149.05ms
step:328/1000 train_loss:4.8417 train_time:47396ms step_avg:149.05ms
step:329/1000 train_loss:4.5070 train_time:47546ms step_avg:149.05ms
step:330/1000 train_loss:4.2802 train_time:47694ms step_avg:149.04ms
step:331/1000 train_loss:4.2503 train_time:47843ms step_avg:149.05ms
step:332/1000 train_loss:4.4116 train_time:47992ms step_avg:149.04ms
step:333/1000 train_loss:4.3484 train_time:48140ms step_avg:149.04ms
step:334/1000 train_loss:4.3288 train_time:48288ms step_avg:149.04ms
step:335/1000 train_loss:4.2853 train_time:48436ms step_avg:149.03ms
step:336/1000 train_loss:4.4583 train_time:48585ms step_avg:149.03ms
step:337/1000 train_loss:4.4048 train_time:48733ms step_avg:149.03ms
step:338/1000 train_loss:4.8999 train_time:48880ms step_avg:149.03ms
step:339/1000 train_loss:4.3861 train_time:49031ms step_avg:149.03ms
step:340/1000 train_loss:4.3320 train_time:49178ms step_avg:149.02ms
step:341/1000 train_loss:4.3483 train_time:49327ms step_avg:149.02ms
step:342/1000 train_loss:4.2809 train_time:49475ms step_avg:149.02ms
step:343/1000 train_loss:4.2491 train_time:49623ms step_avg:149.02ms
step:344/1000 train_loss:4.3066 train_time:49773ms step_avg:149.02ms
step:345/1000 train_loss:4.4174 train_time:49921ms step_avg:149.02ms
step:346/1000 train_loss:4.3002 train_time:50070ms step_avg:149.02ms
step:347/1000 train_loss:4.2156 train_time:50217ms step_avg:149.01ms
step:348/1000 train_loss:4.2659 train_time:50365ms step_avg:149.01ms
step:349/1000 train_loss:4.3003 train_time:50512ms step_avg:149.00ms
step:350/1000 train_loss:4.2459 train_time:50662ms step_avg:149.01ms
step:351/1000 train_loss:3.9529 train_time:50810ms step_avg:149.00ms
step:352/1000 train_loss:4.2205 train_time:50958ms step_avg:149.00ms
step:353/1000 train_loss:4.5939 train_time:51107ms step_avg:149.00ms
step:354/1000 train_loss:4.0935 train_time:51255ms step_avg:149.00ms
step:355/1000 train_loss:4.3414 train_time:51404ms step_avg:149.00ms
step:356/1000 train_loss:4.2275 train_time:51552ms step_avg:148.99ms
step:357/1000 train_loss:4.3359 train_time:51700ms step_avg:148.99ms
step:358/1000 train_loss:4.2932 train_time:51849ms step_avg:148.99ms
step:359/1000 train_loss:4.2680 train_time:51997ms step_avg:148.99ms
step:360/1000 train_loss:4.3929 train_time:52146ms step_avg:148.99ms
step:361/1000 train_loss:3.9265 train_time:52294ms step_avg:148.98ms
step:362/1000 train_loss:4.4503 train_time:52443ms step_avg:148.99ms
step:363/1000 train_loss:4.3537 train_time:52590ms step_avg:148.98ms
step:364/1000 train_loss:4.2635 train_time:52738ms step_avg:148.98ms
step:365/1000 train_loss:4.1776 train_time:52886ms step_avg:148.97ms
step:366/1000 train_loss:4.3307 train_time:53034ms step_avg:148.97ms
step:367/1000 train_loss:4.2857 train_time:53182ms step_avg:148.97ms
step:368/1000 train_loss:4.2700 train_time:53331ms step_avg:148.97ms
step:369/1000 train_loss:4.2611 train_time:53479ms step_avg:148.97ms
step:370/1000 train_loss:4.1602 train_time:53628ms step_avg:148.97ms
step:371/1000 train_loss:4.3083 train_time:53775ms step_avg:148.96ms
step:372/1000 train_loss:4.2003 train_time:53925ms step_avg:148.96ms
step:373/1000 train_loss:4.1131 train_time:54073ms step_avg:148.96ms
step:374/1000 train_loss:4.3194 train_time:54221ms step_avg:148.96ms
step:375/1000 train_loss:4.2444 train_time:54372ms step_avg:148.96ms
step:375/1000 val_loss:4.2505 train_time:54395ms step_avg:149.03ms
step:376/1000 train_loss:4.2243 train_time:54533ms step_avg:149.00ms
step:377/1000 train_loss:4.2831 train_time:54682ms step_avg:149.00ms
step:378/1000 train_loss:4.1966 train_time:54985ms step_avg:149.41ms
step:379/1000 train_loss:4.2668 train_time:55140ms step_avg:149.43ms
step:380/1000 train_loss:4.3088 train_time:55457ms step_avg:149.88ms
step:381/1000 train_loss:4.3539 train_time:55605ms step_avg:149.88ms
step:382/1000 train_loss:4.2733 train_time:55751ms step_avg:149.87ms
step:383/1000 train_loss:4.2427 train_time:55897ms step_avg:149.86ms
step:384/1000 train_loss:4.1897 train_time:56042ms step_avg:149.85ms
step:385/1000 train_loss:4.2804 train_time:56189ms step_avg:149.84ms
step:386/1000 train_loss:4.1957 train_time:56346ms step_avg:149.86ms
step:387/1000 train_loss:4.3107 train_time:56496ms step_avg:149.86ms
step:388/1000 train_loss:4.5043 train_time:56643ms step_avg:149.85ms
step:389/1000 train_loss:4.2049 train_time:56790ms step_avg:149.84ms
step:390/1000 train_loss:4.1878 train_time:56937ms step_avg:149.83ms
step:391/1000 train_loss:4.2978 train_time:57084ms step_avg:149.83ms
step:392/1000 train_loss:4.2134 train_time:57233ms step_avg:149.82ms
step:393/1000 train_loss:4.3270 train_time:57383ms step_avg:149.82ms
step:394/1000 train_loss:4.1583 train_time:57534ms step_avg:149.83ms
step:395/1000 train_loss:4.2919 train_time:57681ms step_avg:149.82ms
step:396/1000 train_loss:4.0427 train_time:57828ms step_avg:149.81ms
step:397/1000 train_loss:4.2386 train_time:57976ms step_avg:149.81ms
step:398/1000 train_loss:4.3046 train_time:58123ms step_avg:149.80ms
step:399/1000 train_loss:4.2794 train_time:58272ms step_avg:149.80ms
step:400/1000 train_loss:4.1880 train_time:58422ms step_avg:149.80ms
step:401/1000 train_loss:4.2409 train_time:58570ms step_avg:149.80ms
step:402/1000 train_loss:4.2957 train_time:58719ms step_avg:149.79ms
step:403/1000 train_loss:4.2503 train_time:58868ms step_avg:149.79ms
step:404/1000 train_loss:4.3437 train_time:59016ms step_avg:149.79ms
step:405/1000 train_loss:4.1211 train_time:59163ms step_avg:149.78ms
step:406/1000 train_loss:4.1878 train_time:59311ms step_avg:149.78ms
step:407/1000 train_loss:4.4695 train_time:59460ms step_avg:149.77ms
step:408/1000 train_loss:4.2086 train_time:59608ms step_avg:149.77ms
step:409/1000 train_loss:4.2192 train_time:59756ms step_avg:149.76ms
step:410/1000 train_loss:4.2621 train_time:59903ms step_avg:149.76ms
step:411/1000 train_loss:4.1423 train_time:60051ms step_avg:149.75ms
step:412/1000 train_loss:4.1727 train_time:60200ms step_avg:149.75ms
step:413/1000 train_loss:4.5864 train_time:60346ms step_avg:149.74ms
step:414/1000 train_loss:4.0417 train_time:60496ms step_avg:149.74ms
step:415/1000 train_loss:4.4027 train_time:60643ms step_avg:149.74ms
step:416/1000 train_loss:4.1540 train_time:60792ms step_avg:149.73ms
step:417/1000 train_loss:4.1603 train_time:60940ms step_avg:149.73ms
step:418/1000 train_loss:4.3553 train_time:61088ms step_avg:149.73ms
step:419/1000 train_loss:4.0849 train_time:61237ms step_avg:149.72ms
step:420/1000 train_loss:4.1986 train_time:61385ms step_avg:149.72ms
step:421/1000 train_loss:4.1411 train_time:61534ms step_avg:149.72ms
step:422/1000 train_loss:4.0363 train_time:61681ms step_avg:149.71ms
step:423/1000 train_loss:4.1726 train_time:61831ms step_avg:149.71ms
step:424/1000 train_loss:4.2635 train_time:61978ms step_avg:149.70ms
step:425/1000 train_loss:4.0315 train_time:62126ms step_avg:149.70ms
step:426/1000 train_loss:4.2221 train_time:62275ms step_avg:149.70ms
step:427/1000 train_loss:4.0947 train_time:62423ms step_avg:149.70ms
step:428/1000 train_loss:4.2912 train_time:62571ms step_avg:149.69ms
step:429/1000 train_loss:4.2207 train_time:62720ms step_avg:149.69ms
step:430/1000 train_loss:4.1499 train_time:62865ms step_avg:149.68ms
step:431/1000 train_loss:4.1250 train_time:63014ms step_avg:149.68ms
step:432/1000 train_loss:4.0467 train_time:63161ms step_avg:149.67ms
step:433/1000 train_loss:4.1593 train_time:63311ms step_avg:149.67ms
step:434/1000 train_loss:4.2254 train_time:63459ms step_avg:149.67ms
step:435/1000 train_loss:4.1560 train_time:63608ms step_avg:149.66ms
step:436/1000 train_loss:4.2033 train_time:63756ms step_avg:149.66ms
step:437/1000 train_loss:4.2139 train_time:63904ms step_avg:149.66ms
step:438/1000 train_loss:4.0952 train_time:64052ms step_avg:149.65ms
step:439/1000 train_loss:4.1176 train_time:64201ms step_avg:149.65ms
step:440/1000 train_loss:4.1005 train_time:64348ms step_avg:149.65ms
step:441/1000 train_loss:4.2790 train_time:64498ms step_avg:149.65ms
step:442/1000 train_loss:4.1687 train_time:64645ms step_avg:149.64ms
step:443/1000 train_loss:4.1521 train_time:64793ms step_avg:149.64ms
step:444/1000 train_loss:4.0371 train_time:64941ms step_avg:149.63ms
step:445/1000 train_loss:4.3021 train_time:65089ms step_avg:149.63ms
step:446/1000 train_loss:4.2294 train_time:65238ms step_avg:149.63ms
step:447/1000 train_loss:4.2180 train_time:65384ms step_avg:149.62ms
step:448/1000 train_loss:4.1327 train_time:65533ms step_avg:149.62ms
step:449/1000 train_loss:4.2290 train_time:65681ms step_avg:149.62ms
step:450/1000 train_loss:4.0614 train_time:65829ms step_avg:149.61ms
step:451/1000 train_loss:4.1140 train_time:65978ms step_avg:149.61ms
step:452/1000 train_loss:3.9757 train_time:66126ms step_avg:149.61ms
step:453/1000 train_loss:4.0843 train_time:66274ms step_avg:149.60ms
step:454/1000 train_loss:4.0602 train_time:66423ms step_avg:149.60ms
step:455/1000 train_loss:4.0332 train_time:66570ms step_avg:149.60ms
step:456/1000 train_loss:4.2432 train_time:66718ms step_avg:149.59ms
step:457/1000 train_loss:4.1080 train_time:66866ms step_avg:149.59ms
step:458/1000 train_loss:4.1818 train_time:67015ms step_avg:149.59ms
step:459/1000 train_loss:4.2215 train_time:67163ms step_avg:149.58ms
step:460/1000 train_loss:4.0200 train_time:67311ms step_avg:149.58ms
step:461/1000 train_loss:4.1933 train_time:67459ms step_avg:149.58ms
step:462/1000 train_loss:4.0895 train_time:67608ms step_avg:149.58ms
step:463/1000 train_loss:4.0951 train_time:67756ms step_avg:149.57ms
step:464/1000 train_loss:4.1672 train_time:67903ms step_avg:149.57ms
step:465/1000 train_loss:4.1073 train_time:68051ms step_avg:149.56ms
step:466/1000 train_loss:4.1046 train_time:68199ms step_avg:149.56ms
step:467/1000 train_loss:4.2141 train_time:68346ms step_avg:149.55ms
step:468/1000 train_loss:4.2119 train_time:68494ms step_avg:149.55ms
step:469/1000 train_loss:4.1933 train_time:68642ms step_avg:149.55ms
step:470/1000 train_loss:4.0903 train_time:68790ms step_avg:149.54ms
step:471/1000 train_loss:4.1646 train_time:68939ms step_avg:149.54ms
step:472/1000 train_loss:4.2170 train_time:69087ms step_avg:149.54ms
step:473/1000 train_loss:4.1512 train_time:69234ms step_avg:149.53ms
step:474/1000 train_loss:4.1041 train_time:69382ms step_avg:149.53ms
step:475/1000 train_loss:3.9701 train_time:69531ms step_avg:149.53ms
step:476/1000 train_loss:4.4101 train_time:69678ms step_avg:149.52ms
step:477/1000 train_loss:4.1615 train_time:69827ms step_avg:149.52ms
step:478/1000 train_loss:3.9699 train_time:69975ms step_avg:149.52ms
step:479/1000 train_loss:4.1827 train_time:70122ms step_avg:149.51ms
step:480/1000 train_loss:4.1547 train_time:70271ms step_avg:149.51ms
step:481/1000 train_loss:4.2864 train_time:70419ms step_avg:149.51ms
step:482/1000 train_loss:4.1115 train_time:70565ms step_avg:149.50ms
step:483/1000 train_loss:3.9148 train_time:70715ms step_avg:149.50ms
step:484/1000 train_loss:4.1890 train_time:70864ms step_avg:149.50ms
step:485/1000 train_loss:4.0509 train_time:71013ms step_avg:149.50ms
step:486/1000 train_loss:4.0607 train_time:71160ms step_avg:149.50ms
step:487/1000 train_loss:3.9914 train_time:71308ms step_avg:149.49ms
step:488/1000 train_loss:4.0478 train_time:71457ms step_avg:149.49ms
step:489/1000 train_loss:4.2457 train_time:71604ms step_avg:149.49ms
step:490/1000 train_loss:4.0998 train_time:71751ms step_avg:149.48ms
step:491/1000 train_loss:3.9919 train_time:71899ms step_avg:149.48ms
step:492/1000 train_loss:4.0091 train_time:72047ms step_avg:149.48ms
step:493/1000 train_loss:4.1198 train_time:72197ms step_avg:149.48ms
step:494/1000 train_loss:3.9650 train_time:72344ms step_avg:149.47ms
step:495/1000 train_loss:4.1012 train_time:72492ms step_avg:149.47ms
step:496/1000 train_loss:4.0334 train_time:72639ms step_avg:149.46ms
step:497/1000 train_loss:3.9341 train_time:72788ms step_avg:149.46ms
step:498/1000 train_loss:4.1147 train_time:72937ms step_avg:149.46ms
step:499/1000 train_loss:4.1911 train_time:73084ms step_avg:149.46ms
step:500/1000 train_loss:4.2315 train_time:73234ms step_avg:149.46ms
step:500/1000 val_loss:4.0942 train_time:73257ms step_avg:149.50ms
step:501/1000 train_loss:4.1341 train_time:73390ms step_avg:149.47ms
step:502/1000 train_loss:4.1722 train_time:73539ms step_avg:149.47ms
step:503/1000 train_loss:4.1274 train_time:73686ms step_avg:149.46ms
step:504/1000 train_loss:4.1547 train_time:73832ms step_avg:149.46ms
step:505/1000 train_loss:4.1172 train_time:73977ms step_avg:149.45ms
step:506/1000 train_loss:4.1997 train_time:74124ms step_avg:149.44ms
step:507/1000 train_loss:4.0086 train_time:74276ms step_avg:149.45ms
step:508/1000 train_loss:4.1398 train_time:74427ms step_avg:149.45ms
step:509/1000 train_loss:4.2188 train_time:74576ms step_avg:149.45ms
step:510/1000 train_loss:4.1602 train_time:74724ms step_avg:149.45ms
step:511/1000 train_loss:3.9705 train_time:74871ms step_avg:149.44ms
step:512/1000 train_loss:4.1651 train_time:75018ms step_avg:149.44ms
step:513/1000 train_loss:4.1034 train_time:75165ms step_avg:149.43ms
step:514/1000 train_loss:4.0704 train_time:75315ms step_avg:149.44ms
step:515/1000 train_loss:4.1205 train_time:75466ms step_avg:149.44ms
step:516/1000 train_loss:4.1258 train_time:75614ms step_avg:149.43ms
step:517/1000 train_loss:4.4616 train_time:75762ms step_avg:149.43ms
step:518/1000 train_loss:4.0538 train_time:75910ms step_avg:149.43ms
step:519/1000 train_loss:4.1683 train_time:76057ms step_avg:149.42ms
step:520/1000 train_loss:4.0919 train_time:76206ms step_avg:149.42ms
step:521/1000 train_loss:4.0699 train_time:76354ms step_avg:149.42ms
step:522/1000 train_loss:4.0137 train_time:76503ms step_avg:149.42ms
step:523/1000 train_loss:4.0309 train_time:76651ms step_avg:149.42ms
step:524/1000 train_loss:4.6524 train_time:76798ms step_avg:149.41ms
step:525/1000 train_loss:4.1225 train_time:76947ms step_avg:149.41ms
step:526/1000 train_loss:4.0709 train_time:77094ms step_avg:149.41ms
step:527/1000 train_loss:4.0709 train_time:77243ms step_avg:149.41ms
step:528/1000 train_loss:4.0259 train_time:77392ms step_avg:149.41ms
step:529/1000 train_loss:3.9984 train_time:77540ms step_avg:149.40ms
step:530/1000 train_loss:4.2103 train_time:77689ms step_avg:149.40ms
step:531/1000 train_loss:4.0243 train_time:77836ms step_avg:149.40ms
step:532/1000 train_loss:4.2902 train_time:77985ms step_avg:149.40ms
step:533/1000 train_loss:4.1038 train_time:78132ms step_avg:149.39ms
step:534/1000 train_loss:4.0302 train_time:78282ms step_avg:149.39ms
step:535/1000 train_loss:4.0637 train_time:78432ms step_avg:149.39ms
step:536/1000 train_loss:3.9881 train_time:78580ms step_avg:149.39ms
step:537/1000 train_loss:4.1013 train_time:78729ms step_avg:149.39ms
step:538/1000 train_loss:4.1064 train_time:78877ms step_avg:149.39ms
step:539/1000 train_loss:4.0117 train_time:79026ms step_avg:149.39ms
step:540/1000 train_loss:4.4864 train_time:79174ms step_avg:149.38ms
step:541/1000 train_loss:4.0359 train_time:79323ms step_avg:149.38ms
step:542/1000 train_loss:4.1461 train_time:79471ms step_avg:149.38ms
step:543/1000 train_loss:3.9867 train_time:79619ms step_avg:149.38ms
step:544/1000 train_loss:3.9629 train_time:79769ms step_avg:149.38ms
step:545/1000 train_loss:4.0460 train_time:79916ms step_avg:149.38ms
step:546/1000 train_loss:3.9698 train_time:80065ms step_avg:149.37ms
step:547/1000 train_loss:4.0196 train_time:80212ms step_avg:149.37ms
step:548/1000 train_loss:4.0279 train_time:80361ms step_avg:149.37ms
step:549/1000 train_loss:4.0046 train_time:80512ms step_avg:149.37ms
step:550/1000 train_loss:4.0968 train_time:80660ms step_avg:149.37ms
step:551/1000 train_loss:3.9680 train_time:80808ms step_avg:149.37ms
step:552/1000 train_loss:3.9918 train_time:80956ms step_avg:149.37ms
step:553/1000 train_loss:4.3267 train_time:81106ms step_avg:149.37ms
step:554/1000 train_loss:4.1082 train_time:81253ms step_avg:149.36ms
step:555/1000 train_loss:4.0798 train_time:81404ms step_avg:149.36ms
step:556/1000 train_loss:4.0504 train_time:81551ms step_avg:149.36ms
step:557/1000 train_loss:4.0478 train_time:81700ms step_avg:149.36ms
step:558/1000 train_loss:3.7267 train_time:81848ms step_avg:149.36ms
step:559/1000 train_loss:3.9741 train_time:81996ms step_avg:149.36ms
step:560/1000 train_loss:4.0168 train_time:82144ms step_avg:149.35ms
step:561/1000 train_loss:4.0591 train_time:82291ms step_avg:149.35ms
step:562/1000 train_loss:3.9679 train_time:82440ms step_avg:149.35ms
step:563/1000 train_loss:3.9152 train_time:82590ms step_avg:149.35ms
step:564/1000 train_loss:4.1131 train_time:82738ms step_avg:149.35ms
step:565/1000 train_loss:3.9261 train_time:82886ms step_avg:149.35ms
step:566/1000 train_loss:4.0474 train_time:83034ms step_avg:149.34ms
step:567/1000 train_loss:4.0069 train_time:83334ms step_avg:149.61ms
step:568/1000 train_loss:3.9487 train_time:83489ms step_avg:149.62ms
step:569/1000 train_loss:4.0470 train_time:83635ms step_avg:149.61ms
step:570/1000 train_loss:4.0121 train_time:83952ms step_avg:149.91ms
step:571/1000 train_loss:4.0367 train_time:84099ms step_avg:149.91ms
step:572/1000 train_loss:4.1344 train_time:84245ms step_avg:149.90ms
step:573/1000 train_loss:4.0568 train_time:84391ms step_avg:149.90ms
step:574/1000 train_loss:4.0701 train_time:84538ms step_avg:149.89ms
step:575/1000 train_loss:4.1363 train_time:84685ms step_avg:149.88ms
step:576/1000 train_loss:4.0919 train_time:84837ms step_avg:149.89ms
step:577/1000 train_loss:4.0965 train_time:84987ms step_avg:149.89ms
step:578/1000 train_loss:4.0433 train_time:85135ms step_avg:149.89ms
step:579/1000 train_loss:4.0138 train_time:85284ms step_avg:149.88ms
step:580/1000 train_loss:4.0057 train_time:85430ms step_avg:149.88ms
step:581/1000 train_loss:3.9562 train_time:85577ms step_avg:149.87ms
step:582/1000 train_loss:3.9956 train_time:85726ms step_avg:149.87ms
step:583/1000 train_loss:4.2120 train_time:85874ms step_avg:149.87ms
step:584/1000 train_loss:3.9862 train_time:86023ms step_avg:149.87ms
step:585/1000 train_loss:3.9423 train_time:86172ms step_avg:149.86ms
step:586/1000 train_loss:4.1217 train_time:86319ms step_avg:149.86ms
step:587/1000 train_loss:3.8843 train_time:86467ms step_avg:149.86ms
step:588/1000 train_loss:4.0138 train_time:86613ms step_avg:149.85ms
step:589/1000 train_loss:4.0115 train_time:86761ms step_avg:149.85ms
step:590/1000 train_loss:4.3470 train_time:86911ms step_avg:149.85ms
step:591/1000 train_loss:4.1365 train_time:87058ms step_avg:149.84ms
step:592/1000 train_loss:3.8746 train_time:87207ms step_avg:149.84ms
step:593/1000 train_loss:3.8814 train_time:87355ms step_avg:149.84ms
step:594/1000 train_loss:3.8887 train_time:87503ms step_avg:149.83ms
step:595/1000 train_loss:3.9175 train_time:87650ms step_avg:149.83ms
step:596/1000 train_loss:4.2724 train_time:87798ms step_avg:149.83ms
step:597/1000 train_loss:4.0042 train_time:87947ms step_avg:149.82ms
step:598/1000 train_loss:3.9402 train_time:88094ms step_avg:149.82ms
step:599/1000 train_loss:3.9954 train_time:88243ms step_avg:149.82ms
step:600/1000 train_loss:3.8215 train_time:88391ms step_avg:149.82ms
step:601/1000 train_loss:3.9475 train_time:88539ms step_avg:149.81ms
step:602/1000 train_loss:3.9773 train_time:88688ms step_avg:149.81ms
step:603/1000 train_loss:3.9936 train_time:88834ms step_avg:149.80ms
step:604/1000 train_loss:4.1208 train_time:88983ms step_avg:149.80ms
step:605/1000 train_loss:3.9890 train_time:89131ms step_avg:149.80ms
step:606/1000 train_loss:3.9699 train_time:89278ms step_avg:149.79ms
step:607/1000 train_loss:3.9071 train_time:89426ms step_avg:149.79ms
step:608/1000 train_loss:4.1427 train_time:89573ms step_avg:149.79ms
step:609/1000 train_loss:3.9813 train_time:89721ms step_avg:149.78ms
step:610/1000 train_loss:3.9540 train_time:89870ms step_avg:149.78ms
step:611/1000 train_loss:4.0631 train_time:90016ms step_avg:149.78ms
step:612/1000 train_loss:3.9687 train_time:90167ms step_avg:149.78ms
step:613/1000 train_loss:3.9378 train_time:90314ms step_avg:149.77ms
step:614/1000 train_loss:4.1010 train_time:90462ms step_avg:149.77ms
step:615/1000 train_loss:4.0709 train_time:90610ms step_avg:149.77ms
step:616/1000 train_loss:4.0372 train_time:90757ms step_avg:149.76ms
step:617/1000 train_loss:3.9534 train_time:90906ms step_avg:149.76ms
step:618/1000 train_loss:3.9084 train_time:91053ms step_avg:149.76ms
step:619/1000 train_loss:4.0057 train_time:91202ms step_avg:149.76ms
step:620/1000 train_loss:3.9183 train_time:91349ms step_avg:149.75ms
step:621/1000 train_loss:3.9270 train_time:91497ms step_avg:149.75ms
step:622/1000 train_loss:4.2245 train_time:91645ms step_avg:149.75ms
step:623/1000 train_loss:3.9380 train_time:91793ms step_avg:149.74ms
step:624/1000 train_loss:3.9675 train_time:91941ms step_avg:149.74ms
step:625/1000 train_loss:4.0420 train_time:92090ms step_avg:149.74ms
step:625/1000 val_loss:3.9670 train_time:92113ms step_avg:149.78ms
step:626/1000 train_loss:4.0615 train_time:92248ms step_avg:149.75ms
step:627/1000 train_loss:4.0814 train_time:92397ms step_avg:149.75ms
step:628/1000 train_loss:4.0710 train_time:92545ms step_avg:149.75ms
step:629/1000 train_loss:4.1037 train_time:92690ms step_avg:149.74ms
step:630/1000 train_loss:3.9316 train_time:92839ms step_avg:149.74ms
step:631/1000 train_loss:4.0494 train_time:92986ms step_avg:149.74ms
step:632/1000 train_loss:4.0893 train_time:93135ms step_avg:149.73ms
step:633/1000 train_loss:3.9936 train_time:93287ms step_avg:149.74ms
step:634/1000 train_loss:3.9160 train_time:93436ms step_avg:149.74ms
step:635/1000 train_loss:4.0167 train_time:93585ms step_avg:149.74ms
step:636/1000 train_loss:4.2702 train_time:93731ms step_avg:149.73ms
step:637/1000 train_loss:3.8669 train_time:93878ms step_avg:149.73ms
step:638/1000 train_loss:3.6859 train_time:94024ms step_avg:149.72ms
step:639/1000 train_loss:3.9108 train_time:94174ms step_avg:149.72ms
step:640/1000 train_loss:3.9433 train_time:94325ms step_avg:149.72ms
step:641/1000 train_loss:3.9168 train_time:94474ms step_avg:149.72ms
step:642/1000 train_loss:3.9092 train_time:94623ms step_avg:149.72ms
step:643/1000 train_loss:3.9477 train_time:94770ms step_avg:149.72ms
step:644/1000 train_loss:3.9719 train_time:94918ms step_avg:149.71ms
step:645/1000 train_loss:3.8871 train_time:95066ms step_avg:149.71ms
step:646/1000 train_loss:4.1084 train_time:95214ms step_avg:149.71ms
step:647/1000 train_loss:4.0112 train_time:95364ms step_avg:149.71ms
step:648/1000 train_loss:3.9946 train_time:95511ms step_avg:149.70ms
step:649/1000 train_loss:4.0162 train_time:95661ms step_avg:149.70ms
step:650/1000 train_loss:4.0839 train_time:95808ms step_avg:149.70ms
step:651/1000 train_loss:3.9515 train_time:95956ms step_avg:149.70ms
step:652/1000 train_loss:4.0807 train_time:96105ms step_avg:149.70ms
step:653/1000 train_loss:3.9127 train_time:96252ms step_avg:149.69ms
step:654/1000 train_loss:3.9921 train_time:96402ms step_avg:149.69ms
step:655/1000 train_loss:3.7640 train_time:96550ms step_avg:149.69ms
step:656/1000 train_loss:3.8993 train_time:96698ms step_avg:149.69ms
step:657/1000 train_loss:3.9132 train_time:96847ms step_avg:149.69ms
step:658/1000 train_loss:3.8415 train_time:96994ms step_avg:149.68ms
step:659/1000 train_loss:4.0184 train_time:97143ms step_avg:149.68ms
step:660/1000 train_loss:3.9233 train_time:97290ms step_avg:149.68ms
step:661/1000 train_loss:3.9911 train_time:97440ms step_avg:149.68ms
step:662/1000 train_loss:4.0791 train_time:97590ms step_avg:149.68ms
step:663/1000 train_loss:3.9898 train_time:97739ms step_avg:149.68ms
step:664/1000 train_loss:3.8686 train_time:97887ms step_avg:149.67ms
step:665/1000 train_loss:3.9498 train_time:98035ms step_avg:149.67ms
step:666/1000 train_loss:3.8160 train_time:98183ms step_avg:149.67ms
step:667/1000 train_loss:4.1078 train_time:98331ms step_avg:149.67ms
step:668/1000 train_loss:3.9499 train_time:98480ms step_avg:149.66ms
step:669/1000 train_loss:3.9472 train_time:98628ms step_avg:149.66ms
step:670/1000 train_loss:3.8131 train_time:98776ms step_avg:149.66ms
step:671/1000 train_loss:3.9171 train_time:98925ms step_avg:149.66ms
step:672/1000 train_loss:3.8713 train_time:99074ms step_avg:149.66ms
step:673/1000 train_loss:3.8982 train_time:99221ms step_avg:149.66ms
step:674/1000 train_loss:4.1703 train_time:99370ms step_avg:149.65ms
step:675/1000 train_loss:3.9691 train_time:99518ms step_avg:149.65ms
step:676/1000 train_loss:4.0440 train_time:99667ms step_avg:149.65ms
step:677/1000 train_loss:3.8036 train_time:99816ms step_avg:149.65ms
step:678/1000 train_loss:3.9161 train_time:99965ms step_avg:149.65ms
step:679/1000 train_loss:3.8618 train_time:100114ms step_avg:149.65ms
step:680/1000 train_loss:3.9950 train_time:100263ms step_avg:149.65ms
step:681/1000 train_loss:3.9050 train_time:100410ms step_avg:149.64ms
step:682/1000 train_loss:3.9278 train_time:100559ms step_avg:149.64ms
step:683/1000 train_loss:4.0016 train_time:100707ms step_avg:149.64ms
step:684/1000 train_loss:4.0559 train_time:100857ms step_avg:149.64ms
step:685/1000 train_loss:3.9478 train_time:101006ms step_avg:149.64ms
step:686/1000 train_loss:4.0271 train_time:101155ms step_avg:149.64ms
step:687/1000 train_loss:3.9496 train_time:101303ms step_avg:149.63ms
step:688/1000 train_loss:3.9937 train_time:101451ms step_avg:149.63ms
step:689/1000 train_loss:3.6486 train_time:101601ms step_avg:149.63ms
step:690/1000 train_loss:3.7422 train_time:101748ms step_avg:149.63ms
step:691/1000 train_loss:3.8700 train_time:101895ms step_avg:149.63ms
step:692/1000 train_loss:3.7484 train_time:102045ms step_avg:149.63ms
step:693/1000 train_loss:3.9682 train_time:102193ms step_avg:149.62ms
step:694/1000 train_loss:3.9853 train_time:102342ms step_avg:149.62ms
step:695/1000 train_loss:3.8748 train_time:102490ms step_avg:149.62ms
step:696/1000 train_loss:3.8538 train_time:102639ms step_avg:149.62ms
step:697/1000 train_loss:4.1685 train_time:102787ms step_avg:149.62ms
step:698/1000 train_loss:3.9235 train_time:102935ms step_avg:149.62ms
step:699/1000 train_loss:3.9528 train_time:103085ms step_avg:149.62ms
step:700/1000 train_loss:4.1247 train_time:103233ms step_avg:149.61ms
step:701/1000 train_loss:3.8956 train_time:103382ms step_avg:149.61ms
step:702/1000 train_loss:3.8425 train_time:103528ms step_avg:149.61ms
step:703/1000 train_loss:3.8410 train_time:103676ms step_avg:149.61ms
step:704/1000 train_loss:3.7890 train_time:103825ms step_avg:149.60ms
step:705/1000 train_loss:3.8752 train_time:103972ms step_avg:149.60ms
step:706/1000 train_loss:3.8657 train_time:104122ms step_avg:149.60ms
step:707/1000 train_loss:3.8910 train_time:104270ms step_avg:149.60ms
step:708/1000 train_loss:3.9606 train_time:104418ms step_avg:149.60ms
step:709/1000 train_loss:3.8955 train_time:104566ms step_avg:149.59ms
step:710/1000 train_loss:3.8813 train_time:104715ms step_avg:149.59ms
step:711/1000 train_loss:3.8504 train_time:104863ms step_avg:149.59ms
step:712/1000 train_loss:3.8960 train_time:105011ms step_avg:149.59ms
step:713/1000 train_loss:3.9602 train_time:105160ms step_avg:149.59ms
step:714/1000 train_loss:3.9734 train_time:105308ms step_avg:149.59ms
step:715/1000 train_loss:3.8773 train_time:105457ms step_avg:149.59ms
step:716/1000 train_loss:3.8815 train_time:105605ms step_avg:149.58ms
step:717/1000 train_loss:3.8999 train_time:105754ms step_avg:149.58ms
step:718/1000 train_loss:4.0380 train_time:105903ms step_avg:149.58ms
step:719/1000 train_loss:3.9047 train_time:106051ms step_avg:149.58ms
step:720/1000 train_loss:3.9675 train_time:106199ms step_avg:149.58ms
step:721/1000 train_loss:4.1282 train_time:106348ms step_avg:149.58ms
step:722/1000 train_loss:3.7713 train_time:106496ms step_avg:149.57ms
step:723/1000 train_loss:4.0241 train_time:106645ms step_avg:149.57ms
step:724/1000 train_loss:4.0855 train_time:106792ms step_avg:149.57ms
step:725/1000 train_loss:3.8619 train_time:106943ms step_avg:149.57ms
step:726/1000 train_loss:3.9478 train_time:107091ms step_avg:149.57ms
step:727/1000 train_loss:3.8540 train_time:107241ms step_avg:149.57ms
step:728/1000 train_loss:3.8628 train_time:107389ms step_avg:149.57ms
step:729/1000 train_loss:4.0339 train_time:107537ms step_avg:149.56ms
step:730/1000 train_loss:3.9863 train_time:107686ms step_avg:149.56ms
step:731/1000 train_loss:3.9925 train_time:107834ms step_avg:149.56ms
step:732/1000 train_loss:3.8743 train_time:107983ms step_avg:149.56ms
step:733/1000 train_loss:3.8907 train_time:108130ms step_avg:149.56ms
step:734/1000 train_loss:4.1206 train_time:108279ms step_avg:149.56ms
step:735/1000 train_loss:3.8646 train_time:108427ms step_avg:149.55ms
step:736/1000 train_loss:3.9282 train_time:108576ms step_avg:149.55ms
step:737/1000 train_loss:4.0467 train_time:108725ms step_avg:149.55ms
step:738/1000 train_loss:3.9613 train_time:108874ms step_avg:149.55ms
step:739/1000 train_loss:3.9050 train_time:109023ms step_avg:149.55ms
step:740/1000 train_loss:3.8077 train_time:109171ms step_avg:149.55ms
step:741/1000 train_loss:4.4329 train_time:109319ms step_avg:149.55ms
step:742/1000 train_loss:3.8037 train_time:109466ms step_avg:149.54ms
step:743/1000 train_loss:3.8920 train_time:109615ms step_avg:149.54ms
step:744/1000 train_loss:3.8750 train_time:109765ms step_avg:149.54ms
step:745/1000 train_loss:3.9336 train_time:109912ms step_avg:149.54ms
step:746/1000 train_loss:3.9172 train_time:110061ms step_avg:149.54ms
step:747/1000 train_loss:3.8980 train_time:110208ms step_avg:149.54ms
step:748/1000 train_loss:3.9268 train_time:110357ms step_avg:149.53ms
step:749/1000 train_loss:3.8545 train_time:110505ms step_avg:149.53ms
step:750/1000 train_loss:3.8635 train_time:110653ms step_avg:149.53ms
step:750/1000 val_loss:3.8719 train_time:110677ms step_avg:149.56ms
step:751/1000 train_loss:3.9040 train_time:110813ms step_avg:149.55ms
step:752/1000 train_loss:3.8658 train_time:110964ms step_avg:149.55ms
step:753/1000 train_loss:3.8950 train_time:111112ms step_avg:149.55ms
step:754/1000 train_loss:3.9195 train_time:111258ms step_avg:149.54ms
step:755/1000 train_loss:3.8840 train_time:111406ms step_avg:149.54ms
step:756/1000 train_loss:3.9611 train_time:111704ms step_avg:149.74ms
step:757/1000 train_loss:3.8000 train_time:111861ms step_avg:149.75ms
step:758/1000 train_loss:4.0198 train_time:112008ms step_avg:149.74ms
step:759/1000 train_loss:3.9454 train_time:112153ms step_avg:149.74ms
step:760/1000 train_loss:3.8737 train_time:112482ms step_avg:149.98ms
step:761/1000 train_loss:3.9747 train_time:112630ms step_avg:149.97ms
step:762/1000 train_loss:3.6958 train_time:112777ms step_avg:149.97ms
step:763/1000 train_loss:3.8569 train_time:112924ms step_avg:149.96ms
step:764/1000 train_loss:3.9589 train_time:113071ms step_avg:149.96ms
step:765/1000 train_loss:3.6133 train_time:113216ms step_avg:149.96ms
step:766/1000 train_loss:4.0451 train_time:113371ms step_avg:149.96ms
step:767/1000 train_loss:3.8996 train_time:113520ms step_avg:149.96ms
step:768/1000 train_loss:3.8395 train_time:113668ms step_avg:149.96ms
step:769/1000 train_loss:3.8713 train_time:113817ms step_avg:149.96ms
step:770/1000 train_loss:3.8841 train_time:113965ms step_avg:149.95ms
step:771/1000 train_loss:3.9434 train_time:114111ms step_avg:149.95ms
step:772/1000 train_loss:4.1695 train_time:114261ms step_avg:149.95ms
step:773/1000 train_loss:3.7604 train_time:114411ms step_avg:149.95ms
step:774/1000 train_loss:3.9542 train_time:114559ms step_avg:149.95ms
step:775/1000 train_loss:3.9319 train_time:114707ms step_avg:149.94ms
step:776/1000 train_loss:3.8963 train_time:114855ms step_avg:149.94ms
step:777/1000 train_loss:3.6960 train_time:115003ms step_avg:149.94ms
step:778/1000 train_loss:3.7028 train_time:115151ms step_avg:149.94ms
step:779/1000 train_loss:3.7667 train_time:115299ms step_avg:149.93ms
step:780/1000 train_loss:3.8584 train_time:115449ms step_avg:149.93ms
step:781/1000 train_loss:3.8946 train_time:115596ms step_avg:149.93ms
step:782/1000 train_loss:3.9507 train_time:115746ms step_avg:149.93ms
step:783/1000 train_loss:3.8577 train_time:115893ms step_avg:149.93ms
step:784/1000 train_loss:3.8688 train_time:116039ms step_avg:149.92ms
step:785/1000 train_loss:3.8553 train_time:116189ms step_avg:149.92ms
step:786/1000 train_loss:3.8446 train_time:116338ms step_avg:149.92ms
step:787/1000 train_loss:3.7561 train_time:116485ms step_avg:149.92ms
step:788/1000 train_loss:4.0294 train_time:116633ms step_avg:149.91ms
step:789/1000 train_loss:3.8005 train_time:116781ms step_avg:149.91ms
step:790/1000 train_loss:3.8516 train_time:116930ms step_avg:149.91ms
step:791/1000 train_loss:3.9176 train_time:117078ms step_avg:149.91ms
step:792/1000 train_loss:4.0425 train_time:117228ms step_avg:149.91ms
step:793/1000 train_loss:4.0592 train_time:117375ms step_avg:149.90ms
step:794/1000 train_loss:3.7807 train_time:117523ms step_avg:149.90ms
step:795/1000 train_loss:3.8920 train_time:117672ms step_avg:149.90ms
step:796/1000 train_loss:3.9411 train_time:117820ms step_avg:149.90ms
step:797/1000 train_loss:4.0671 train_time:117968ms step_avg:149.90ms
step:798/1000 train_loss:3.8055 train_time:118116ms step_avg:149.89ms
step:799/1000 train_loss:3.9493 train_time:118265ms step_avg:149.89ms
step:800/1000 train_loss:3.8511 train_time:118414ms step_avg:149.89ms
step:801/1000 train_loss:3.8240 train_time:118562ms step_avg:149.89ms
step:802/1000 train_loss:3.9215 train_time:118711ms step_avg:149.89ms
step:803/1000 train_loss:3.7847 train_time:118857ms step_avg:149.88ms
step:804/1000 train_loss:3.8141 train_time:119007ms step_avg:149.88ms
step:805/1000 train_loss:3.9190 train_time:119154ms step_avg:149.88ms
step:806/1000 train_loss:3.8260 train_time:119303ms step_avg:149.88ms
step:807/1000 train_loss:3.8251 train_time:119452ms step_avg:149.88ms
step:808/1000 train_loss:3.9236 train_time:119600ms step_avg:149.87ms
step:809/1000 train_loss:3.8444 train_time:119750ms step_avg:149.87ms
step:810/1000 train_loss:3.7723 train_time:119896ms step_avg:149.87ms
step:811/1000 train_loss:3.8535 train_time:120045ms step_avg:149.87ms
step:812/1000 train_loss:3.8871 train_time:120192ms step_avg:149.87ms
step:813/1000 train_loss:3.8753 train_time:120341ms step_avg:149.86ms
step:814/1000 train_loss:3.9153 train_time:120490ms step_avg:149.86ms
step:815/1000 train_loss:3.8637 train_time:120638ms step_avg:149.86ms
step:816/1000 train_loss:3.8431 train_time:120787ms step_avg:149.86ms
step:817/1000 train_loss:3.9396 train_time:120935ms step_avg:149.86ms
step:818/1000 train_loss:4.0305 train_time:121082ms step_avg:149.85ms
step:819/1000 train_loss:3.8161 train_time:121230ms step_avg:149.85ms
step:820/1000 train_loss:4.0098 train_time:121378ms step_avg:149.85ms
step:821/1000 train_loss:3.7912 train_time:121527ms step_avg:149.85ms
step:822/1000 train_loss:3.8290 train_time:121676ms step_avg:149.85ms
step:823/1000 train_loss:3.9388 train_time:121824ms step_avg:149.84ms
step:824/1000 train_loss:3.8660 train_time:121972ms step_avg:149.84ms
step:825/1000 train_loss:3.7991 train_time:122120ms step_avg:149.84ms
step:826/1000 train_loss:3.8964 train_time:122267ms step_avg:149.84ms
step:827/1000 train_loss:3.7951 train_time:122415ms step_avg:149.83ms
step:828/1000 train_loss:3.9994 train_time:122563ms step_avg:149.83ms
step:829/1000 train_loss:3.8990 train_time:122712ms step_avg:149.83ms
step:830/1000 train_loss:3.9688 train_time:122859ms step_avg:149.83ms
step:831/1000 train_loss:3.8138 train_time:123009ms step_avg:149.83ms
step:832/1000 train_loss:3.8693 train_time:123156ms step_avg:149.83ms
step:833/1000 train_loss:3.7999 train_time:123305ms step_avg:149.82ms
step:834/1000 train_loss:3.9125 train_time:123452ms step_avg:149.82ms
step:835/1000 train_loss:3.7702 train_time:123599ms step_avg:149.82ms
step:836/1000 train_loss:3.7372 train_time:123750ms step_avg:149.82ms
step:837/1000 train_loss:3.9995 train_time:123898ms step_avg:149.82ms
step:838/1000 train_loss:3.7115 train_time:124047ms step_avg:149.82ms
step:839/1000 train_loss:3.8644 train_time:124195ms step_avg:149.81ms
step:840/1000 train_loss:3.7279 train_time:124344ms step_avg:149.81ms
step:841/1000 train_loss:3.7596 train_time:124491ms step_avg:149.81ms
step:842/1000 train_loss:3.8363 train_time:124641ms step_avg:149.81ms
step:843/1000 train_loss:3.8540 train_time:124789ms step_avg:149.81ms
step:844/1000 train_loss:3.8612 train_time:124937ms step_avg:149.80ms
step:845/1000 train_loss:3.7095 train_time:125085ms step_avg:149.80ms
step:846/1000 train_loss:3.9461 train_time:125234ms step_avg:149.80ms
step:847/1000 train_loss:3.8120 train_time:125381ms step_avg:149.80ms
step:848/1000 train_loss:3.7681 train_time:125530ms step_avg:149.80ms
step:849/1000 train_loss:3.9018 train_time:125677ms step_avg:149.79ms
step:850/1000 train_loss:3.7763 train_time:125826ms step_avg:149.79ms
step:851/1000 train_loss:3.7314 train_time:125974ms step_avg:149.79ms
step:852/1000 train_loss:4.0228 train_time:126123ms step_avg:149.79ms
step:853/1000 train_loss:3.7369 train_time:126272ms step_avg:149.79ms
step:854/1000 train_loss:3.8380 train_time:126421ms step_avg:149.79ms
step:855/1000 train_loss:3.9330 train_time:126570ms step_avg:149.79ms
step:856/1000 train_loss:3.8196 train_time:126718ms step_avg:149.78ms
step:857/1000 train_loss:3.8219 train_time:126865ms step_avg:149.78ms
step:858/1000 train_loss:3.8756 train_time:127014ms step_avg:149.78ms
step:859/1000 train_loss:3.7809 train_time:127162ms step_avg:149.78ms
step:860/1000 train_loss:3.8449 train_time:127310ms step_avg:149.78ms
step:861/1000 train_loss:3.8724 train_time:127459ms step_avg:149.78ms
step:862/1000 train_loss:3.9273 train_time:127608ms step_avg:149.77ms
step:863/1000 train_loss:3.8603 train_time:127754ms step_avg:149.77ms
step:864/1000 train_loss:3.8509 train_time:127902ms step_avg:149.77ms
step:865/1000 train_loss:3.6688 train_time:128051ms step_avg:149.77ms
step:866/1000 train_loss:3.8618 train_time:128199ms step_avg:149.77ms
step:867/1000 train_loss:4.1518 train_time:128348ms step_avg:149.76ms
step:868/1000 train_loss:3.7185 train_time:128496ms step_avg:149.76ms
step:869/1000 train_loss:3.9065 train_time:128645ms step_avg:149.76ms
step:870/1000 train_loss:3.8842 train_time:128793ms step_avg:149.76ms
step:871/1000 train_loss:3.7272 train_time:128941ms step_avg:149.76ms
step:872/1000 train_loss:3.6993 train_time:129090ms step_avg:149.76ms
step:873/1000 train_loss:3.9378 train_time:129239ms step_avg:149.76ms
step:874/1000 train_loss:3.7227 train_time:129387ms step_avg:149.75ms
step:875/1000 train_loss:3.4229 train_time:129536ms step_avg:149.75ms
step:875/1000 val_loss:3.8013 train_time:129559ms step_avg:149.78ms
step:876/1000 train_loss:3.9111 train_time:129695ms step_avg:149.76ms
step:877/1000 train_loss:3.7339 train_time:129844ms step_avg:149.76ms
step:878/1000 train_loss:3.8991 train_time:129991ms step_avg:149.76ms
step:879/1000 train_loss:3.7629 train_time:130138ms step_avg:149.76ms
step:880/1000 train_loss:3.9339 train_time:130285ms step_avg:149.75ms
step:881/1000 train_loss:3.6133 train_time:130431ms step_avg:149.75ms
step:882/1000 train_loss:3.7729 train_time:130582ms step_avg:149.75ms
step:883/1000 train_loss:3.9595 train_time:130734ms step_avg:149.75ms
step:884/1000 train_loss:4.1236 train_time:130883ms step_avg:149.75ms
step:885/1000 train_loss:3.8487 train_time:131030ms step_avg:149.75ms
step:886/1000 train_loss:3.7576 train_time:131177ms step_avg:149.75ms
step:887/1000 train_loss:3.8601 train_time:131325ms step_avg:149.74ms
step:888/1000 train_loss:4.3410 train_time:131473ms step_avg:149.74ms
step:889/1000 train_loss:4.1188 train_time:131621ms step_avg:149.74ms
step:890/1000 train_loss:3.8054 train_time:131772ms step_avg:149.74ms
step:891/1000 train_loss:3.8104 train_time:131920ms step_avg:149.74ms
step:892/1000 train_loss:3.6321 train_time:132068ms step_avg:149.74ms
step:893/1000 train_loss:3.9671 train_time:132215ms step_avg:149.73ms
step:894/1000 train_loss:3.7104 train_time:132363ms step_avg:149.73ms
step:895/1000 train_loss:3.9634 train_time:132510ms step_avg:149.73ms
step:896/1000 train_loss:3.9671 train_time:132660ms step_avg:149.73ms
step:897/1000 train_loss:3.7763 train_time:132811ms step_avg:149.73ms
step:898/1000 train_loss:3.8086 train_time:132958ms step_avg:149.73ms
step:899/1000 train_loss:3.8640 train_time:133107ms step_avg:149.73ms
step:900/1000 train_loss:3.7554 train_time:133254ms step_avg:149.72ms
step:901/1000 train_loss:3.6954 train_time:133402ms step_avg:149.72ms
step:902/1000 train_loss:3.9103 train_time:133551ms step_avg:149.72ms
step:903/1000 train_loss:3.9131 train_time:133699ms step_avg:149.72ms
step:904/1000 train_loss:3.8074 train_time:133849ms step_avg:149.72ms
step:905/1000 train_loss:3.7696 train_time:133997ms step_avg:149.72ms
step:906/1000 train_loss:3.7686 train_time:134145ms step_avg:149.72ms
step:907/1000 train_loss:3.9744 train_time:134293ms step_avg:149.71ms
step:908/1000 train_loss:3.7868 train_time:134440ms step_avg:149.71ms
step:909/1000 train_loss:3.8222 train_time:134590ms step_avg:149.71ms
step:910/1000 train_loss:3.7288 train_time:134738ms step_avg:149.71ms
step:911/1000 train_loss:3.8288 train_time:134887ms step_avg:149.71ms
step:912/1000 train_loss:3.8878 train_time:135035ms step_avg:149.71ms
step:913/1000 train_loss:3.8753 train_time:135183ms step_avg:149.70ms
step:914/1000 train_loss:3.7645 train_time:135331ms step_avg:149.70ms
step:915/1000 train_loss:4.0081 train_time:135479ms step_avg:149.70ms
step:916/1000 train_loss:3.8014 train_time:135627ms step_avg:149.70ms
step:917/1000 train_loss:3.8906 train_time:135775ms step_avg:149.70ms
step:918/1000 train_loss:3.8709 train_time:135924ms step_avg:149.70ms
step:919/1000 train_loss:5.0659 train_time:136073ms step_avg:149.69ms
step:920/1000 train_loss:3.7929 train_time:136220ms step_avg:149.69ms
step:921/1000 train_loss:3.8372 train_time:136368ms step_avg:149.69ms
step:922/1000 train_loss:3.8040 train_time:136515ms step_avg:149.69ms
step:923/1000 train_loss:3.8564 train_time:136664ms step_avg:149.69ms
step:924/1000 train_loss:3.8566 train_time:136813ms step_avg:149.69ms
step:925/1000 train_loss:3.9505 train_time:136962ms step_avg:149.69ms
step:926/1000 train_loss:3.9297 train_time:137110ms step_avg:149.68ms
step:927/1000 train_loss:3.8258 train_time:137258ms step_avg:149.68ms
step:928/1000 train_loss:3.8118 train_time:137408ms step_avg:149.68ms
step:929/1000 train_loss:4.0280 train_time:137555ms step_avg:149.68ms
step:930/1000 train_loss:3.8790 train_time:137704ms step_avg:149.68ms
step:931/1000 train_loss:3.6624 train_time:137851ms step_avg:149.68ms
step:932/1000 train_loss:3.7647 train_time:138000ms step_avg:149.67ms
step:933/1000 train_loss:3.9473 train_time:138148ms step_avg:149.67ms
step:934/1000 train_loss:3.6787 train_time:138296ms step_avg:149.67ms
step:935/1000 train_loss:3.8319 train_time:138445ms step_avg:149.67ms
step:936/1000 train_loss:3.7275 train_time:138594ms step_avg:149.67ms
step:937/1000 train_loss:3.7813 train_time:138741ms step_avg:149.67ms
step:938/1000 train_loss:3.8820 train_time:138889ms step_avg:149.66ms
step:939/1000 train_loss:3.8057 train_time:139036ms step_avg:149.66ms
step:940/1000 train_loss:3.9752 train_time:139185ms step_avg:149.66ms
step:941/1000 train_loss:3.7630 train_time:139333ms step_avg:149.66ms
step:942/1000 train_loss:3.8206 train_time:139482ms step_avg:149.66ms
step:943/1000 train_loss:3.6203 train_time:139630ms step_avg:149.66ms
step:944/1000 train_loss:3.9602 train_time:139779ms step_avg:149.66ms
step:945/1000 train_loss:3.6739 train_time:140077ms step_avg:149.81ms
step:946/1000 train_loss:3.6995 train_time:140232ms step_avg:149.82ms
step:947/1000 train_loss:5.2736 train_time:140378ms step_avg:149.82ms
step:948/1000 train_loss:3.8651 train_time:140525ms step_avg:149.81ms
step:949/1000 train_loss:3.7762 train_time:140671ms step_avg:149.81ms
step:950/1000 train_loss:3.6674 train_time:141008ms step_avg:150.01ms
step:951/1000 train_loss:3.7206 train_time:141153ms step_avg:150.00ms
step:952/1000 train_loss:3.6693 train_time:141299ms step_avg:150.00ms
step:953/1000 train_loss:3.7506 train_time:141446ms step_avg:150.00ms
step:954/1000 train_loss:3.8204 train_time:141593ms step_avg:149.99ms
step:955/1000 train_loss:3.7030 train_time:141738ms step_avg:149.99ms
step:956/1000 train_loss:3.7402 train_time:141892ms step_avg:149.99ms
step:957/1000 train_loss:3.7123 train_time:142040ms step_avg:149.99ms
step:958/1000 train_loss:3.7719 train_time:142189ms step_avg:149.99ms
step:959/1000 train_loss:3.7537 train_time:142335ms step_avg:149.98ms
step:960/1000 train_loss:3.7810 train_time:142483ms step_avg:149.98ms
step:961/1000 train_loss:3.6640 train_time:142631ms step_avg:149.98ms
step:962/1000 train_loss:3.9176 train_time:142780ms step_avg:149.98ms
step:963/1000 train_loss:3.8701 train_time:142931ms step_avg:149.98ms
step:964/1000 train_loss:3.7070 train_time:143079ms step_avg:149.98ms
step:965/1000 train_loss:3.7235 train_time:143227ms step_avg:149.98ms
step:966/1000 train_loss:3.7559 train_time:143375ms step_avg:149.97ms
step:967/1000 train_loss:3.9696 train_time:143523ms step_avg:149.97ms
step:968/1000 train_loss:3.7980 train_time:143672ms step_avg:149.97ms
step:969/1000 train_loss:3.7965 train_time:143819ms step_avg:149.97ms
step:970/1000 train_loss:3.8367 train_time:143970ms step_avg:149.97ms
step:971/1000 train_loss:3.6594 train_time:144118ms step_avg:149.97ms
step:972/1000 train_loss:3.8187 train_time:144266ms step_avg:149.96ms
step:973/1000 train_loss:3.7570 train_time:144414ms step_avg:149.96ms
step:974/1000 train_loss:3.8119 train_time:144562ms step_avg:149.96ms
step:975/1000 train_loss:3.8989 train_time:144711ms step_avg:149.96ms
step:976/1000 train_loss:3.7513 train_time:144857ms step_avg:149.96ms
step:977/1000 train_loss:3.9481 train_time:145008ms step_avg:149.96ms
step:978/1000 train_loss:3.8345 train_time:145156ms step_avg:149.95ms
step:979/1000 train_loss:3.6687 train_time:145304ms step_avg:149.95ms
step:980/1000 train_loss:3.9635 train_time:145452ms step_avg:149.95ms
step:981/1000 train_loss:3.6929 train_time:145599ms step_avg:149.95ms
step:982/1000 train_loss:3.8590 train_time:145748ms step_avg:149.95ms
step:983/1000 train_loss:3.8371 train_time:145895ms step_avg:149.94ms
step:984/1000 train_loss:3.8392 train_time:146044ms step_avg:149.94ms
step:985/1000 train_loss:3.7973 train_time:146193ms step_avg:149.94ms
step:986/1000 train_loss:3.8716 train_time:146341ms step_avg:149.94ms
step:987/1000 train_loss:3.6781 train_time:146490ms step_avg:149.94ms
step:988/1000 train_loss:3.7707 train_time:146636ms step_avg:149.93ms
step:989/1000 train_loss:3.7265 train_time:146785ms step_avg:149.93ms
step:990/1000 train_loss:3.7140 train_time:146933ms step_avg:149.93ms
step:991/1000 train_loss:3.9184 train_time:147082ms step_avg:149.93ms
step:992/1000 train_loss:3.7504 train_time:147230ms step_avg:149.93ms
step:993/1000 train_loss:3.7215 train_time:147379ms step_avg:149.93ms
step:994/1000 train_loss:3.7871 train_time:147527ms step_avg:149.93ms
step:995/1000 train_loss:3.8782 train_time:147675ms step_avg:149.92ms
step:996/1000 train_loss:3.8275 train_time:147824ms step_avg:149.92ms
step:997/1000 train_loss:3.7248 train_time:147973ms step_avg:149.92ms
step:998/1000 train_loss:4.0890 train_time:148120ms step_avg:149.92ms
step:999/1000 train_loss:3.7396 train_time:148268ms step_avg:149.92ms
step:1000/1000 train_loss:3.8575 train_time:148416ms step_avg:149.91ms
step:1000/1000 val_loss:3.7641 train_time:148439ms step_avg:149.94ms
